{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 정보로드를 위한 라이브러리\n",
    "# 설치: pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 토큰 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 오디오파일에 자막생성\n",
    "- response_format=\"srt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = open(\"data/채용면접_샘플_01.wav\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    file=audio_file,\n",
    "    model=\"whisper-1\",\n",
    "    language=\"ko\",\n",
    "    response_format=\"srt\",  # 자막 포맷\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:00,000 --> 00:00:07,000\n",
      "지금 생각해보면 가장 기억에 남는 것이 외환위기 때의 경험입니다.\n",
      "\n",
      "2\n",
      "00:00:07,000 --> 00:00:13,000\n",
      "외환위기 때 나라뿐 아니라 회사에서는 달러가 많이 부족했고\n",
      "\n",
      "3\n",
      "00:00:13,000 --> 00:00:19,000\n",
      "우리는 수출을 하기 위해서 원자재라든지 대금지급\n",
      "\n",
      "4\n",
      "00:00:19,000 --> 00:00:23,000\n",
      "혹은 또 우리가 수출한 물건에 대한 대금을 받아야 되는 상황이었습니다.\n",
      "\n",
      "5\n",
      "00:00:23,000 --> 00:00:29,000\n",
      "일단은 우리가 해외로부터 받아야 될 때는 최대한 그것을 달러로 받았고\n",
      "\n",
      "6\n",
      "00:00:29,000 --> 00:00:36,000\n",
      "반대로 우리가 지불해야 될 것은 가능한 한 원자재를 통해서 지급을 했습니다.\n",
      "\n",
      "7\n",
      "00:00:36,000 --> 00:00:40,000\n",
      "그 원자재라는 것이 결국은 반도체 쪽이었는데\n",
      "\n",
      "8\n",
      "00:00:40,000 --> 00:00:46,000\n",
      "우리나라에서 가장 나름 손쉽게 구하면서도 꼭 필요한 제품인 반도체를\n",
      "\n",
      "9\n",
      "00:00:46,000 --> 00:00:48,000\n",
      "원자재값 대응으로 물건을 주면서\n",
      "\n",
      "10\n",
      "00:00:48,000 --> 00:00:53,000\n",
      "반대로 해외에서 우리가 받아야 될 것은 달러로 받으면서\n",
      "\n",
      "11\n",
      "00:00:53,000 --> 00:00:57,000\n",
      "그 환율차를 가장 줄일 수가 있었습니다.\n",
      "\n",
      "12\n",
      "00:00:57,000 --> 00:01:06,000\n",
      "그것으로 인해서 약 6개월 동안에 나름 회사에서는 달러 지출을 막을 수 있었고\n",
      "\n",
      "13\n",
      "00:01:06,000 --> 00:01:15,000\n",
      "그때 그나마 달러를 회사에서 확보를 해서 나름의 위기를 극복할 수 있었던 것으로 생각합니다.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. YouTube url에서 음성파일을 만들기\n",
    "- pytube 라이브러리 오류 -> pytubefix 라이브러리 사용\n",
    "\n",
    "- https://pypi.org/project/pytubefix/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략\n"
     ]
    }
   ],
   "source": [
    "from pytubefix import YouTube\n",
    "from pytubefix.cli import on_progress\n",
    "import os\n",
    "\n",
    "# [SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략\n",
    "link = \"https://youtu.be/sy2asT2c8FM?si=BJzcXPR6c26Efn2b\"\n",
    "\n",
    "yt = YouTube(link, on_progress_callback = on_progress)\n",
    "print(yt.title)\n",
    "\n",
    "ys = yt.streams.get_highest_resolution()\n",
    "\n",
    "ys = yt.streams.get_audio_only()\n",
    "filename = ys.download(mp3=True) # pass the parameter mp3=True to save in .mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략.mp3'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 MP3를  WAV 변환\n",
    "변환을 해야 api 먹힘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import AudioFileClip\n",
    "\n",
    "\n",
    "def convert_mp3_to_wav(filepath):\n",
    "    # WAV 파일 경로\n",
    "    wav_file_path = filepath.replace(\".mp3\", \".wav\")\n",
    "\n",
    "    # MP4 파일 로드\n",
    "    audio_clip = AudioFileClip(filepath)\n",
    "\n",
    "    # WAV 형식으로 오디오 추출 및 저장\n",
    "    audio_clip.write_audiofile(wav_file_path, fps=44100, nbytes=2, codec=\"pcm_s16le\")\n",
    "    return wav_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in /Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "wav_file = convert_mp3_to_wav(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 MP3를  chunk 단위로 분할\n",
    "whisper api의 stt를 위해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# 오디오 파일 불러오기\n",
    "audio = AudioSegment.from_file(wav_file, format=\"wav\")\n",
    "\n",
    "total_length = len(audio)\n",
    "length_per_chunk = 60 * 1000  # 60초\n",
    "\n",
    "if not os.path.exists(\".tmp\"):\n",
    "    os.mkdir(\".tmp\")\n",
    "\n",
    "folder_path = os.path.join(\".tmp\", wav_file[:-4])\n",
    "\n",
    "if not os.path.exists(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "\n",
    "chunks = []\n",
    "for i in range(0, total_length, length_per_chunk):\n",
    "    chunk_file_path = os.path.join(folder_path, f\"{i}.wav\")\n",
    "    audio[i : i + length_per_chunk].export(chunk_file_path, format=\"wav\")\n",
    "    chunks.append(chunk_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/0.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/60000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/120000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/180000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/240000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/300000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/360000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/420000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/480000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/540000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/600000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/660000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/720000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/780000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/840000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/900000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/960000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/1020000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/1080000.wav',\n",
       " '/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/1140000.wav']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "\n",
    "def adjust_timestamps(transcript, minutes=1):\n",
    "    # Define the regular expression pattern for timestamps\n",
    "    timestamp_pattern = re.compile(r\"(\\d{2}:\\d{2}:\\d{2},\\d{3})\")\n",
    "\n",
    "    # Function to add minutes to a timestamp\n",
    "    def add_minutes(timestamp_str, minutes):\n",
    "        timestamp = datetime.strptime(timestamp_str, \"%H:%M:%S,%f\")\n",
    "        adjusted_timestamp = timestamp + timedelta(minutes=minutes)\n",
    "        return adjusted_timestamp.strftime(\"%H:%M:%S,%f\")[:-3]\n",
    "\n",
    "    # Replace timestamps in the transcript\n",
    "    adjusted_transcript = timestamp_pattern.sub(\n",
    "        lambda match: add_minutes(match.group(1), minutes), transcript\n",
    "    )\n",
    "    return adjusted_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 텍스트로 추출한거 리스트에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/0.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/60000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/120000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/180000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/240000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/300000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/360000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/420000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/480000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/540000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/600000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/660000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/720000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/780000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/840000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/900000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/960000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/1020000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/1080000.wav\n",
      "/Users/jiminking/Documents/김지민/projects/langchain_study/03.OpenAI/[SK TECH SUMMIT 2023] RAG를 위한 Retriever 전략/1140000.wav\n"
     ]
    }
   ],
   "source": [
    "transcripts = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(chunk)\n",
    "    audio_file = open(chunk, \"rb\")\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        file=audio_file,\n",
    "        model=\"whisper-1\",\n",
    "        language=\"ko\",\n",
    "        response_format=\"srt\",\n",
    "        temperature=0.01,\n",
    "    )\n",
    "    transcripts.append(adjust_timestamps(transcript, minutes=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1\\n00:00:00,000 --> 00:00:16,440\\n안녕하세요. 저는 오늘 RAG를 위한 리트리버 전략이라는 내용으로 발표를 하도록 하겠습니다.\\n\\n2\\n00:00:16,440 --> 00:00:21,400\\n저는 Organize에서 최고 AI 책임자를 맡고 있고요.\\n\\n3\\n00:00:21,400 --> 00:00:27,600\\n10몇 년 동안 백엔드 엔지니어로 일하다가 지금은 AI 이렇게 하고 있네요.\\n\\n4\\n00:00:27,600 --> 00:00:34,040\\n오늘 말씀드릴 내용은 RAG가 대충 뭔지 간략하게 설명드리고\\n\\n5\\n00:00:34,040 --> 00:00:40,400\\n지금 채취 PT나 이런 것 때문에 굉장히 LRM이 발전되어 있어서 말 되게 잘하는데\\n\\n6\\n00:00:40,400 --> 00:00:46,959\\n왜 아직도 답변은 제대로 안 나오는가? 그거에 대해서 좀 얘기를 해보려고 합니다.\\n\\n7\\n00:00:46,959 --> 00:00:52,119\\n그래서 RAG가 뭔지부터 좀 말씀드리도록 하겠습니다.\\n\\n8\\n00:00:52,160 --> 00:00:55,680\\n나온 것은 2021년에 맨 처음 나왔고요.\\n\\n9\\n00:00:55,680 --> 00:00:59,959\\n이때는 아직 GPT3 시절이죠.\\n\\n\\n', '1\\n00:01:00,000 --> 00:01:09,080\\n그래서 이 제네레이션 쪽에서 그렇게 말이 되기는 하고는 있지만 그렇게 놀라운 성능을 발휘하지 못하는 시절이었는데\\n\\n2\\n00:01:09,080 --> 00:01:18,280\\n이제 오픈AI에서 모델을 발표하고 난 다음부터는 이 제네레이터의 성능이 엄청나게 높아지니까\\n\\n3\\n00:01:18,280 --> 00:01:26,080\\n얘가 이제 이거를 이용해서 어떻게 이제 기업 내부 문서 혹은 나만이 알고 있는 문서들에서 답변을 할 수 있을까\\n\\n4\\n00:01:26,080 --> 00:01:32,759\\n하는 고민들을 하기 시작했습니다 그래서 이제 사람들이 많이 채택하고 있는 방법이 RAZ 인데요\\n\\n5\\n00:01:32,759 --> 00:01:40,599\\n프롬프트에다가 그냥 내가 물어본 질문에 대한 답변 후보들을 프롬프트에다 집어 넣는 방식입니다\\n\\n6\\n00:01:40,599 --> 00:01:49,840\\n어려운 것은 개념상 어려운 것은 아니구요 그래서 그렇게만 잘 들어가면 이 놀라운 LLM이 답변을 잘 해주고 있는데요\\n\\n7\\n00:01:49,840 --> 00:01:57,160\\n문제는 그럼에도 불구하고 아직도 내 기업 내부 문서에서 답변을 해주는 것은 갈 길이 멀어 보인다\\n\\n8\\n00:01:57,160 --> 00:02:00,560\\n뭐 이런 느낌이 좀 있습니다\\n\\n\\n', '1\\n00:02:00,000 --> 00:02:05,060\\n퍼플렉스틱 AI처럼 웹사이트에서 답변을 해주는 것도 성능이 놀라운데\\n\\n2\\n00:02:05,060 --> 00:02:08,260\\n유독 기업 내부 문서에서는 아직도 어려워요\\n\\n3\\n00:02:08,260 --> 00:02:11,200\\n근데 그 어려운 이유가 보통 또 찾아보면\\n\\n4\\n00:02:11,200 --> 00:02:14,980\\n리트리버 성능이 잘 안 나온다 뭐 이런 얘기가 있는데\\n\\n5\\n00:02:14,980 --> 00:02:17,620\\n저는 오늘 리트리버 얘기도 할 거고\\n\\n6\\n00:02:17,620 --> 00:02:21,040\\n그 다음에 흔히 강가되고 있는 도큐먼트 파싱에 관한 내용도\\n\\n7\\n00:02:21,040 --> 00:02:22,940\\n이야기를 좀 해보려고 합니다\\n\\n8\\n00:02:23,940 --> 00:02:30,260\\n이 발표 자료를 이제 한참 만들고 있었는데\\n\\n9\\n00:02:30,260 --> 00:02:32,619\\n저번주 화요일이었죠\\n\\n10\\n00:02:32,619 --> 00:02:37,779\\n오픈AI에서 리트리버, 어시스턴트 API의 리트리버를 발표했다\\n\\n11\\n00:02:37,779 --> 00:02:40,340\\n는 소식을 듣고 망했다\\n\\n12\\n00:02:40,340 --> 00:02:42,419\\n이렇게 생각했는데\\n\\n13\\n00:02:42,419 --> 00:02:45,939\\n그래서 바로 출근해서 바로 확인을 해봤죠\\n\\n14\\n00:02:45,939 --> 00:02:49,660\\n근데 보고 난 느낌은 뭐냐면\\n\\n15\\n00:02:50,660 --> 00:02:55,299\\n우리가 고민하고 있는 게 지금 남들도 여전히 어렵구나\\n\\n16\\n00:02:55,299 --> 00:02:57,340\\n라는 생각이 좀 들었고요\\n\\n\\n', '1\\n00:03:00,000 --> 00:03:03,760\\n올 수 있었는데 날먹하지는 못했고요\\n\\n2\\n00:03:03,760 --> 00:03:10,400\\n어 이게 뭐 범령 관련된 문서 인터넷 다운받아 가지고\\n\\n3\\n00:03:10,400 --> 00:03:16,400\\n그 오픈 AI 어시스턴트 API 이용해서 넣어서 확인을 해본 결과인데요\\n\\n4\\n00:03:16,400 --> 00:03:22,280\\n전자 말찌 부착 명령에 뭐 어쩌고 저쩌고 물어보면\\n\\n5\\n00:03:22,280 --> 00:03:24,840\\n이 문서에 대해서 답을 해줘야 될 것 같은데\\n\\n6\\n00:03:24,840 --> 00:03:26,840\\n답을 제대로 못해주고 있어요\\n\\n7\\n00:03:26,840 --> 00:03:31,520\\n보면은 표에 있는 숫자 내용도 틀렸고요\\n\\n8\\n00:03:31,520 --> 00:03:36,799\\n전반적으로 숫자가 틀렸으니까 당연히 답변 내용은 틀렸습니다\\n\\n9\\n00:03:36,799 --> 00:03:41,840\\n왜 이런 일이 벌어질까요? 무려 오픈 AI인데\\n\\n10\\n00:03:41,840 --> 00:03:48,320\\n먼저 우리가 다루는 문서에 대해서 좀 한번 좀 먼저 고려를 해봅시다\\n\\n11\\n00:03:48,320 --> 00:03:52,360\\n주로 기업 내부 문서라면 pdf 혹은 워드 문서\\n\\n12\\n00:03:52,360 --> 00:03:58,680\\n엑셀 아래 앙글 등등 있겠지만 요 3개를 다 비슷해 보여요\\n\\n13\\n00:03:58,680 --> 00:03:59,880\\npdf 같은 경우에는\\n\\n\\n', '1\\n00:04:00,000 --> 00:04:04,360\\n얘가 문서를 작성하기 위한 시스템이 아니에요\\n\\n2\\n00:04:04,360 --> 00:04:06,840\\n조판 시스템, 출력용 시스템\\n\\n3\\n00:04:06,840 --> 00:04:09,440\\n그러니까 이렇게 우리 눈에 이렇게 보인다 그래서\\n\\n4\\n00:04:09,440 --> 00:04:13,760\\n글이 절대 그 순서대로 있을 거라는 보장이 없습니다\\n\\n5\\n00:04:13,760 --> 00:04:18,280\\n그래서 다단 같은 경우에 깨지기도 하고 그렇습니다\\n\\n6\\n00:04:18,280 --> 00:04:20,120\\n레이아웃을 잘 알 수 있으면 좋겠는데\\n\\n7\\n00:04:20,120 --> 00:04:22,240\\n레이아웃은 잘 몰라요\\n\\n8\\n00:04:22,240 --> 00:04:25,480\\n그 다음에 헤더프터 정보도 없습니다\\n\\n9\\n00:04:25,480 --> 00:04:32,279\\n그리고 목차 같은 경우에 가끔씩 메타 정보로 있기도 하지만\\n\\n10\\n00:04:32,279 --> 00:04:34,480\\n없는 경우가 대부분이고요\\n\\n11\\n00:04:34,480 --> 00:04:37,040\\n테이블 같은 경우는 그냥 글씨에요\\n\\n12\\n00:04:37,040 --> 00:04:39,680\\n그냥 글씨에다가 그림 입혀놓은 겁니다\\n\\n13\\n00:04:39,680 --> 00:04:42,320\\n그래서 절대 테이블이라는 것도 알 수가 없어요\\n\\n14\\n00:04:42,320 --> 00:04:46,520\\n이런 문서를 우리는 다뤄야 해요\\n\\n15\\n00:04:46,520 --> 00:04:49,240\\n워드로 가면 좀 낫냐? 아무래도 좀 낫죠\\n\\n16\\n00:04:49,240 --> 00:04:51,720\\n왜냐하면 얘는 조판 시스템이 아니니까요\\n\\n17\\n00:04:51,720 --> 00:04:57,320\\n근데 현실에서 우리한테 들어오는 기업용 문서는\\n\\n18\\n00:04:57,320 --> 00:05:00,000\\n사실상 조파는 시스템처럼 써요\\n\\n\\n', '1\\n00:05:00,000 --> 00:05:07,320\\n그래서 뭐 워드 문서를 열어보시면 위에 제목도 있고 뭐 이렇게 시스템적으로 잘 되어 있죠\\n\\n2\\n00:05:07,320 --> 00:05:12,160\\n그걸로 스타일 잘 맞춰서 문서를 작성을 안합니다\\n\\n3\\n00:05:12,160 --> 00:05:20,280\\n그런거랑 상관없이 그냥 눈에 보이기만 그렇게 보이게 만든 문서들이 상당히 많구요\\n\\n4\\n00:05:20,639 --> 00:05:29,799\\n테이블 같은 경우에도 머지 드 세일이 많죠 그리고 이쁘게 얼라인을 맞추기 위해서 그냥 테이블이 아닌데 테이블 안에다가\\n\\n5\\n00:05:29,799 --> 00:05:35,160\\n글씨를 집어 넣기도 하죠 자 그러면 태생이 테이블인 엑셀은 어떠냐\\n\\n6\\n00:05:35,160 --> 00:05:41,799\\n얘도 마찬가지에요 테이블이 아니에요 그래서 요런 문서들을 가지고 우리는 답변을 해야 됩니다\\n\\n7\\n00:05:41,799 --> 00:05:49,200\\n글이 어디 있는지도 모르고 이게 테이블인데 테이블인지도 모르고 깜깜한 상태에서 이제 찾아봐야 되는 거죠\\n\\n8\\n00:05:49,200 --> 00:05:57,480\\n이런 문제점을 오픈AI도 똑같이 겪고 있었을 거라고 생각을 하고 이제 몇몇 뭐\\n\\n9\\n00:05:57,480 --> 00:06:01,439\\n몇몇 문서만 올린 시스템에서는\\n\\n\\n', '1\\n00:06:00,000 --> 00:06:02,000\\n뭐 이렇게 해도 잘 동작을 하겠죠\\n\\n2\\n00:06:02,000 --> 00:06:04,400\\n근데 아까 봤던 것들은\\n\\n3\\n00:06:04,400 --> 00:06:06,400\\ncitation이 이제 고치겠지만\\n\\n4\\n00:06:06,400 --> 00:06:09,660\\n깨져서 나와서 그걸로 테스트는 못해봤고\\n\\n5\\n00:06:09,660 --> 00:06:12,700\\ncitation이 깨지지 않은 예제를 좀 찾아봤어요\\n\\n6\\n00:06:12,700 --> 00:06:16,559\\n그래서 CUDA API 문서를 올려놓고\\n\\n7\\n00:06:16,559 --> 00:06:18,980\\n질문을 해봤더니 citation이 나오더라구요\\n\\n8\\n00:06:18,980 --> 00:06:21,580\\n그래서 citation이 어떻게 잡혔나 이렇게 딱 봤더니\\n\\n9\\n00:06:21,580 --> 00:06:26,120\\n문서의 그 제목...\\n\\n10\\n00:06:26,120 --> 00:06:28,980\\n아 제목이 아니라 목차 부분을 찾았어요\\n\\n11\\n00:06:29,200 --> 00:06:31,520\\n우리가 원하는 대답은 목차에 있진 않겠죠\\n\\n12\\n00:06:31,520 --> 00:06:33,520\\n어쨌든 목차 부분을 찾았고\\n\\n13\\n00:06:33,520 --> 00:06:36,919\\n그 다음에 페이지 번호나 이런 건 제거했는데\\n\\n14\\n00:06:36,919 --> 00:06:39,279\\n이게 전체 페이지가 아니고 일부분만 잘랐고\\n\\n15\\n00:06:39,279 --> 00:06:42,119\\n자른 그 일부분 페이지도\\n\\n16\\n00:06:42,119 --> 00:06:45,439\\n의미가 있게 자르지는 않았더라구요\\n\\n17\\n00:06:45,439 --> 00:06:48,680\\n그렇게 자르면 잘 안됩니다\\n\\n18\\n00:06:48,680 --> 00:06:51,959\\n당연하게도 잘 안되겠죠\\n\\n19\\n00:06:51,959 --> 00:06:54,279\\n그래서 우리 시스템에다가\\n\\n20\\n00:06:54,279 --> 00:06:58,240\\n저희 제품에다가 했는데 다행히 잘 나왔어요\\n\\n21\\n00:06:58,240 --> 00:07:00,020\\n이렇게 나오기 위해서는\\n\\n\\n', '1\\n00:07:00,000 --> 00:07:06,840\\n지금 잠깐 잠깐 말씀드렸다시피 이게 문서를 좀 이해할 수 있는 방법이 필요합니다\\n\\n2\\n00:07:06,840 --> 00:07:13,300\\n그리고 그 이해할 수 있는 방법이 굉장히 많은 노가다와 꼼수와 이런 것들도 점처리 되어 있는데\\n\\n3\\n00:07:13,300 --> 00:07:18,260\\n그 중에서 테이블을 어떻게 다룰 것인가 이 부분을 많이 물어보셔가지고\\n\\n4\\n00:07:18,260 --> 00:07:20,900\\n테이블 쪽 얘기를 좀 해보려고 합니다 먼저\\n\\n5\\n00:07:20,900 --> 00:07:27,620\\n오픈 AI 에다가 넣을 때 테이블은 MD로 넣으면은 잘 된다\\n\\n6\\n00:07:27,959 --> 00:07:30,260\\n이거는 많이 알려져 있을 겁니다\\n\\n7\\n00:07:30,260 --> 00:07:38,099\\n그래서 아까 그 문서 그대로 좀 간략하게 하긴 했지만 MD로 넣어놓고 질문을 하면 질문이 잘 됩니다\\n\\n8\\n00:07:38,099 --> 00:07:42,740\\n자 근데 모든 걸 MD로 하면 진짜 잘 될까요?\\n\\n9\\n00:07:42,740 --> 00:07:49,299\\n실험을 해보신 분들은 아시겠지만 테이블이 조금만 커져도 얘가 헷갈리기 시작합니다\\n\\n10\\n00:07:49,299 --> 00:07:56,619\\n그리고 실제 이제 우리가 다루는 문서에서는 테이블 안에 이렇게 숫자만 있는 테이블도 있겠지만\\n\\n11\\n00:07:56,619 --> 00:07:59,900\\n거기에 글이 있는 경우들도 있고 글에는 당연히\\n\\n\\n', '1\\n00:08:00,000 --> 00:08:04,320\\n멘터라든지 아니면 이렇게 수직파 같은 특수 문자들이 들어가고\\n\\n2\\n00:08:04,320 --> 00:08:06,840\\n그런 것 때문에 MD의 포맷이 깨집니다.\\n\\n3\\n00:08:06,840 --> 00:08:11,360\\n이제 그런 것들을 다루면은 이제 더 이상 MD가 MD가 아니에요.\\n\\n4\\n00:08:11,360 --> 00:08:15,800\\n그래서 요거를 그럼 어떻게 처리해야 되지?\\n\\n5\\n00:08:15,800 --> 00:08:17,400\\n를 고민을 하다가\\n\\n6\\n00:08:17,400 --> 00:08:20,920\\n그럼 테이블을 그냥 말처럼 써볼까?\\n\\n7\\n00:08:20,920 --> 00:08:23,600\\n해서 테이블로 자연어를 만들었어요.\\n\\n8\\n00:08:23,600 --> 00:08:27,600\\n그래서 요 테이블을 그냥 말로 이렇게 쭉 설명을 한 거죠.\\n\\n9\\n00:08:28,120 --> 00:08:32,200\\n헤더를 보고 연도는 2008년이고\\n\\n10\\n00:08:32,200 --> 00:08:35,080\\n이렇게 쭉 헤더만 알 수 있다면\\n\\n11\\n00:08:35,080 --> 00:08:38,279\\n저렇게 말을 만들어내는 건 기계적으로 만들어낼 수 있겠죠.\\n\\n12\\n00:08:38,279 --> 00:08:41,840\\n저런 식의 기계적으로 자연어로 만들었을 때\\n\\n13\\n00:08:41,840 --> 00:08:43,680\\n답변 역시 훌륭합니다.\\n\\n14\\n00:08:43,680 --> 00:08:45,840\\n저렇게 했을 때의 장점은 뭐냐면\\n\\n15\\n00:08:45,840 --> 00:08:49,880\\nMerged Cell이나 아니면 큰 테이블 혹은\\n\\n16\\n00:08:49,880 --> 00:08:52,840\\n좀 복잡해진 것들에 대해서도 얼마든지\\n\\n17\\n00:08:52,840 --> 00:08:56,799\\n헤더만 제대로 잡을 수 있다면 처리가 가능하다는 거죠.\\n\\n18\\n00:08:56,799 --> 00:08:59,919\\n그리고 후술하겠지만 리트리버에서도 요렇게\\n\\n\\n', '1\\n00:09:00,000 --> 00:09:04,520\\n했을 때 훨씬 잘 잡는 경우들이 있구요\\n\\n2\\n00:09:04,520 --> 00:09:07,520\\n그래서 테이블을 다루기 위해서는\\n\\n3\\n00:09:07,520 --> 00:09:12,520\\n그 문서를 이제 이해하는데 필요한 일부 기술들이지만\\n\\n4\\n00:09:12,520 --> 00:09:15,200\\n이렇게 많은 기술들이 들어갈 수밖에 없어요\\n\\n5\\n00:09:15,200 --> 00:09:18,160\\n아까 말씀드렸다시피 테이블이 테이블이 아니라 그랬잖아요\\n\\n6\\n00:09:18,160 --> 00:09:20,959\\n그래서 테이블이 어디 있는지를 확인을 해야 되구요\\n\\n7\\n00:09:20,959 --> 00:09:27,639\\n그 다음에 그 영역을 셀별로 잘 뜯어서 셀로 잘 구축을 해야 되구요\\n\\n8\\n00:09:27,639 --> 00:09:33,040\\n그게 만약에 간단하다라고 판단이 들었다면 마크다운으로 처리하면 간단해지지만\\n\\n9\\n00:09:33,040 --> 00:09:38,799\\n만약에 복잡하다라고 판단이 되면 거기서 어디가 헤더인지를 찾아야 돼요\\n\\n10\\n00:09:38,799 --> 00:09:45,439\\n간단히 생각하면 첫 열, 첫 번째 컬러가 헤더가 아니겠냐 라고 하지만\\n\\n11\\n00:09:45,439 --> 00:09:48,160\\n세상은 그렇게 호락호락하지 않습니다\\n\\n12\\n00:09:48,160 --> 00:09:53,279\\n머지드셀도 있고 헤더가 두 개 세 개 이렇게 된 데도 있고\\n\\n13\\n00:09:53,279 --> 00:09:56,480\\n열 중간에 들어가는 것도 있고\\n\\n14\\n00:09:56,480 --> 00:09:59,439\\n물론 이거를 완벽하게 잘 했다면\\n\\n\\n', '1\\n00:10:00,000 --> 00:10:05,040\\n큰 기업이 됐겠지만 저희도 열심히 실제 더러운 데이터들이랑 싸우면서\\n\\n2\\n00:10:05,040 --> 00:10:07,140\\n노하우를 열심히 쌓고 있습니다\\n\\n3\\n00:10:07,140 --> 00:10:12,280\\n굉장히 어려운 일이다 라는 것을 말씀드리고 싶은 거고요\\n\\n4\\n00:10:12,280 --> 00:10:17,680\\n이제 리트리버 얘기를 또 하겠습니다\\n\\n5\\n00:10:17,680 --> 00:10:22,680\\n그렇게 해서 문서를 적당한 크기로 자르고\\n\\n6\\n00:10:22,680 --> 00:10:26,320\\n그 다음에 뭐 구조와 unstructured 데이터를\\n\\n7\\n00:10:26,320 --> 00:10:30,280\\nunstructured을 잘 이해할 수 있도록 만들고 다 좋았어요\\n\\n8\\n00:10:30,280 --> 00:10:35,939\\n그러면 질문을 하면 이제 질문에 답변이 될 만한 것들을 찾아서\\n\\n9\\n00:10:35,939 --> 00:10:38,119\\n프롬프트에다 집어 넣어 줘야 되죠\\n\\n10\\n00:10:38,119 --> 00:10:45,080\\n근데 답변이 될 만한 걸 어떻게 찾을 수 있을까요\\n\\n11\\n00:10:45,080 --> 00:10:48,439\\n가장 간단한 건 그냥 es 에다 물어보는 거죠\\n\\n12\\n00:10:48,439 --> 00:10:56,680\\nes 에다 물어보는 것은 뭐 그냥 등장한 키워드가 나오는 거니까\\n\\n13\\n00:10:56,680 --> 00:10:58,160\\n그냥 직관적으로 이해가 가죠\\n\\n14\\n00:10:58,160 --> 00:11:01,160\\n근데 질문이 꼭 키워드가\\n\\n\\n', '1\\n00:11:00,000 --> 00:11:02,000\\n코드가 나온 데서만 나올 수는 없잖아요\\n\\n2\\n00:11:02,000 --> 00:11:06,320\\n그런 이런 스팟스 리트리버의 한계도 물론 명확하고\\n\\n3\\n00:11:06,320 --> 00:11:10,080\\n그래서 댄스 리트리버들이 이제 나왔죠\\n\\n4\\n00:11:10,080 --> 00:11:14,200\\ndpr이나 오픈웨어 인베딩이나 이런 것들을 써서\\n\\n5\\n00:11:14,200 --> 00:11:18,000\\n그런 것들도 보면은 비슷비슷합니다\\n\\n6\\n00:11:18,000 --> 00:11:20,920\\n어떤 데이터셋은 bm25가 이기고\\n\\n7\\n00:11:20,920 --> 00:11:22,240\\n어떤 건 자기네가 이기고\\n\\n8\\n00:11:22,240 --> 00:11:24,799\\n어떤 건 앙상블 했더니 다 이기고\\n\\n9\\n00:11:24,799 --> 00:11:28,080\\n실제 고객 문서로 가면 비슷해요\\n\\n10\\n00:11:28,799 --> 00:11:32,080\\n어떤 때는 얘가 이기고 어떤 때는 얘가 이기고 그렇습니다\\n\\n11\\n00:11:32,080 --> 00:11:35,680\\n그거를 보여줄 수 있는 내용이 요건데요\\n\\n12\\n00:11:35,680 --> 00:11:39,639\\n이제 저희가 이제 고객들의 공개는 안되겠지만\\n\\n13\\n00:11:39,639 --> 00:11:43,439\\n고객들의 문서와 고객들의 질문들을 수집해서\\n\\n14\\n00:11:43,439 --> 00:11:45,080\\n벤치마크를 해봤습니다\\n\\n15\\n00:11:45,080 --> 00:11:47,560\\n그래서 파란색은 bm25\\n\\n16\\n00:11:47,560 --> 00:11:49,080\\n빨간색은 그냥 인베딩\\n\\n17\\n00:11:49,080 --> 00:11:51,119\\n여기는 오픈웨어 인베딩으로 했지만\\n\\n18\\n00:11:51,119 --> 00:11:54,880\\n인베딩 그 다음에 노란색은 앙상블한 거\\n\\n19\\n00:11:54,880 --> 00:11:57,360\\n그 다음에 초록색은 제목까지 앙상블한 거\\n\\n20\\n00:11:57,360 --> 00:11:58,000\\n이렇게 해봤습니다\\n\\n21\\n00:11:58,000 --> 00:11:59,799\\n천차만별이죠\\n\\n\\n', '1\\n00:12:00,000 --> 00:12:08,840\\n어렵습니다. 그래서 이 고객 문서와 질문에 따라서 이 웨이트를 자동으로 조정할 필요가 있어요\\n\\n2\\n00:12:08,840 --> 00:12:15,040\\n그런 것들을 어떻게 하는지도 실제 세계에서는 어려운 일이구요\\n\\n3\\n00:12:15,040 --> 00:12:24,440\\n여기까지 했다면 이제 어 그러면 뭐 dpr 이든지 뭐든지 이런 리트리버를 학습을 시켜보면 더 성능이 좋아질 수 있지 않을까\\n\\n4\\n00:12:24,440 --> 00:12:32,480\\n농물 많잖아요. 댄스리트리브에 관한 농물. 자 근데 이거의 실제 세계 보면 또 어느 문제가 있냐면\\n\\n5\\n00:12:32,480 --> 00:12:40,160\\n자 이런 거를 학습을 시키기 위해서 뭐 크게 두 가지 방법이 있을 겁니다. 하나는 크로스 인코더 하나는 바이 인코더\\n\\n6\\n00:12:40,160 --> 00:12:50,520\\n잠깐 설명 드리자면 크로스 인코더는 질문과 컨텍스트가 한 개의 모델로 들어가서 답변이 나오는 거고 바이 인코더는 각각 들어가서\\n\\n7\\n00:12:50,520 --> 00:12:56,320\\n각각 계산된 다음에 나중에 합쳐지는 방식인데요 크로스 인코더는 실제로 쓸 수가 없어요\\n\\n8\\n00:12:56,320 --> 00:13:01,520\\n왜냐하면은 이 컨텍스트가 수십만 개\\n\\n\\n', '1\\n00:13:00,000 --> 00:13:06,600\\n수백만개인데 이거를 실시간에 질문 하나 할 때마다 이걸 모델에 다 태운다 이건 불가능한 일이잖아요\\n\\n2\\n00:13:06,600 --> 00:13:11,000\\n그래서 애초에 크로스 인코더는 고려대상이 아닙니다\\n\\n3\\n00:13:11,400 --> 00:13:16,559\\n바이 인코더라고 해서 그러면은 모델을 잘 학습을 할 수 있을까요?\\n\\n4\\n00:13:16,559 --> 00:13:22,200\\n아까랑 비슷한 문제가 발생을 해요 자 우리가 데이터를 열심히 모아서 좋은 모델을 만들었어요\\n\\n5\\n00:13:22,200 --> 00:13:26,480\\n그래서 짠 하고 배포를 해서 문서를 짜고 그러고 와 성능이 괜찮다\\n\\n6\\n00:13:26,480 --> 00:13:32,279\\n절대 세상은 그렇게 끝날 일이 없죠 아쉬움이 발생하고 데이터는 더 쌓이고 또 학습을 시킬 겁니다\\n\\n7\\n00:13:32,279 --> 00:13:40,080\\n그러면 10만개 되는 컨텍스트 벡터 100만개 되는 컨텍스트 벡터를 다 업데이트를 해줘야 될까요?\\n\\n8\\n00:13:40,080 --> 00:13:48,000\\n사실상 힘들죠 그래서 컨텍스트 쪽 벡터 역시 학습이 불가능합니다\\n\\n9\\n00:13:48,560 --> 00:13:51,599\\n그래서 이런 모양이 될 수밖에 없어요\\n\\n10\\n00:13:51,599 --> 00:14:00,880\\n퀘스천만 학습이 될 수 있는, 되게 기형적으로 보이는데 학습이 또 그 문제만 있는 건 아니에요\\n\\n\\n', '1\\n00:14:00,000 --> 00:14:02,400\\n학습 데이터를 만들어야 되는데\\n\\n2\\n00:14:02,400 --> 00:14:04,680\\n이게 질문이\\n\\n3\\n00:14:04,680 --> 00:14:10,280\\n여기서 정답 세트는 질문과 이 질문의 후보입니다\\n\\n4\\n00:14:10,280 --> 00:14:14,780\\n근데 달걀과 닭과 달걀이 문제인데\\n\\n5\\n00:14:14,780 --> 00:14:17,900\\n이 리트리버가 굉장히 똑똑하다면\\n\\n6\\n00:14:17,900 --> 00:14:19,580\\n대부분을 맞출 거고\\n\\n7\\n00:14:19,580 --> 00:14:21,840\\n그러면 틀린 것만 걸러내도\\n\\n8\\n00:14:21,840 --> 00:14:26,120\\n굉장히 좋은 이런 답변 세트를 만들어낼 수 있을 거예요\\n\\n9\\n00:14:26,120 --> 00:14:28,120\\n근데 성능이 안 좋아서 학습을 시키려고\\n\\n10\\n00:14:28,120 --> 00:14:31,320\\n학습 데이터를 만드는데 그렇게 쓸 수는 없겠죠\\n\\n11\\n00:14:31,320 --> 00:14:34,160\\n근데 사람한테 질문을 만들라 그러면 엄청 힘들겠죠\\n\\n12\\n00:14:34,160 --> 00:14:35,720\\n근데 요새 세상은\\n\\n13\\n00:14:35,720 --> 00:14:38,400\\nGPT님이 다 해주시니까\\n\\n14\\n00:14:38,400 --> 00:14:40,480\\nGPT로 데이터를 생성할 수 있습니다\\n\\n15\\n00:14:40,480 --> 00:14:41,799\\n오, 되게 좋아요\\n\\n16\\n00:14:41,799 --> 00:14:42,959\\n그래서 생성을 했어요\\n\\n17\\n00:14:42,959 --> 00:14:45,639\\n근데 네거티브 데이터는 어떡하지?\\n\\n18\\n00:14:45,639 --> 00:14:48,080\\nGPT로 네거티브 데이터를 만들라 그러면\\n\\n19\\n00:14:48,080 --> 00:14:50,919\\n그러니까 페이지를 주고\\n\\n20\\n00:14:50,919 --> 00:14:55,759\\n이 페이지에서 답변이 나오지 않을 만한 질문을 생성을 해봐\\n\\n21\\n00:14:55,759 --> 00:14:57,840\\n죽어도 못합니다\\n\\n22\\n00:14:57,840 --> 00:14:59,160\\n안 돼요, 잘\\n\\n23\\n00:14:59,160 --> 00:15:01,160\\n나중에 되면...\\n\\n\\n', '1\\n00:15:00,000 --> 00:15:04,000\\n할 수 있겠죠 근데 적어도 지금은 잘 안 돼요 gpt4 안됩니다\\n\\n2\\n00:15:04,000 --> 00:15:07,000\\n그래서 포지티브 데이터는 대량으로 생성이 가능한데\\n\\n3\\n00:15:07,000 --> 00:15:10,000\\n네거티브 데이터를 생성하기가 어려워요\\n\\n4\\n00:15:10,000 --> 00:15:15,000\\n또 다시 생각하는 게 1번 페이지에서 만든 정답을\\n\\n5\\n00:15:15,000 --> 00:15:19,000\\n2번 페이지에 네거티브 샘플로 쓸 수 있지 않을까?\\n\\n6\\n00:15:19,000 --> 00:15:21,000\\n네 그렇지 않는 경우가 많습니다\\n\\n7\\n00:15:21,000 --> 00:15:25,000\\n그걸 사람 눈으로 하기에는 필요한 데이터가 너무 많고요\\n\\n8\\n00:15:25,000 --> 00:15:27,000\\n그래서 웬만하면 포지티브 데이터만 가지고\\n\\n9\\n00:15:27,000 --> 00:15:29,000\\n학습을 할 수 있었으면 좋겠어요\\n\\n10\\n00:15:29,000 --> 00:15:33,000\\n그렇다 보니까 로스 펑션은 저렇게 될 수밖에 없습니다\\n\\n11\\n00:15:33,000 --> 00:15:39,000\\n이거를 그냥 바이너리 크로스 엔트로피 같은 경우로 사용을 하면\\n\\n12\\n00:15:39,000 --> 00:15:41,000\\n모델이 망가질 수밖에 없어요\\n\\n13\\n00:15:41,000 --> 00:15:43,000\\n왜냐면 포지티브밖에 없으니까\\n\\n14\\n00:15:43,000 --> 00:15:46,000\\n그래서 벡터끼리 비교하는 mse나\\n\\n15\\n00:15:46,000 --> 00:15:49,000\\n코사인 인베링 로스 같은 걸 쓸 수밖에 없어요\\n\\n16\\n00:15:49,000 --> 00:15:50,000\\n되게 논리적인 비주얼이죠\\n\\n17\\n00:15:50,000 --> 00:15:53,000\\n근데 이게 동작을 할 것 같지가 않잖아요\\n\\n18\\n00:15:53,000 --> 00:15:54,000\\n암만 생각해도\\n\\n19\\n00:15:54,000 --> 00:15:56,000\\n되게 삐꾸고\\n\\n20\\n00:15:56,000 --> 00:16:00,000\\n그래서 실제로 해봤습니다\\n\\n\\n', '1\\n00:16:00,000 --> 00:16:05,360\\n보시면 빨간색이 퀘스천 임베딩이고\\n\\n2\\n00:16:05,360 --> 00:16:09,280\\n다시 한번 말씀드리지만 베이스라인은 오픈 AI 임베딩을 썼습니다\\n\\n3\\n00:16:09,280 --> 00:16:12,480\\n이유는 그게 성능이 제일 좋아서가 아니라\\n\\n4\\n00:16:12,480 --> 00:16:16,000\\n컨텍스트 윈도우가 제일 길어서\\n\\n5\\n00:16:16,000 --> 00:16:20,280\\n빨간색은 질문에 대한 임베딩이고요\\n\\n6\\n00:16:20,280 --> 00:16:24,280\\n파란색은 이제 문서 컨셉트에 대한 임베딩입니다\\n\\n7\\n00:16:24,600 --> 00:16:29,959\\n이거를 이제 2차원으로 PCA를 해 본 거고요\\n\\n8\\n00:16:29,959 --> 00:16:31,120\\n명확하게 갈려 있죠\\n\\n9\\n00:16:31,120 --> 00:16:36,119\\n그래서 빨간색이 저 파란색 쪽으로 보낼 수 있다면\\n\\n10\\n00:16:36,119 --> 00:16:40,599\\n이 모델이 워킹 할 거다 라는 생각이 든 거고\\n\\n11\\n00:16:40,599 --> 00:16:45,480\\n두 번째 그림이 실제로 초록색이 모델을 통과한 퀘스천 벡터들입니다\\n\\n12\\n00:16:45,480 --> 00:16:49,680\\n워킹을 하는 것 같습니다\\n\\n13\\n00:16:49,680 --> 00:16:52,080\\n그럼에도 불구하고\\n\\n14\\n00:16:52,080 --> 00:16:55,759\\n이게 왜 되지? 혹은 이게 맞는 건가?\\n\\n15\\n00:16:55,759 --> 00:16:57,599\\n라는 의문을 떨칠 수는 없는데\\n\\n16\\n00:16:57,599 --> 00:17:00,000\\n어쨌든 실제로 되니까 우리가\\n\\n\\n', '1\\n00:17:00,000 --> 00:17:03,840\\n고차원의 세계는 상상하기 참 힘들죠\\n\\n2\\n00:17:03,840 --> 00:17:06,880\\n그래서 결과를 요약하면 이렇게 얻었습니다\\n\\n3\\n00:17:06,880 --> 00:17:11,800\\n이것 역시 그 데이터셋에 따라서 또 많이 달라져요\\n\\n4\\n00:17:11,800 --> 00:17:14,960\\n근데 그 중에 한 데이터셋인 거고요\\n\\n5\\n00:17:14,960 --> 00:17:19,760\\n탑3 기준으로 실제로는 탑3나 탑5를 넘어갈 수가 없어요\\n\\n6\\n00:17:19,760 --> 00:17:25,040\\n다른 논문에 보면은 막 탑 100 뭐 이런 것들을 쓰긴 하는데\\n\\n7\\n00:17:25,040 --> 00:17:26,920\\n우리는 프롬프트에 넣어 줘야 되는데\\n\\n8\\n00:17:26,920 --> 00:17:31,879\\n프롬프트에다가 물론 지금은 128k gpt4가 나왔기 때문에\\n\\n9\\n00:17:31,879 --> 00:17:34,880\\n이제 100페이지 집어 넣을 수 있을지 모르겠지만\\n\\n10\\n00:17:34,880 --> 00:17:37,080\\n어쨌든 그런 돈이 참 비싸잖아요\\n\\n11\\n00:17:37,080 --> 00:17:40,400\\n그래서 현실적으로 탑3나 탑5 정도가 한계인데\\n\\n12\\n00:17:40,400 --> 00:17:48,040\\n그 정도에서 10% 5% 정도의 리트리버 성능을 올라간다는 것을 확인할 수 있었습니다\\n\\n13\\n00:17:48,040 --> 00:17:50,720\\n이거는 사실 앙상블 하기 전이고요\\n\\n14\\n00:17:50,720 --> 00:17:57,799\\n앙상블 하면 더 높은 성능 개선을 기대할 수 있겠죠\\n\\n15\\n00:17:57,799 --> 00:17:59,799\\n네 그래서 정리를 하자면\\n\\n\\n', '1\\n00:18:00,000 --> 00:18:03,640\\nLLM은 굉장히 뛰어나졌어요\\n\\n2\\n00:18:03,640 --> 00:18:09,800\\n그럴수록 이제 도크멘트 파싱과 리트리벌 단계가 매우 중요해지고 있고\\n\\n3\\n00:18:09,800 --> 00:18:12,000\\n그게 지금의 상태인 것 같습니다\\n\\n4\\n00:18:12,000 --> 00:18:15,280\\n그래서 이 리트리벌 쪽 성능이\\n\\n5\\n00:18:15,280 --> 00:18:20,940\\n실제 이제 제네레이티브 앤서에서\\n\\n6\\n00:18:20,940 --> 00:18:23,080\\n굉장히 중요한 컴포넌트가 되고 있고요\\n\\n7\\n00:18:23,080 --> 00:18:26,680\\n세상의 문서의 형태는 굉장히 다양하기 때문에\\n\\n8\\n00:18:26,680 --> 00:18:30,680\\n그거를 다루기 위한 노하우가 많이 필요하다\\n\\n9\\n00:18:30,680 --> 00:18:36,520\\n특히 기업용 문서에 대해서는 경험들이 많이 필요하다고 생각을 합니다\\n\\n10\\n00:18:36,520 --> 00:18:39,720\\n그리고 리트리버는 다룰 때\\n\\n11\\n00:18:39,720 --> 00:18:43,000\\n인터넷에 있는 것처럼 그냥 인베링 써서 하는 것보다는\\n\\n12\\n00:18:43,000 --> 00:18:44,639\\n앙상블 해야 되는 거고\\n\\n13\\n00:18:44,639 --> 00:18:50,080\\n앙상블로도 성능이 안 되는 것은 트레인을 해야 되는데\\n\\n14\\n00:18:50,080 --> 00:18:54,919\\n이게 실제 데이터는 굉장히 많고 지저분하고\\n\\n15\\n00:18:54,919 --> 00:18:57,480\\n업데이트의 제약이 많고 그렇기 때문에\\n\\n16\\n00:18:57,480 --> 00:19:00,040\\n학습할 수 있는 이런 형태 자체에\\n\\n\\n', '1\\n00:19:00,000 --> 00:19:02,800\\n많은 제약이 있을 수밖에 없다.\\n\\n2\\n00:19:02,800 --> 00:19:06,800\\n여기까지가 저희가 얻은 결론이고\\n\\n3\\n00:19:06,800 --> 00:19:08,800\\n그 내용을 발표 드렸습니다.\\n\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "print(transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 자막을 위해 영상 시간 분리 깔끔하게 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_transcripts(*transcripts):\n",
    "    merged_transcript = \"\"\n",
    "    current_number = 1\n",
    "\n",
    "    for transcript in transcripts:\n",
    "        # Split the transcript into segments\n",
    "        segments = transcript.strip().split(\"\\n\\n\")\n",
    "        for segment in segments:\n",
    "            # Split each segment into lines\n",
    "            lines = segment.split(\"\\n\")\n",
    "            # Replace the number at the beginning of each segment with the correct sequence number\n",
    "            lines[0] = str(current_number)\n",
    "            # Increment the sequence number\n",
    "            current_number += 1\n",
    "            # Reassemble the segment\n",
    "            merged_transcript += \"\\n\".join(lines) + \"\\n\\n\"\n",
    "\n",
    "    return merged_transcript.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자막 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:00,000 --> 00:00:16,440\n",
      "안녕하세요. 저는 오늘 RAG를 위한 리트리버 전략이라는 내용으로 발표를 하도록 하겠습니다.\n",
      "\n",
      "2\n",
      "00:00:16,440 --> 00:00:21,400\n",
      "저는 Organize에서 최고 AI 책임자를 맡고 있고요.\n",
      "\n",
      "3\n",
      "00:00:21,400 --> 00:00:27,600\n",
      "10몇 년 동안 백엔드 엔지니어로 일하다가 지금은 AI 이렇게 하고 있네요.\n",
      "\n",
      "4\n",
      "00:00:27,600 --> 00:00:34,040\n",
      "오늘 말씀드릴 내용은 RAG가 대충 뭔지 간략하게 설명드리고\n",
      "\n",
      "5\n",
      "00:00:34,040 --> 00:00:40,400\n",
      "지금 채취 PT나 이런 것 때문에 굉장히 LRM이 발전되어 있어서 말 되게 잘하는데\n",
      "\n",
      "6\n",
      "00:00:40,400 --> 00:00:46,959\n",
      "왜 아직도 답변은 제대로 안 나오는가? 그거에 대해서 좀 얘기를 해보려고 합니다.\n",
      "\n",
      "7\n",
      "00:00:46,959 --> 00:00:52,119\n",
      "그래서 RAG가 뭔지부터 좀 말씀드리도록 하겠습니다.\n",
      "\n",
      "8\n",
      "00:00:52,160 --> 00:00:55,680\n",
      "나온 것은 2021년에 맨 처음 나왔고요.\n",
      "\n",
      "9\n",
      "00:00:55,680 --> 00:00:59,959\n",
      "이때는 아직 GPT3 시절이죠.\n",
      "\n",
      "10\n",
      "00:01:00,000 --> 00:01:09,080\n",
      "그래서 이 제네레이션 쪽에서 그렇게 말이 되기는 하고는 있지만 그렇게 놀라운 성능을 발휘하지 못하는 시절이었는데\n",
      "\n",
      "11\n",
      "00:01:09,080 --> 00:01:18,280\n",
      "이제 오픈AI에서 모델을 발표하고 난 다음부터는 이 제네레이터의 성능이 엄청나게 높아지니까\n",
      "\n",
      "12\n",
      "00:01:18,280 --> 00:01:26,080\n",
      "얘가 이제 이거를 이용해서 어떻게 이제 기업 내부 문서 혹은 나만이 알고 있는 문서들에서 답변을 할 수 있을까\n",
      "\n",
      "13\n",
      "00:01:26,080 --> 00:01:32,759\n",
      "하는 고민들을 하기 시작했습니다 그래서 이제 사람들이 많이 채택하고 있는 방법이 RAZ 인데요\n",
      "\n",
      "14\n",
      "00:01:32,759 --> 00:01:40,599\n",
      "프롬프트에다가 그냥 내가 물어본 질문에 대한 답변 후보들을 프롬프트에다 집어 넣는 방식입니다\n",
      "\n",
      "15\n",
      "00:01:40,599 --> 00:01:49,840\n",
      "어려운 것은 개념상 어려운 것은 아니구요 그래서 그렇게만 잘 들어가면 이 놀라운 LLM이 답변을 잘 해주고 있는데요\n",
      "\n",
      "16\n",
      "00:01:49,840 --> 00:01:57,160\n",
      "문제는 그럼에도 불구하고 아직도 내 기업 내부 문서에서 답변을 해주는 것은 갈 길이 멀어 보인다\n",
      "\n",
      "17\n",
      "00:01:57,160 --> 00:02:00,560\n",
      "뭐 이런 느낌이 좀 있습니다\n",
      "\n",
      "18\n",
      "00:02:00,000 --> 00:02:05,060\n",
      "퍼플렉스틱 AI처럼 웹사이트에서 답변을 해주는 것도 성능이 놀라운데\n",
      "\n",
      "19\n",
      "00:02:05,060 --> 00:02:08,260\n",
      "유독 기업 내부 문서에서는 아직도 어려워요\n",
      "\n",
      "20\n",
      "00:02:08,260 --> 00:02:11,200\n",
      "근데 그 어려운 이유가 보통 또 찾아보면\n",
      "\n",
      "21\n",
      "00:02:11,200 --> 00:02:14,980\n",
      "리트리버 성능이 잘 안 나온다 뭐 이런 얘기가 있는데\n",
      "\n",
      "22\n",
      "00:02:14,980 --> 00:02:17,620\n",
      "저는 오늘 리트리버 얘기도 할 거고\n",
      "\n",
      "23\n",
      "00:02:17,620 --> 00:02:21,040\n",
      "그 다음에 흔히 강가되고 있는 도큐먼트 파싱에 관한 내용도\n",
      "\n",
      "24\n",
      "00:02:21,040 --> 00:02:22,940\n",
      "이야기를 좀 해보려고 합니다\n",
      "\n",
      "25\n",
      "00:02:23,940 --> 00:02:30,260\n",
      "이 발표 자료를 이제 한참 만들고 있었는데\n",
      "\n",
      "26\n",
      "00:02:30,260 --> 00:02:32,619\n",
      "저번주 화요일이었죠\n",
      "\n",
      "27\n",
      "00:02:32,619 --> 00:02:37,779\n",
      "오픈AI에서 리트리버, 어시스턴트 API의 리트리버를 발표했다\n",
      "\n",
      "28\n",
      "00:02:37,779 --> 00:02:40,340\n",
      "는 소식을 듣고 망했다\n",
      "\n",
      "29\n",
      "00:02:40,340 --> 00:02:42,419\n",
      "이렇게 생각했는데\n",
      "\n",
      "30\n",
      "00:02:42,419 --> 00:02:45,939\n",
      "그래서 바로 출근해서 바로 확인을 해봤죠\n",
      "\n",
      "31\n",
      "00:02:45,939 --> 00:02:49,660\n",
      "근데 보고 난 느낌은 뭐냐면\n",
      "\n",
      "32\n",
      "00:02:50,660 --> 00:02:55,299\n",
      "우리가 고민하고 있는 게 지금 남들도 여전히 어렵구나\n",
      "\n",
      "33\n",
      "00:02:55,299 --> 00:02:57,340\n",
      "라는 생각이 좀 들었고요\n",
      "\n",
      "34\n",
      "00:03:00,000 --> 00:03:03,760\n",
      "올 수 있었는데 날먹하지는 못했고요\n",
      "\n",
      "35\n",
      "00:03:03,760 --> 00:03:10,400\n",
      "어 이게 뭐 범령 관련된 문서 인터넷 다운받아 가지고\n",
      "\n",
      "36\n",
      "00:03:10,400 --> 00:03:16,400\n",
      "그 오픈 AI 어시스턴트 API 이용해서 넣어서 확인을 해본 결과인데요\n",
      "\n",
      "37\n",
      "00:03:16,400 --> 00:03:22,280\n",
      "전자 말찌 부착 명령에 뭐 어쩌고 저쩌고 물어보면\n",
      "\n",
      "38\n",
      "00:03:22,280 --> 00:03:24,840\n",
      "이 문서에 대해서 답을 해줘야 될 것 같은데\n",
      "\n",
      "39\n",
      "00:03:24,840 --> 00:03:26,840\n",
      "답을 제대로 못해주고 있어요\n",
      "\n",
      "40\n",
      "00:03:26,840 --> 00:03:31,520\n",
      "보면은 표에 있는 숫자 내용도 틀렸고요\n",
      "\n",
      "41\n",
      "00:03:31,520 --> 00:03:36,799\n",
      "전반적으로 숫자가 틀렸으니까 당연히 답변 내용은 틀렸습니다\n",
      "\n",
      "42\n",
      "00:03:36,799 --> 00:03:41,840\n",
      "왜 이런 일이 벌어질까요? 무려 오픈 AI인데\n",
      "\n",
      "43\n",
      "00:03:41,840 --> 00:03:48,320\n",
      "먼저 우리가 다루는 문서에 대해서 좀 한번 좀 먼저 고려를 해봅시다\n",
      "\n",
      "44\n",
      "00:03:48,320 --> 00:03:52,360\n",
      "주로 기업 내부 문서라면 pdf 혹은 워드 문서\n",
      "\n",
      "45\n",
      "00:03:52,360 --> 00:03:58,680\n",
      "엑셀 아래 앙글 등등 있겠지만 요 3개를 다 비슷해 보여요\n",
      "\n",
      "46\n",
      "00:03:58,680 --> 00:03:59,880\n",
      "pdf 같은 경우에는\n",
      "\n",
      "47\n",
      "00:04:00,000 --> 00:04:04,360\n",
      "얘가 문서를 작성하기 위한 시스템이 아니에요\n",
      "\n",
      "48\n",
      "00:04:04,360 --> 00:04:06,840\n",
      "조판 시스템, 출력용 시스템\n",
      "\n",
      "49\n",
      "00:04:06,840 --> 00:04:09,440\n",
      "그러니까 이렇게 우리 눈에 이렇게 보인다 그래서\n",
      "\n",
      "50\n",
      "00:04:09,440 --> 00:04:13,760\n",
      "글이 절대 그 순서대로 있을 거라는 보장이 없습니다\n",
      "\n",
      "51\n",
      "00:04:13,760 --> 00:04:18,280\n",
      "그래서 다단 같은 경우에 깨지기도 하고 그렇습니다\n",
      "\n",
      "52\n",
      "00:04:18,280 --> 00:04:20,120\n",
      "레이아웃을 잘 알 수 있으면 좋겠는데\n",
      "\n",
      "53\n",
      "00:04:20,120 --> 00:04:22,240\n",
      "레이아웃은 잘 몰라요\n",
      "\n",
      "54\n",
      "00:04:22,240 --> 00:04:25,480\n",
      "그 다음에 헤더프터 정보도 없습니다\n",
      "\n",
      "55\n",
      "00:04:25,480 --> 00:04:32,279\n",
      "그리고 목차 같은 경우에 가끔씩 메타 정보로 있기도 하지만\n",
      "\n",
      "56\n",
      "00:04:32,279 --> 00:04:34,480\n",
      "없는 경우가 대부분이고요\n",
      "\n",
      "57\n",
      "00:04:34,480 --> 00:04:37,040\n",
      "테이블 같은 경우는 그냥 글씨에요\n",
      "\n",
      "58\n",
      "00:04:37,040 --> 00:04:39,680\n",
      "그냥 글씨에다가 그림 입혀놓은 겁니다\n",
      "\n",
      "59\n",
      "00:04:39,680 --> 00:04:42,320\n",
      "그래서 절대 테이블이라는 것도 알 수가 없어요\n",
      "\n",
      "60\n",
      "00:04:42,320 --> 00:04:46,520\n",
      "이런 문서를 우리는 다뤄야 해요\n",
      "\n",
      "61\n",
      "00:04:46,520 --> 00:04:49,240\n",
      "워드로 가면 좀 낫냐? 아무래도 좀 낫죠\n",
      "\n",
      "62\n",
      "00:04:49,240 --> 00:04:51,720\n",
      "왜냐하면 얘는 조판 시스템이 아니니까요\n",
      "\n",
      "63\n",
      "00:04:51,720 --> 00:04:57,320\n",
      "근데 현실에서 우리한테 들어오는 기업용 문서는\n",
      "\n",
      "64\n",
      "00:04:57,320 --> 00:05:00,000\n",
      "사실상 조파는 시스템처럼 써요\n",
      "\n",
      "65\n",
      "00:05:00,000 --> 00:05:07,320\n",
      "그래서 뭐 워드 문서를 열어보시면 위에 제목도 있고 뭐 이렇게 시스템적으로 잘 되어 있죠\n",
      "\n",
      "66\n",
      "00:05:07,320 --> 00:05:12,160\n",
      "그걸로 스타일 잘 맞춰서 문서를 작성을 안합니다\n",
      "\n",
      "67\n",
      "00:05:12,160 --> 00:05:20,280\n",
      "그런거랑 상관없이 그냥 눈에 보이기만 그렇게 보이게 만든 문서들이 상당히 많구요\n",
      "\n",
      "68\n",
      "00:05:20,639 --> 00:05:29,799\n",
      "테이블 같은 경우에도 머지 드 세일이 많죠 그리고 이쁘게 얼라인을 맞추기 위해서 그냥 테이블이 아닌데 테이블 안에다가\n",
      "\n",
      "69\n",
      "00:05:29,799 --> 00:05:35,160\n",
      "글씨를 집어 넣기도 하죠 자 그러면 태생이 테이블인 엑셀은 어떠냐\n",
      "\n",
      "70\n",
      "00:05:35,160 --> 00:05:41,799\n",
      "얘도 마찬가지에요 테이블이 아니에요 그래서 요런 문서들을 가지고 우리는 답변을 해야 됩니다\n",
      "\n",
      "71\n",
      "00:05:41,799 --> 00:05:49,200\n",
      "글이 어디 있는지도 모르고 이게 테이블인데 테이블인지도 모르고 깜깜한 상태에서 이제 찾아봐야 되는 거죠\n",
      "\n",
      "72\n",
      "00:05:49,200 --> 00:05:57,480\n",
      "이런 문제점을 오픈AI도 똑같이 겪고 있었을 거라고 생각을 하고 이제 몇몇 뭐\n",
      "\n",
      "73\n",
      "00:05:57,480 --> 00:06:01,439\n",
      "몇몇 문서만 올린 시스템에서는\n",
      "\n",
      "74\n",
      "00:06:00,000 --> 00:06:02,000\n",
      "뭐 이렇게 해도 잘 동작을 하겠죠\n",
      "\n",
      "75\n",
      "00:06:02,000 --> 00:06:04,400\n",
      "근데 아까 봤던 것들은\n",
      "\n",
      "76\n",
      "00:06:04,400 --> 00:06:06,400\n",
      "citation이 이제 고치겠지만\n",
      "\n",
      "77\n",
      "00:06:06,400 --> 00:06:09,660\n",
      "깨져서 나와서 그걸로 테스트는 못해봤고\n",
      "\n",
      "78\n",
      "00:06:09,660 --> 00:06:12,700\n",
      "citation이 깨지지 않은 예제를 좀 찾아봤어요\n",
      "\n",
      "79\n",
      "00:06:12,700 --> 00:06:16,559\n",
      "그래서 CUDA API 문서를 올려놓고\n",
      "\n",
      "80\n",
      "00:06:16,559 --> 00:06:18,980\n",
      "질문을 해봤더니 citation이 나오더라구요\n",
      "\n",
      "81\n",
      "00:06:18,980 --> 00:06:21,580\n",
      "그래서 citation이 어떻게 잡혔나 이렇게 딱 봤더니\n",
      "\n",
      "82\n",
      "00:06:21,580 --> 00:06:26,120\n",
      "문서의 그 제목...\n",
      "\n",
      "83\n",
      "00:06:26,120 --> 00:06:28,980\n",
      "아 제목이 아니라 목차 부분을 찾았어요\n",
      "\n",
      "84\n",
      "00:06:29,200 --> 00:06:31,520\n",
      "우리가 원하는 대답은 목차에 있진 않겠죠\n",
      "\n",
      "85\n",
      "00:06:31,520 --> 00:06:33,520\n",
      "어쨌든 목차 부분을 찾았고\n",
      "\n",
      "86\n",
      "00:06:33,520 --> 00:06:36,919\n",
      "그 다음에 페이지 번호나 이런 건 제거했는데\n",
      "\n",
      "87\n",
      "00:06:36,919 --> 00:06:39,279\n",
      "이게 전체 페이지가 아니고 일부분만 잘랐고\n",
      "\n",
      "88\n",
      "00:06:39,279 --> 00:06:42,119\n",
      "자른 그 일부분 페이지도\n",
      "\n",
      "89\n",
      "00:06:42,119 --> 00:06:45,439\n",
      "의미가 있게 자르지는 않았더라구요\n",
      "\n",
      "90\n",
      "00:06:45,439 --> 00:06:48,680\n",
      "그렇게 자르면 잘 안됩니다\n",
      "\n",
      "91\n",
      "00:06:48,680 --> 00:06:51,959\n",
      "당연하게도 잘 안되겠죠\n",
      "\n",
      "92\n",
      "00:06:51,959 --> 00:06:54,279\n",
      "그래서 우리 시스템에다가\n",
      "\n",
      "93\n",
      "00:06:54,279 --> 00:06:58,240\n",
      "저희 제품에다가 했는데 다행히 잘 나왔어요\n",
      "\n",
      "94\n",
      "00:06:58,240 --> 00:07:00,020\n",
      "이렇게 나오기 위해서는\n",
      "\n",
      "95\n",
      "00:07:00,000 --> 00:07:06,840\n",
      "지금 잠깐 잠깐 말씀드렸다시피 이게 문서를 좀 이해할 수 있는 방법이 필요합니다\n",
      "\n",
      "96\n",
      "00:07:06,840 --> 00:07:13,300\n",
      "그리고 그 이해할 수 있는 방법이 굉장히 많은 노가다와 꼼수와 이런 것들도 점처리 되어 있는데\n",
      "\n",
      "97\n",
      "00:07:13,300 --> 00:07:18,260\n",
      "그 중에서 테이블을 어떻게 다룰 것인가 이 부분을 많이 물어보셔가지고\n",
      "\n",
      "98\n",
      "00:07:18,260 --> 00:07:20,900\n",
      "테이블 쪽 얘기를 좀 해보려고 합니다 먼저\n",
      "\n",
      "99\n",
      "00:07:20,900 --> 00:07:27,620\n",
      "오픈 AI 에다가 넣을 때 테이블은 MD로 넣으면은 잘 된다\n",
      "\n",
      "100\n",
      "00:07:27,959 --> 00:07:30,260\n",
      "이거는 많이 알려져 있을 겁니다\n",
      "\n",
      "101\n",
      "00:07:30,260 --> 00:07:38,099\n",
      "그래서 아까 그 문서 그대로 좀 간략하게 하긴 했지만 MD로 넣어놓고 질문을 하면 질문이 잘 됩니다\n",
      "\n",
      "102\n",
      "00:07:38,099 --> 00:07:42,740\n",
      "자 근데 모든 걸 MD로 하면 진짜 잘 될까요?\n",
      "\n",
      "103\n",
      "00:07:42,740 --> 00:07:49,299\n",
      "실험을 해보신 분들은 아시겠지만 테이블이 조금만 커져도 얘가 헷갈리기 시작합니다\n",
      "\n",
      "104\n",
      "00:07:49,299 --> 00:07:56,619\n",
      "그리고 실제 이제 우리가 다루는 문서에서는 테이블 안에 이렇게 숫자만 있는 테이블도 있겠지만\n",
      "\n",
      "105\n",
      "00:07:56,619 --> 00:07:59,900\n",
      "거기에 글이 있는 경우들도 있고 글에는 당연히\n",
      "\n",
      "106\n",
      "00:08:00,000 --> 00:08:04,320\n",
      "멘터라든지 아니면 이렇게 수직파 같은 특수 문자들이 들어가고\n",
      "\n",
      "107\n",
      "00:08:04,320 --> 00:08:06,840\n",
      "그런 것 때문에 MD의 포맷이 깨집니다.\n",
      "\n",
      "108\n",
      "00:08:06,840 --> 00:08:11,360\n",
      "이제 그런 것들을 다루면은 이제 더 이상 MD가 MD가 아니에요.\n",
      "\n",
      "109\n",
      "00:08:11,360 --> 00:08:15,800\n",
      "그래서 요거를 그럼 어떻게 처리해야 되지?\n",
      "\n",
      "110\n",
      "00:08:15,800 --> 00:08:17,400\n",
      "를 고민을 하다가\n",
      "\n",
      "111\n",
      "00:08:17,400 --> 00:08:20,920\n",
      "그럼 테이블을 그냥 말처럼 써볼까?\n",
      "\n",
      "112\n",
      "00:08:20,920 --> 00:08:23,600\n",
      "해서 테이블로 자연어를 만들었어요.\n",
      "\n",
      "113\n",
      "00:08:23,600 --> 00:08:27,600\n",
      "그래서 요 테이블을 그냥 말로 이렇게 쭉 설명을 한 거죠.\n",
      "\n",
      "114\n",
      "00:08:28,120 --> 00:08:32,200\n",
      "헤더를 보고 연도는 2008년이고\n",
      "\n",
      "115\n",
      "00:08:32,200 --> 00:08:35,080\n",
      "이렇게 쭉 헤더만 알 수 있다면\n",
      "\n",
      "116\n",
      "00:08:35,080 --> 00:08:38,279\n",
      "저렇게 말을 만들어내는 건 기계적으로 만들어낼 수 있겠죠.\n",
      "\n",
      "117\n",
      "00:08:38,279 --> 00:08:41,840\n",
      "저런 식의 기계적으로 자연어로 만들었을 때\n",
      "\n",
      "118\n",
      "00:08:41,840 --> 00:08:43,680\n",
      "답변 역시 훌륭합니다.\n",
      "\n",
      "119\n",
      "00:08:43,680 --> 00:08:45,840\n",
      "저렇게 했을 때의 장점은 뭐냐면\n",
      "\n",
      "120\n",
      "00:08:45,840 --> 00:08:49,880\n",
      "Merged Cell이나 아니면 큰 테이블 혹은\n",
      "\n",
      "121\n",
      "00:08:49,880 --> 00:08:52,840\n",
      "좀 복잡해진 것들에 대해서도 얼마든지\n",
      "\n",
      "122\n",
      "00:08:52,840 --> 00:08:56,799\n",
      "헤더만 제대로 잡을 수 있다면 처리가 가능하다는 거죠.\n",
      "\n",
      "123\n",
      "00:08:56,799 --> 00:08:59,919\n",
      "그리고 후술하겠지만 리트리버에서도 요렇게\n",
      "\n",
      "124\n",
      "00:09:00,000 --> 00:09:04,520\n",
      "했을 때 훨씬 잘 잡는 경우들이 있구요\n",
      "\n",
      "125\n",
      "00:09:04,520 --> 00:09:07,520\n",
      "그래서 테이블을 다루기 위해서는\n",
      "\n",
      "126\n",
      "00:09:07,520 --> 00:09:12,520\n",
      "그 문서를 이제 이해하는데 필요한 일부 기술들이지만\n",
      "\n",
      "127\n",
      "00:09:12,520 --> 00:09:15,200\n",
      "이렇게 많은 기술들이 들어갈 수밖에 없어요\n",
      "\n",
      "128\n",
      "00:09:15,200 --> 00:09:18,160\n",
      "아까 말씀드렸다시피 테이블이 테이블이 아니라 그랬잖아요\n",
      "\n",
      "129\n",
      "00:09:18,160 --> 00:09:20,959\n",
      "그래서 테이블이 어디 있는지를 확인을 해야 되구요\n",
      "\n",
      "130\n",
      "00:09:20,959 --> 00:09:27,639\n",
      "그 다음에 그 영역을 셀별로 잘 뜯어서 셀로 잘 구축을 해야 되구요\n",
      "\n",
      "131\n",
      "00:09:27,639 --> 00:09:33,040\n",
      "그게 만약에 간단하다라고 판단이 들었다면 마크다운으로 처리하면 간단해지지만\n",
      "\n",
      "132\n",
      "00:09:33,040 --> 00:09:38,799\n",
      "만약에 복잡하다라고 판단이 되면 거기서 어디가 헤더인지를 찾아야 돼요\n",
      "\n",
      "133\n",
      "00:09:38,799 --> 00:09:45,439\n",
      "간단히 생각하면 첫 열, 첫 번째 컬러가 헤더가 아니겠냐 라고 하지만\n",
      "\n",
      "134\n",
      "00:09:45,439 --> 00:09:48,160\n",
      "세상은 그렇게 호락호락하지 않습니다\n",
      "\n",
      "135\n",
      "00:09:48,160 --> 00:09:53,279\n",
      "머지드셀도 있고 헤더가 두 개 세 개 이렇게 된 데도 있고\n",
      "\n",
      "136\n",
      "00:09:53,279 --> 00:09:56,480\n",
      "열 중간에 들어가는 것도 있고\n",
      "\n",
      "137\n",
      "00:09:56,480 --> 00:09:59,439\n",
      "물론 이거를 완벽하게 잘 했다면\n",
      "\n",
      "138\n",
      "00:10:00,000 --> 00:10:05,040\n",
      "큰 기업이 됐겠지만 저희도 열심히 실제 더러운 데이터들이랑 싸우면서\n",
      "\n",
      "139\n",
      "00:10:05,040 --> 00:10:07,140\n",
      "노하우를 열심히 쌓고 있습니다\n",
      "\n",
      "140\n",
      "00:10:07,140 --> 00:10:12,280\n",
      "굉장히 어려운 일이다 라는 것을 말씀드리고 싶은 거고요\n",
      "\n",
      "141\n",
      "00:10:12,280 --> 00:10:17,680\n",
      "이제 리트리버 얘기를 또 하겠습니다\n",
      "\n",
      "142\n",
      "00:10:17,680 --> 00:10:22,680\n",
      "그렇게 해서 문서를 적당한 크기로 자르고\n",
      "\n",
      "143\n",
      "00:10:22,680 --> 00:10:26,320\n",
      "그 다음에 뭐 구조와 unstructured 데이터를\n",
      "\n",
      "144\n",
      "00:10:26,320 --> 00:10:30,280\n",
      "unstructured을 잘 이해할 수 있도록 만들고 다 좋았어요\n",
      "\n",
      "145\n",
      "00:10:30,280 --> 00:10:35,939\n",
      "그러면 질문을 하면 이제 질문에 답변이 될 만한 것들을 찾아서\n",
      "\n",
      "146\n",
      "00:10:35,939 --> 00:10:38,119\n",
      "프롬프트에다 집어 넣어 줘야 되죠\n",
      "\n",
      "147\n",
      "00:10:38,119 --> 00:10:45,080\n",
      "근데 답변이 될 만한 걸 어떻게 찾을 수 있을까요\n",
      "\n",
      "148\n",
      "00:10:45,080 --> 00:10:48,439\n",
      "가장 간단한 건 그냥 es 에다 물어보는 거죠\n",
      "\n",
      "149\n",
      "00:10:48,439 --> 00:10:56,680\n",
      "es 에다 물어보는 것은 뭐 그냥 등장한 키워드가 나오는 거니까\n",
      "\n",
      "150\n",
      "00:10:56,680 --> 00:10:58,160\n",
      "그냥 직관적으로 이해가 가죠\n",
      "\n",
      "151\n",
      "00:10:58,160 --> 00:11:01,160\n",
      "근데 질문이 꼭 키워드가\n",
      "\n",
      "152\n",
      "00:11:00,000 --> 00:11:02,000\n",
      "코드가 나온 데서만 나올 수는 없잖아요\n",
      "\n",
      "153\n",
      "00:11:02,000 --> 00:11:06,320\n",
      "그런 이런 스팟스 리트리버의 한계도 물론 명확하고\n",
      "\n",
      "154\n",
      "00:11:06,320 --> 00:11:10,080\n",
      "그래서 댄스 리트리버들이 이제 나왔죠\n",
      "\n",
      "155\n",
      "00:11:10,080 --> 00:11:14,200\n",
      "dpr이나 오픈웨어 인베딩이나 이런 것들을 써서\n",
      "\n",
      "156\n",
      "00:11:14,200 --> 00:11:18,000\n",
      "그런 것들도 보면은 비슷비슷합니다\n",
      "\n",
      "157\n",
      "00:11:18,000 --> 00:11:20,920\n",
      "어떤 데이터셋은 bm25가 이기고\n",
      "\n",
      "158\n",
      "00:11:20,920 --> 00:11:22,240\n",
      "어떤 건 자기네가 이기고\n",
      "\n",
      "159\n",
      "00:11:22,240 --> 00:11:24,799\n",
      "어떤 건 앙상블 했더니 다 이기고\n",
      "\n",
      "160\n",
      "00:11:24,799 --> 00:11:28,080\n",
      "실제 고객 문서로 가면 비슷해요\n",
      "\n",
      "161\n",
      "00:11:28,799 --> 00:11:32,080\n",
      "어떤 때는 얘가 이기고 어떤 때는 얘가 이기고 그렇습니다\n",
      "\n",
      "162\n",
      "00:11:32,080 --> 00:11:35,680\n",
      "그거를 보여줄 수 있는 내용이 요건데요\n",
      "\n",
      "163\n",
      "00:11:35,680 --> 00:11:39,639\n",
      "이제 저희가 이제 고객들의 공개는 안되겠지만\n",
      "\n",
      "164\n",
      "00:11:39,639 --> 00:11:43,439\n",
      "고객들의 문서와 고객들의 질문들을 수집해서\n",
      "\n",
      "165\n",
      "00:11:43,439 --> 00:11:45,080\n",
      "벤치마크를 해봤습니다\n",
      "\n",
      "166\n",
      "00:11:45,080 --> 00:11:47,560\n",
      "그래서 파란색은 bm25\n",
      "\n",
      "167\n",
      "00:11:47,560 --> 00:11:49,080\n",
      "빨간색은 그냥 인베딩\n",
      "\n",
      "168\n",
      "00:11:49,080 --> 00:11:51,119\n",
      "여기는 오픈웨어 인베딩으로 했지만\n",
      "\n",
      "169\n",
      "00:11:51,119 --> 00:11:54,880\n",
      "인베딩 그 다음에 노란색은 앙상블한 거\n",
      "\n",
      "170\n",
      "00:11:54,880 --> 00:11:57,360\n",
      "그 다음에 초록색은 제목까지 앙상블한 거\n",
      "\n",
      "171\n",
      "00:11:57,360 --> 00:11:58,000\n",
      "이렇게 해봤습니다\n",
      "\n",
      "172\n",
      "00:11:58,000 --> 00:11:59,799\n",
      "천차만별이죠\n",
      "\n",
      "173\n",
      "00:12:00,000 --> 00:12:08,840\n",
      "어렵습니다. 그래서 이 고객 문서와 질문에 따라서 이 웨이트를 자동으로 조정할 필요가 있어요\n",
      "\n",
      "174\n",
      "00:12:08,840 --> 00:12:15,040\n",
      "그런 것들을 어떻게 하는지도 실제 세계에서는 어려운 일이구요\n",
      "\n",
      "175\n",
      "00:12:15,040 --> 00:12:24,440\n",
      "여기까지 했다면 이제 어 그러면 뭐 dpr 이든지 뭐든지 이런 리트리버를 학습을 시켜보면 더 성능이 좋아질 수 있지 않을까\n",
      "\n",
      "176\n",
      "00:12:24,440 --> 00:12:32,480\n",
      "농물 많잖아요. 댄스리트리브에 관한 농물. 자 근데 이거의 실제 세계 보면 또 어느 문제가 있냐면\n",
      "\n",
      "177\n",
      "00:12:32,480 --> 00:12:40,160\n",
      "자 이런 거를 학습을 시키기 위해서 뭐 크게 두 가지 방법이 있을 겁니다. 하나는 크로스 인코더 하나는 바이 인코더\n",
      "\n",
      "178\n",
      "00:12:40,160 --> 00:12:50,520\n",
      "잠깐 설명 드리자면 크로스 인코더는 질문과 컨텍스트가 한 개의 모델로 들어가서 답변이 나오는 거고 바이 인코더는 각각 들어가서\n",
      "\n",
      "179\n",
      "00:12:50,520 --> 00:12:56,320\n",
      "각각 계산된 다음에 나중에 합쳐지는 방식인데요 크로스 인코더는 실제로 쓸 수가 없어요\n",
      "\n",
      "180\n",
      "00:12:56,320 --> 00:13:01,520\n",
      "왜냐하면은 이 컨텍스트가 수십만 개\n",
      "\n",
      "181\n",
      "00:13:00,000 --> 00:13:06,600\n",
      "수백만개인데 이거를 실시간에 질문 하나 할 때마다 이걸 모델에 다 태운다 이건 불가능한 일이잖아요\n",
      "\n",
      "182\n",
      "00:13:06,600 --> 00:13:11,000\n",
      "그래서 애초에 크로스 인코더는 고려대상이 아닙니다\n",
      "\n",
      "183\n",
      "00:13:11,400 --> 00:13:16,559\n",
      "바이 인코더라고 해서 그러면은 모델을 잘 학습을 할 수 있을까요?\n",
      "\n",
      "184\n",
      "00:13:16,559 --> 00:13:22,200\n",
      "아까랑 비슷한 문제가 발생을 해요 자 우리가 데이터를 열심히 모아서 좋은 모델을 만들었어요\n",
      "\n",
      "185\n",
      "00:13:22,200 --> 00:13:26,480\n",
      "그래서 짠 하고 배포를 해서 문서를 짜고 그러고 와 성능이 괜찮다\n",
      "\n",
      "186\n",
      "00:13:26,480 --> 00:13:32,279\n",
      "절대 세상은 그렇게 끝날 일이 없죠 아쉬움이 발생하고 데이터는 더 쌓이고 또 학습을 시킬 겁니다\n",
      "\n",
      "187\n",
      "00:13:32,279 --> 00:13:40,080\n",
      "그러면 10만개 되는 컨텍스트 벡터 100만개 되는 컨텍스트 벡터를 다 업데이트를 해줘야 될까요?\n",
      "\n",
      "188\n",
      "00:13:40,080 --> 00:13:48,000\n",
      "사실상 힘들죠 그래서 컨텍스트 쪽 벡터 역시 학습이 불가능합니다\n",
      "\n",
      "189\n",
      "00:13:48,560 --> 00:13:51,599\n",
      "그래서 이런 모양이 될 수밖에 없어요\n",
      "\n",
      "190\n",
      "00:13:51,599 --> 00:14:00,880\n",
      "퀘스천만 학습이 될 수 있는, 되게 기형적으로 보이는데 학습이 또 그 문제만 있는 건 아니에요\n",
      "\n",
      "191\n",
      "00:14:00,000 --> 00:14:02,400\n",
      "학습 데이터를 만들어야 되는데\n",
      "\n",
      "192\n",
      "00:14:02,400 --> 00:14:04,680\n",
      "이게 질문이\n",
      "\n",
      "193\n",
      "00:14:04,680 --> 00:14:10,280\n",
      "여기서 정답 세트는 질문과 이 질문의 후보입니다\n",
      "\n",
      "194\n",
      "00:14:10,280 --> 00:14:14,780\n",
      "근데 달걀과 닭과 달걀이 문제인데\n",
      "\n",
      "195\n",
      "00:14:14,780 --> 00:14:17,900\n",
      "이 리트리버가 굉장히 똑똑하다면\n",
      "\n",
      "196\n",
      "00:14:17,900 --> 00:14:19,580\n",
      "대부분을 맞출 거고\n",
      "\n",
      "197\n",
      "00:14:19,580 --> 00:14:21,840\n",
      "그러면 틀린 것만 걸러내도\n",
      "\n",
      "198\n",
      "00:14:21,840 --> 00:14:26,120\n",
      "굉장히 좋은 이런 답변 세트를 만들어낼 수 있을 거예요\n",
      "\n",
      "199\n",
      "00:14:26,120 --> 00:14:28,120\n",
      "근데 성능이 안 좋아서 학습을 시키려고\n",
      "\n",
      "200\n",
      "00:14:28,120 --> 00:14:31,320\n",
      "학습 데이터를 만드는데 그렇게 쓸 수는 없겠죠\n",
      "\n",
      "201\n",
      "00:14:31,320 --> 00:14:34,160\n",
      "근데 사람한테 질문을 만들라 그러면 엄청 힘들겠죠\n",
      "\n",
      "202\n",
      "00:14:34,160 --> 00:14:35,720\n",
      "근데 요새 세상은\n",
      "\n",
      "203\n",
      "00:14:35,720 --> 00:14:38,400\n",
      "GPT님이 다 해주시니까\n",
      "\n",
      "204\n",
      "00:14:38,400 --> 00:14:40,480\n",
      "GPT로 데이터를 생성할 수 있습니다\n",
      "\n",
      "205\n",
      "00:14:40,480 --> 00:14:41,799\n",
      "오, 되게 좋아요\n",
      "\n",
      "206\n",
      "00:14:41,799 --> 00:14:42,959\n",
      "그래서 생성을 했어요\n",
      "\n",
      "207\n",
      "00:14:42,959 --> 00:14:45,639\n",
      "근데 네거티브 데이터는 어떡하지?\n",
      "\n",
      "208\n",
      "00:14:45,639 --> 00:14:48,080\n",
      "GPT로 네거티브 데이터를 만들라 그러면\n",
      "\n",
      "209\n",
      "00:14:48,080 --> 00:14:50,919\n",
      "그러니까 페이지를 주고\n",
      "\n",
      "210\n",
      "00:14:50,919 --> 00:14:55,759\n",
      "이 페이지에서 답변이 나오지 않을 만한 질문을 생성을 해봐\n",
      "\n",
      "211\n",
      "00:14:55,759 --> 00:14:57,840\n",
      "죽어도 못합니다\n",
      "\n",
      "212\n",
      "00:14:57,840 --> 00:14:59,160\n",
      "안 돼요, 잘\n",
      "\n",
      "213\n",
      "00:14:59,160 --> 00:15:01,160\n",
      "나중에 되면...\n",
      "\n",
      "214\n",
      "00:15:00,000 --> 00:15:04,000\n",
      "할 수 있겠죠 근데 적어도 지금은 잘 안 돼요 gpt4 안됩니다\n",
      "\n",
      "215\n",
      "00:15:04,000 --> 00:15:07,000\n",
      "그래서 포지티브 데이터는 대량으로 생성이 가능한데\n",
      "\n",
      "216\n",
      "00:15:07,000 --> 00:15:10,000\n",
      "네거티브 데이터를 생성하기가 어려워요\n",
      "\n",
      "217\n",
      "00:15:10,000 --> 00:15:15,000\n",
      "또 다시 생각하는 게 1번 페이지에서 만든 정답을\n",
      "\n",
      "218\n",
      "00:15:15,000 --> 00:15:19,000\n",
      "2번 페이지에 네거티브 샘플로 쓸 수 있지 않을까?\n",
      "\n",
      "219\n",
      "00:15:19,000 --> 00:15:21,000\n",
      "네 그렇지 않는 경우가 많습니다\n",
      "\n",
      "220\n",
      "00:15:21,000 --> 00:15:25,000\n",
      "그걸 사람 눈으로 하기에는 필요한 데이터가 너무 많고요\n",
      "\n",
      "221\n",
      "00:15:25,000 --> 00:15:27,000\n",
      "그래서 웬만하면 포지티브 데이터만 가지고\n",
      "\n",
      "222\n",
      "00:15:27,000 --> 00:15:29,000\n",
      "학습을 할 수 있었으면 좋겠어요\n",
      "\n",
      "223\n",
      "00:15:29,000 --> 00:15:33,000\n",
      "그렇다 보니까 로스 펑션은 저렇게 될 수밖에 없습니다\n",
      "\n",
      "224\n",
      "00:15:33,000 --> 00:15:39,000\n",
      "이거를 그냥 바이너리 크로스 엔트로피 같은 경우로 사용을 하면\n",
      "\n",
      "225\n",
      "00:15:39,000 --> 00:15:41,000\n",
      "모델이 망가질 수밖에 없어요\n",
      "\n",
      "226\n",
      "00:15:41,000 --> 00:15:43,000\n",
      "왜냐면 포지티브밖에 없으니까\n",
      "\n",
      "227\n",
      "00:15:43,000 --> 00:15:46,000\n",
      "그래서 벡터끼리 비교하는 mse나\n",
      "\n",
      "228\n",
      "00:15:46,000 --> 00:15:49,000\n",
      "코사인 인베링 로스 같은 걸 쓸 수밖에 없어요\n",
      "\n",
      "229\n",
      "00:15:49,000 --> 00:15:50,000\n",
      "되게 논리적인 비주얼이죠\n",
      "\n",
      "230\n",
      "00:15:50,000 --> 00:15:53,000\n",
      "근데 이게 동작을 할 것 같지가 않잖아요\n",
      "\n",
      "231\n",
      "00:15:53,000 --> 00:15:54,000\n",
      "암만 생각해도\n",
      "\n",
      "232\n",
      "00:15:54,000 --> 00:15:56,000\n",
      "되게 삐꾸고\n",
      "\n",
      "233\n",
      "00:15:56,000 --> 00:16:00,000\n",
      "그래서 실제로 해봤습니다\n",
      "\n",
      "234\n",
      "00:16:00,000 --> 00:16:05,360\n",
      "보시면 빨간색이 퀘스천 임베딩이고\n",
      "\n",
      "235\n",
      "00:16:05,360 --> 00:16:09,280\n",
      "다시 한번 말씀드리지만 베이스라인은 오픈 AI 임베딩을 썼습니다\n",
      "\n",
      "236\n",
      "00:16:09,280 --> 00:16:12,480\n",
      "이유는 그게 성능이 제일 좋아서가 아니라\n",
      "\n",
      "237\n",
      "00:16:12,480 --> 00:16:16,000\n",
      "컨텍스트 윈도우가 제일 길어서\n",
      "\n",
      "238\n",
      "00:16:16,000 --> 00:16:20,280\n",
      "빨간색은 질문에 대한 임베딩이고요\n",
      "\n",
      "239\n",
      "00:16:20,280 --> 00:16:24,280\n",
      "파란색은 이제 문서 컨셉트에 대한 임베딩입니다\n",
      "\n",
      "240\n",
      "00:16:24,600 --> 00:16:29,959\n",
      "이거를 이제 2차원으로 PCA를 해 본 거고요\n",
      "\n",
      "241\n",
      "00:16:29,959 --> 00:16:31,120\n",
      "명확하게 갈려 있죠\n",
      "\n",
      "242\n",
      "00:16:31,120 --> 00:16:36,119\n",
      "그래서 빨간색이 저 파란색 쪽으로 보낼 수 있다면\n",
      "\n",
      "243\n",
      "00:16:36,119 --> 00:16:40,599\n",
      "이 모델이 워킹 할 거다 라는 생각이 든 거고\n",
      "\n",
      "244\n",
      "00:16:40,599 --> 00:16:45,480\n",
      "두 번째 그림이 실제로 초록색이 모델을 통과한 퀘스천 벡터들입니다\n",
      "\n",
      "245\n",
      "00:16:45,480 --> 00:16:49,680\n",
      "워킹을 하는 것 같습니다\n",
      "\n",
      "246\n",
      "00:16:49,680 --> 00:16:52,080\n",
      "그럼에도 불구하고\n",
      "\n",
      "247\n",
      "00:16:52,080 --> 00:16:55,759\n",
      "이게 왜 되지? 혹은 이게 맞는 건가?\n",
      "\n",
      "248\n",
      "00:16:55,759 --> 00:16:57,599\n",
      "라는 의문을 떨칠 수는 없는데\n",
      "\n",
      "249\n",
      "00:16:57,599 --> 00:17:00,000\n",
      "어쨌든 실제로 되니까 우리가\n",
      "\n",
      "250\n",
      "00:17:00,000 --> 00:17:03,840\n",
      "고차원의 세계는 상상하기 참 힘들죠\n",
      "\n",
      "251\n",
      "00:17:03,840 --> 00:17:06,880\n",
      "그래서 결과를 요약하면 이렇게 얻었습니다\n",
      "\n",
      "252\n",
      "00:17:06,880 --> 00:17:11,800\n",
      "이것 역시 그 데이터셋에 따라서 또 많이 달라져요\n",
      "\n",
      "253\n",
      "00:17:11,800 --> 00:17:14,960\n",
      "근데 그 중에 한 데이터셋인 거고요\n",
      "\n",
      "254\n",
      "00:17:14,960 --> 00:17:19,760\n",
      "탑3 기준으로 실제로는 탑3나 탑5를 넘어갈 수가 없어요\n",
      "\n",
      "255\n",
      "00:17:19,760 --> 00:17:25,040\n",
      "다른 논문에 보면은 막 탑 100 뭐 이런 것들을 쓰긴 하는데\n",
      "\n",
      "256\n",
      "00:17:25,040 --> 00:17:26,920\n",
      "우리는 프롬프트에 넣어 줘야 되는데\n",
      "\n",
      "257\n",
      "00:17:26,920 --> 00:17:31,879\n",
      "프롬프트에다가 물론 지금은 128k gpt4가 나왔기 때문에\n",
      "\n",
      "258\n",
      "00:17:31,879 --> 00:17:34,880\n",
      "이제 100페이지 집어 넣을 수 있을지 모르겠지만\n",
      "\n",
      "259\n",
      "00:17:34,880 --> 00:17:37,080\n",
      "어쨌든 그런 돈이 참 비싸잖아요\n",
      "\n",
      "260\n",
      "00:17:37,080 --> 00:17:40,400\n",
      "그래서 현실적으로 탑3나 탑5 정도가 한계인데\n",
      "\n",
      "261\n",
      "00:17:40,400 --> 00:17:48,040\n",
      "그 정도에서 10% 5% 정도의 리트리버 성능을 올라간다는 것을 확인할 수 있었습니다\n",
      "\n",
      "262\n",
      "00:17:48,040 --> 00:17:50,720\n",
      "이거는 사실 앙상블 하기 전이고요\n",
      "\n",
      "263\n",
      "00:17:50,720 --> 00:17:57,799\n",
      "앙상블 하면 더 높은 성능 개선을 기대할 수 있겠죠\n",
      "\n",
      "264\n",
      "00:17:57,799 --> 00:17:59,799\n",
      "네 그래서 정리를 하자면\n",
      "\n",
      "265\n",
      "00:18:00,000 --> 00:18:03,640\n",
      "LLM은 굉장히 뛰어나졌어요\n",
      "\n",
      "266\n",
      "00:18:03,640 --> 00:18:09,800\n",
      "그럴수록 이제 도크멘트 파싱과 리트리벌 단계가 매우 중요해지고 있고\n",
      "\n",
      "267\n",
      "00:18:09,800 --> 00:18:12,000\n",
      "그게 지금의 상태인 것 같습니다\n",
      "\n",
      "268\n",
      "00:18:12,000 --> 00:18:15,280\n",
      "그래서 이 리트리벌 쪽 성능이\n",
      "\n",
      "269\n",
      "00:18:15,280 --> 00:18:20,940\n",
      "실제 이제 제네레이티브 앤서에서\n",
      "\n",
      "270\n",
      "00:18:20,940 --> 00:18:23,080\n",
      "굉장히 중요한 컴포넌트가 되고 있고요\n",
      "\n",
      "271\n",
      "00:18:23,080 --> 00:18:26,680\n",
      "세상의 문서의 형태는 굉장히 다양하기 때문에\n",
      "\n",
      "272\n",
      "00:18:26,680 --> 00:18:30,680\n",
      "그거를 다루기 위한 노하우가 많이 필요하다\n",
      "\n",
      "273\n",
      "00:18:30,680 --> 00:18:36,520\n",
      "특히 기업용 문서에 대해서는 경험들이 많이 필요하다고 생각을 합니다\n",
      "\n",
      "274\n",
      "00:18:36,520 --> 00:18:39,720\n",
      "그리고 리트리버는 다룰 때\n",
      "\n",
      "275\n",
      "00:18:39,720 --> 00:18:43,000\n",
      "인터넷에 있는 것처럼 그냥 인베링 써서 하는 것보다는\n",
      "\n",
      "276\n",
      "00:18:43,000 --> 00:18:44,639\n",
      "앙상블 해야 되는 거고\n",
      "\n",
      "277\n",
      "00:18:44,639 --> 00:18:50,080\n",
      "앙상블로도 성능이 안 되는 것은 트레인을 해야 되는데\n",
      "\n",
      "278\n",
      "00:18:50,080 --> 00:18:54,919\n",
      "이게 실제 데이터는 굉장히 많고 지저분하고\n",
      "\n",
      "279\n",
      "00:18:54,919 --> 00:18:57,480\n",
      "업데이트의 제약이 많고 그렇기 때문에\n",
      "\n",
      "280\n",
      "00:18:57,480 --> 00:19:00,040\n",
      "학습할 수 있는 이런 형태 자체에\n",
      "\n",
      "281\n",
      "00:19:00,000 --> 00:19:02,800\n",
      "많은 제약이 있을 수밖에 없다.\n",
      "\n",
      "282\n",
      "00:19:02,800 --> 00:19:06,800\n",
      "여기까지가 저희가 얻은 결론이고\n",
      "\n",
      "283\n",
      "00:19:06,800 --> 00:19:08,800\n",
      "그 내용을 발표 드렸습니다.\n"
     ]
    }
   ],
   "source": [
    "print(merge_transcripts(*transcripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:00,000 --> 00:00:08,700\n",
      "2024년 1월 24일 오늘은 생성 AI가 갖는 의미가 단순하게 하나의 트렌드가 아닙니다.\n",
      "\n",
      "2\n",
      "00:00:08,700 --> 00:00:15,700\n",
      "그래서 생성 AI를 기업에서 어떻게 이해하고 준비해야 되고\n",
      "\n",
      "3\n",
      "00:00:15,700 --> 00:00:25,000\n",
      "특히 경영자들이 해야 되는지에 대한 폭넓은 이론서부터 글로벌 현안까지\n",
      "\n",
      "4\n",
      "00:00:25,000 --> 00:00:29,299\n",
      "국내 사타가 인정하는 1인자가 아닌가 싶습니다.\n",
      "\n",
      "5\n",
      "00:00:29,299 --> 00:00:32,500\n",
      "그래서 아주 귀한 장동인 교수님을 모셨고\n",
      "\n",
      "6\n",
      "00:00:32,500 --> 00:00:40,299\n",
      "첫 GPT 기업이 살아남는 법에 대한 내용 외에도 아마 소개를 좀 해주실 것 같은데\n",
      "\n",
      "7\n",
      "00:00:40,299 --> 00:00:45,500\n",
      "첫 GPT를 가지고 어떻게 기업이 살아남아야 되는지\n",
      "\n",
      "8\n",
      "00:00:45,500 --> 00:00:50,500\n",
      "큰 박수로 우리 장동인 교수님의 특강을 시작하도록 하겠습니다.\n",
      "\n",
      "9\n",
      "00:00:50,500 --> 00:00:57,000\n",
      "저는 경영자하고 개발자하고 딱 중간에 있는 사람이에요.\n",
      "\n",
      "10\n",
      "00:00:57,000 --> 00:01:00,000\n",
      "어떻게 보면 교영도 못하고 개발도 못하는\n",
      "\n",
      "11\n",
      "00:01:00,000 --> 00:01:03,700\n",
      "그런 사람일 수도 있고 양쪽을 다 아는 사람일 수도 있고 그런데\n",
      "\n",
      "12\n",
      "00:01:03,700 --> 00:01:11,700\n",
      "어쨌든 이 중간에서 이야기를 하는 게 시간이 지날수록 되게 중요하다는 생각이 점점 듭니다.\n",
      "\n",
      "13\n",
      "00:01:11,700 --> 00:01:19,100\n",
      "왜냐하면 경영하는 사람은 경영만 하고 개발하는 사람은 개발만 하는 그런 시대는 이제 지났잖아요.\n",
      "\n",
      "14\n",
      "00:01:19,100 --> 00:01:20,700\n",
      "양쪽이 양쪽을 다 알아야죠.\n",
      "\n",
      "15\n",
      "00:01:20,700 --> 00:01:25,580\n",
      "그렇기 때문에 개발자들이 현재 어떤 생각을 가지고 있고\n",
      "\n",
      "16\n",
      "00:01:25,580 --> 00:01:30,680\n",
      "또 경영자들이 어떤 생각을 가지고 있는지 양쪽을 다 알아야 되는데\n",
      "\n",
      "17\n",
      "00:01:30,680 --> 00:01:34,080\n",
      "AI는 그렇게 알기가 참 힘들다고 하는 거죠.\n",
      "\n",
      "18\n",
      "00:01:34,080 --> 00:01:39,680\n",
      "왜냐하면 실제로 AI라는 게 다 추상인이잖아요.\n",
      "\n",
      "19\n",
      "00:01:39,680 --> 00:01:46,080\n",
      "학습을 막 얘기를 하는데 안 돌려보는 사람은 학습에 대해서 한 번도 안 해봤으니까\n",
      "\n",
      "20\n",
      "00:01:46,080 --> 00:01:47,580\n",
      "저게 뭔 소리인지는 모르죠.\n",
      "\n",
      "21\n",
      "00:01:47,580 --> 00:01:51,680\n",
      "그러니까 대충 우리가 생각하는 학습이라고 생각을 하게 되는 거죠.\n",
      "\n",
      "22\n",
      "00:01:51,680 --> 00:01:57,680\n",
      "그래서 AI는 개발하는 사람과 또 경영자의 골이 깊습니다.\n",
      "\n",
      "23\n",
      "00:01:57,680 --> 00:01:59,680\n",
      "그래서 지금 AI 시대인데\n",
      "\n",
      "24\n",
      "00:02:00,000 --> 00:02:06,400\n",
      "이거를 메꿔주는 역할이 굉장히 중요하다는 생각이 들어서 이런 책을 썼습니다.\n",
      "\n",
      "25\n",
      "00:02:06,400 --> 00:02:11,600\n",
      "그래서 이 책은 이제 기억이 살아남는 법이라고 제목은 졌지만\n",
      "\n",
      "26\n",
      "00:02:11,600 --> 00:02:15,360\n",
      "양자를 좀 이렇게 결합하고자 하는 노력을 해서 쓴 거고요.\n",
      "\n",
      "27\n",
      "00:02:15,360 --> 00:02:18,799\n",
      "지금 우리가 현재 이제 어디 어느 숫자가 와 있냐면\n",
      "\n",
      "28\n",
      "00:02:18,799 --> 00:02:26,559\n",
      "GPT라는 그런 GPT 3.5, GPT 4, 조금 있으면 이제 GPT 5가 나오겠죠.\n",
      "\n",
      "29\n",
      "00:02:26,559 --> 00:02:30,879\n",
      "그러면서 써보긴 했으나 우리가 저걸 어떻게 활용할지\n",
      "\n",
      "30\n",
      "00:02:30,879 --> 00:02:34,160\n",
      "또 저것 때문에 기억이 어떻게 변할지\n",
      "\n",
      "31\n",
      "00:02:34,160 --> 00:02:37,279\n",
      "그거를 바라보고 있는 그런 시점이죠.\n",
      "\n",
      "32\n",
      "00:02:37,279 --> 00:02:39,919\n",
      "그래서 이제 이거를 도입하지 않겠다고 하는 사람은 별로 없어요.\n",
      "\n",
      "33\n",
      "00:02:39,919 --> 00:02:43,560\n",
      "근데 도입을 하겠는데 뭐가 문제냐?\n",
      "\n",
      "34\n",
      "00:02:43,560 --> 00:02:48,200\n",
      "이런 게 이제 사실은 많은 사람들이 고민하는 고민거리죠.\n",
      "\n",
      "35\n",
      "00:02:48,200 --> 00:02:50,480\n",
      "그래서 이제 그 첫 번째 중요한 것은\n",
      "\n",
      "36\n",
      "00:02:50,480 --> 00:02:56,080\n",
      "이 채집 PT의 한계 이런 거를 명확히 이해를 해야 합니다.\n",
      "\n",
      "37\n",
      "00:02:56,080 --> 00:02:59,360\n",
      "첫 번째는 채집 PT는\n",
      "\n",
      "38\n",
      "00:03:00,000 --> 00:03:03,140\n",
      "콜센터에서 적용하면 되게 좋아요.\n",
      "\n",
      "39\n",
      "00:03:03,140 --> 00:03:05,720\n",
      "왜냐하면 이게 인간 대신에 말을 해주는 거잖아요.\n",
      "\n",
      "40\n",
      "00:03:05,720 --> 00:03:12,120\n",
      "말을 이해를 하고 말을 답변해 주는데 콜센터는 비즈니스의 목적이 있는\n",
      "\n",
      "41\n",
      "00:03:12,120 --> 00:03:14,800\n",
      "거고 거기에 또 법규들이 있어요.\n",
      "\n",
      "42\n",
      "00:03:14,800 --> 00:03:21,879\n",
      "또 이렇게 굉장히 많은 복잡한 법규들이 있어서 그걸 다 지켜야 되는데 아직까지\n",
      "\n",
      "43\n",
      "00:03:21,879 --> 00:03:26,600\n",
      "GPT4가 가장 뛰어나지만 이것도 못하는 부분이 있는 거죠.\n",
      "\n",
      "44\n",
      "00:03:26,600 --> 00:03:31,840\n",
      "이걸 그대로 세팅을 하게 되면 법규에 벗어나는 법이 없는데.\n",
      "\n",
      "45\n",
      "00:03:31,840 --> 00:03:36,980\n",
      "한마디로 기업 입장에서는 저걸 쓰고 싶은데 제가 무슨 말을 할지\n",
      "\n",
      "46\n",
      "00:03:36,980 --> 00:03:38,480\n",
      "모르죠.\n",
      "\n",
      "47\n",
      "00:03:38,480 --> 00:03:43,939\n",
      "이 콜센터가 고객한테 뭐라고 얘기 한 거는 다 녹화가 되고 녹취가\n",
      "\n",
      "48\n",
      "00:03:43,939 --> 00:03:46,959\n",
      "되고 기업이 이걸 책임을 져야 되는 거죠.\n",
      "\n",
      "49\n",
      "00:03:46,959 --> 00:03:53,639\n",
      "그런데 채찍 PT가 고객한테 뭐라고 얘기하는 거를 내가 확신할 수가\n",
      "\n",
      "50\n",
      "00:03:53,639 --> 00:03:54,639\n",
      "없어요.\n",
      "\n",
      "51\n",
      "00:03:54,639 --> 00:03:56,320\n",
      "그러니까 이걸 쓰기가 되게 어려운 거죠.\n",
      "\n",
      "52\n",
      "00:03:56,320 --> 00:03:57,919\n",
      "이게 제일 큰 문제 하나고.\n",
      "\n",
      "53\n",
      "00:04:00,000 --> 00:04:03,400\n",
      "악몽할 재단치라고 하는 Hallucination인데\n",
      "\n",
      "54\n",
      "00:04:03,400 --> 00:04:07,280\n",
      "이걸 해결할 수 있는 방안이 ROG라고 하는 방안이에요.\n",
      "\n",
      "55\n",
      "00:04:07,280 --> 00:04:10,160\n",
      "Retrieval Augmented Generation인데\n",
      "\n",
      "56\n",
      "00:04:10,160 --> 00:04:13,320\n",
      "이게 한마디로 벡터 DB를 써가지고\n",
      "\n",
      "57\n",
      "00:04:13,320 --> 00:04:16,080\n",
      "우리가 원하는 데이터를 거기다 집어넣고\n",
      "\n",
      "58\n",
      "00:04:16,080 --> 00:04:18,360\n",
      "응답을 하면 된다고 하는데\n",
      "\n",
      "59\n",
      "00:04:18,360 --> 00:04:21,840\n",
      "이게 완벽하게 다 해결해 주냐? 그건 또 아닙니다.\n",
      "\n",
      "60\n",
      "00:04:21,840 --> 00:04:26,160\n",
      "그래서 이 부분이 Hallucination 이 부분이 결정적으로\n",
      "\n",
      "61\n",
      "00:04:26,160 --> 00:04:32,000\n",
      "GPT-4가 좀 더 많은 사람들이 활용하게 하는\n",
      "\n",
      "62\n",
      "00:04:32,000 --> 00:04:34,240\n",
      "그런 부분들을 가로막고 있죠.\n",
      "\n",
      "63\n",
      "00:04:34,240 --> 00:04:38,480\n",
      "그래서 GPT-5는 샘 엘트만의 얘기를 들어보면\n",
      "\n",
      "64\n",
      "00:04:38,480 --> 00:04:41,560\n",
      "이 부분은 거의 다 해결된다.\n",
      "\n",
      "65\n",
      "00:04:41,560 --> 00:04:43,639\n",
      "이렇게 얘기를 하더라고요. 거의 다 해결된다.\n",
      "\n",
      "66\n",
      "00:04:43,639 --> 00:04:47,080\n",
      "그거야 봐야 하는 거죠.\n",
      "\n",
      "67\n",
      "00:04:47,080 --> 00:04:50,799\n",
      "그 다음에 중요한 포인트가 이겁니다.\n",
      "\n",
      "68\n",
      "00:04:50,799 --> 00:04:52,880\n",
      "온 디바이스 구현이 안 된다는 거예요.\n",
      "\n",
      "69\n",
      "00:04:52,880 --> 00:04:55,080\n",
      "왜냐하면 이게 사이즈가 너무 크기 때문에\n",
      "\n",
      "70\n",
      "00:04:55,080 --> 00:04:58,439\n",
      "결과를 좀 줄여가지고\n",
      "\n",
      "71\n",
      "00:04:58,439 --> 00:05:01,639\n",
      "자동차 내에다가 연결을 해야 되니까\n",
      "\n",
      "72\n",
      "00:05:00,000 --> 00:05:05,440\n",
      "하면 좋겠다 사실은 이제 그 이 메세지 벤츠가\n",
      "\n",
      "73\n",
      "00:05:05,440 --> 00:05:10,600\n",
      "체치피티를 구글 클라우드 하고 해서 연결하는 그런 계약을\n",
      "\n",
      "74\n",
      "00:05:10,600 --> 00:05:15,680\n",
      "차렸죠. 계기는 뭐냐면 네트워크 끊어지면 안된다 안된다\n",
      "\n",
      "75\n",
      "00:05:15,680 --> 00:05:20,080\n",
      "라고 하는 거예요 그런데 이제 여기서 중요한 건 뭐냐면 이제 삼성전자가\n",
      "\n",
      "76\n",
      "00:05:20,080 --> 00:05:27,440\n",
      "며칠 전에 갤럭시24를 보이면서 네트워크 끊어져도\n",
      "\n",
      "77\n",
      "00:05:27,440 --> 00:05:33,520\n",
      "번역하고 통역하고 여러 나라로 통역하는 걸 보여드렸잖아요\n",
      "\n",
      "78\n",
      "00:05:33,520 --> 00:05:38,599\n",
      "이게 엄청 큰 겁니다 이게 온 디바이스 ai 를 구현하는\n",
      "\n",
      "79\n",
      "00:05:38,599 --> 00:05:44,279\n",
      "굉장히 중요한 사례인 거죠 왜냐하면 지금까지 라지 랭귀지 모델이라고 하는 것은\n",
      "\n",
      "80\n",
      "00:05:44,279 --> 00:05:50,119\n",
      "덩치가 커서 클라우드 서버에만 조정이 됐다 그런데 이걸 줄여가지고\n",
      "\n",
      "81\n",
      "00:05:50,119 --> 00:05:57,279\n",
      "내 스마트폰에도 집어넣을 수 있다고 하는 요 사실은 앞으로 자동차에도 설치가 될 거고\n",
      "\n",
      "82\n",
      "00:05:57,279 --> 00:06:01,439\n",
      "스마트폰은 아니라 다른 예를 들면\n",
      "\n",
      "83\n",
      "00:06:00,000 --> 00:06:08,440\n",
      "로봇같은 데다 이제 집어넣을 수 있게 되고 좀 더 이렇게 더 나가면은\n",
      "\n",
      "84\n",
      "00:06:08,440 --> 00:06:14,080\n",
      "반도체가 들어 있을 만한 모든 디바이스에다가 LLM을 집어넣을 수 있다.\n",
      "\n",
      "85\n",
      "00:06:14,080 --> 00:06:20,320\n",
      "그래서 거울이 내 말을 알아 듣고 TV가 내 말을 알아 듣고 냉장고가 내 말을 알아 듣는\n",
      "\n",
      "86\n",
      "00:06:20,320 --> 00:06:25,400\n",
      "그런 시대가 온다고 하는 거죠. 그래서 이 삼성전자가 구현한 이거는\n",
      "\n",
      "87\n",
      "00:06:25,400 --> 00:06:34,439\n",
      "이거는 하나의 시대를 다시 여는 거다 라고 생각하시면 됩니다. 그래서 이게 이제 되려면 모델 경량화라는 게 이제 핵심이죠.\n",
      "\n",
      "88\n",
      "00:06:34,439 --> 00:06:42,959\n",
      "라지로 랭키지 모델을 작게 경량화 시키는 거 요게 이제 굉장히 중요한 포인트구요.\n",
      "\n",
      "89\n",
      "00:06:42,959 --> 00:06:50,240\n",
      "이게 향후에 좋은 비즈니스 거리가 될 거다 라고 하는 겁니다. 자 그 다음에 이제 마지막으로 중요한 게 뭐냐면\n",
      "\n",
      "90\n",
      "00:06:50,240 --> 00:06:55,360\n",
      "데이터 보안에 대한 이슈죠. GBT 포어를 쓰게 되면 한가지 행위는\n",
      "\n",
      "91\n",
      "00:06:55,360 --> 00:07:01,799\n",
      "쟤네들이 내 데이터를 가지고 학습해서 쓴다?\n",
      "\n",
      "92\n",
      "00:07:00,000 --> 00:07:04,760\n",
      "입력하는게 나중에 넥스트 버전의 답변으로 나온다?\n",
      "\n",
      "93\n",
      "00:07:04,760 --> 00:07:09,720\n",
      "이거 말이 안 되죠. 우리 회사의 비밀을 막 집어 넣었는데 얘가 또\n",
      "\n",
      "94\n",
      "00:07:09,720 --> 00:07:14,960\n",
      "다음 버전에 우리 회사 비밀을 모든 전세계 사람한테 얘기한다?\n",
      "\n",
      "95\n",
      "00:07:14,960 --> 00:07:20,280\n",
      "이거는 어 어느 누구도 받아들일 수 없는 부분입니다. 그런데 이제\n",
      "\n",
      "96\n",
      "00:07:20,280 --> 00:07:23,879\n",
      "이게 대해서 뭐라고 얘기를 하냐면 오픈 AI는\n",
      "\n",
      "97\n",
      "00:07:23,879 --> 00:07:30,200\n",
      "이렇게 답변을 하죠. 홈페이지에서 있는거에요. API 데이터 프라이버시다.\n",
      "\n",
      "98\n",
      "00:07:30,200 --> 00:07:37,360\n",
      "얘네들이 타임를 쳐가지고 오픈 AI 인터페이스에 집어 넣는거를\n",
      "\n",
      "99\n",
      "00:07:37,360 --> 00:07:44,840\n",
      "우리는 학습을 하겠다고 얘기를 했어요. 그런데 이 파이썬으로 오픈 AI API를 써가지고\n",
      "\n",
      "100\n",
      "00:07:44,840 --> 00:07:50,119\n",
      "뭔가 이게 뭐 데이터를 왔다 갔다 하게 만들면 그건 우리가 손 안댔겠게 라고 한거에요.\n",
      "\n",
      "101\n",
      "00:07:50,119 --> 00:07:56,880\n",
      "오픈 AI API를 가지고 할 수 있는게 굉장히 많아요.\n",
      "\n",
      "102\n",
      "00:07:56,880 --> 00:08:01,919\n",
      "별거 다 할 수 있습니다. 그런데 그거를 가지고 실제로\n",
      "\n",
      "103\n",
      "00:08:00,000 --> 00:08:04,880\n",
      "뭔가 어플리케이션을 구현을 하게 되면 오픈에 의한 손을 안대겠다고 하는 건데\n",
      "\n",
      "104\n",
      "00:08:04,880 --> 00:08:10,680\n",
      "이제 이 부분을 요대복이 중요한 건데 이거를 믿느냐 안 믿느냐\n",
      "\n",
      "105\n",
      "00:08:10,680 --> 00:08:15,880\n",
      "아 그래도 뭐 니네가 우리 데이터 가지고 뭐라고 장난치면 내가 어떻게 알아\n",
      "\n",
      "106\n",
      "00:08:15,880 --> 00:08:21,000\n",
      "그러니까 나는 니네꺼 안 쓸거야 라고 하는 그런 자세가 하나 있을 거고\n",
      "\n",
      "107\n",
      "00:08:21,000 --> 00:08:25,080\n",
      "어 그래 그러면 내가 API를 통해 가지고 어플리케이션을 만들면 되지 뭐\n",
      "\n",
      "108\n",
      "00:08:25,080 --> 00:08:30,200\n",
      "이렇게 생각하는 사람이 있어요. 그래서 이게 크게 나눠지는 겁니다.\n",
      "\n",
      "109\n",
      "00:08:30,200 --> 00:08:34,560\n",
      "우리나라는 어떠냐. 우리나라는 하도 많이 들어가지고\n",
      "\n",
      "110\n",
      "00:08:34,560 --> 00:08:39,840\n",
      "GPT4 이런건 안 쓰려고 하죠. 근데 이제 해외에서는 많이 써요.\n",
      "\n",
      "111\n",
      "00:08:39,840 --> 00:08:45,160\n",
      "그렇게 해가지고. 이거는 이제 뭐랑 똑같으냐면 옛날 한 10년 12년\n",
      "\n",
      "112\n",
      "00:08:45,160 --> 00:08:50,480\n",
      "2010년 정도에 클라우드 컴퓨팅이 새로 나왔었죠. 옛날에\n",
      "\n",
      "113\n",
      "00:08:50,480 --> 00:08:55,520\n",
      "근데 그때 이슈가 뭐였냐면 우리가 구글 데이터를 쓰는데\n",
      "\n",
      "114\n",
      "00:08:55,520 --> 00:09:01,959\n",
      "구글 데이터 센터는 어디 있어? 그거 넘기고 있잖아. 그러면 개소리 데이터 해가지고\n",
      "\n",
      "115\n",
      "00:09:00,000 --> 00:09:04,180\n",
      "뭐 거기 뭐 마음대로 하게 되면 어떻게 되는 거야 라는 얘기를\n",
      "\n",
      "116\n",
      "00:09:04,180 --> 00:09:06,960\n",
      "클라우드 컴퓨팅 할 때 무지하게 많이 물어봤습니다.\n",
      "\n",
      "117\n",
      "00:09:06,960 --> 00:09:11,340\n",
      "근데 뭐 구글은 뭐 어 어 아 우리가 절대 손 안대고 뭐 뭐\n",
      "\n",
      "118\n",
      "00:09:11,340 --> 00:09:14,580\n",
      "그렇다고 약속을 한다 하더라도\n",
      "\n",
      "119\n",
      "00:09:14,580 --> 00:09:17,799\n",
      "그거를 어 우리나라 사람들은 안 믿은 거죠.\n",
      "\n",
      "120\n",
      "00:09:17,799 --> 00:09:21,500\n",
      "그래서 그때 이제 클라우드를 쓰지 말자 라고 하는 것도 있었고\n",
      "\n",
      "121\n",
      "00:09:21,500 --> 00:09:23,959\n",
      "뭐 쓰지 말아라 하는 것도 있었고 그랬었는데\n",
      "\n",
      "122\n",
      "00:09:23,959 --> 00:09:29,600\n",
      "지금은 어때요? 지금은 뭐 당연히 클라우드 쓰는 걸로 생각하고 있죠.\n",
      "\n",
      "123\n",
      "00:09:29,600 --> 00:09:32,660\n",
      "그거랑 이거랑 똑같은 거 아니냐 라는 생각이 들긴 해요.\n",
      "\n",
      "124\n",
      "00:09:32,660 --> 00:09:37,279\n",
      "그래서 이 부분은 이제 어 개명자의 몫이다.\n",
      "\n",
      "125\n",
      "00:09:37,279 --> 00:09:43,580\n",
      "그런데 아 이거는 있어요. 뭐냐면 gpt4 하고\n",
      "\n",
      "126\n",
      "00:09:43,580 --> 00:09:49,660\n",
      "일반적인 그 그 오픈소스 llm 하고\n",
      "\n",
      "127\n",
      "00:09:49,660 --> 00:09:53,220\n",
      "어떻게 차이나느냐 라는 것을 많이 물어보죠.\n",
      "\n",
      "128\n",
      "00:09:53,220 --> 00:09:59,500\n",
      "아직까지는 gpt4를 뛰어넘는 오픈소스 llm이 없습니다.\n",
      "\n",
      "129\n",
      "00:10:00,000 --> 00:10:05,000\n",
      "각자 알아서 쓴다고 하면 굉장히 스케일 다운이 되고 기능이 별로 없는\n",
      "\n",
      "130\n",
      "00:10:05,000 --> 00:10:09,000\n",
      "그런 오픈소스를 써야 되는 건 맞아요\n",
      "\n",
      "131\n",
      "00:10:09,000 --> 00:10:15,000\n",
      "요새 GPT-4를 무찌르기 위해서 굉장히 많은 오픈소스를 쓰겠다고 나오고\n",
      "\n",
      "132\n",
      "00:10:15,000 --> 00:10:18,000\n",
      "우리나라도 굉장히 많이 나오고 있는데\n",
      "\n",
      "133\n",
      "00:10:18,000 --> 00:10:23,000\n",
      "그거는 그거는 어쩌냐\n",
      "\n",
      "134\n",
      "00:10:23,000 --> 00:10:26,000\n",
      "그거는 뭐 잘 옛날 보다 훨씬 좋아졌죠 당연히\n",
      "\n",
      "135\n",
      "00:10:27,000 --> 00:10:31,000\n",
      "근데 GPT-4는 이것만 있는 게 아니라\n",
      "\n",
      "136\n",
      "00:10:31,000 --> 00:10:38,000\n",
      "뒷단에 GPT-4와 거기에 따른 많은 API와 다양한 기능들이 있고\n",
      "\n",
      "137\n",
      "00:10:38,000 --> 00:10:42,000\n",
      "이게 또 올라가잖아요 GPT-5로 올라갈 거고\n",
      "\n",
      "138\n",
      "00:10:42,000 --> 00:10:45,000\n",
      "지금 멀티모델이 되잖아요\n",
      "\n",
      "139\n",
      "00:10:45,000 --> 00:10:49,000\n",
      "그런데 이 멀티모델 같은 이런 것들은 아직은 안 된다고\n",
      "\n",
      "140\n",
      "00:10:49,000 --> 00:10:52,000\n",
      "지금 거의 모든 게 텍스트 베이스로 안 된다고 하는 거죠\n",
      "\n",
      "141\n",
      "00:10:52,000 --> 00:10:54,000\n",
      "그래서 저는 그렇게 생각해요\n",
      "\n",
      "142\n",
      "00:10:54,000 --> 00:10:59,000\n",
      "GPT-4 오픈소스 그것도 좋지만\n",
      "\n",
      "143\n",
      "00:11:00,000 --> 00:11:04,600\n",
      "돈 내고 좀 쓰면 어떠냐 라는 생각도 많이 들죠. 그래서 이거는 뭐 자유니까\n",
      "\n",
      "144\n",
      "00:11:04,600 --> 00:11:07,740\n",
      "생각은 자유니까 알아서 판단을 하시고\n",
      "\n",
      "145\n",
      "00:11:07,740 --> 00:11:12,660\n",
      "자 그럼 채집 PT나 오픈소스 LLM을 기업에서 활용하는 방법은 세가지가 있습니다\n",
      "\n",
      "146\n",
      "00:11:12,660 --> 00:11:19,680\n",
      "세가지 근데 이제 이 세가지를 잘 이게 하나의 결과는 하나이고\n",
      "\n",
      "147\n",
      "00:11:19,680 --> 00:11:25,680\n",
      "세가지가 잘 안되면은 우리 기업이 오픈소스를 제대로 이해를 못하는 거고\n",
      "\n",
      "148\n",
      "00:11:25,680 --> 00:11:33,200\n",
      "이 LLM 이라고 하는게 사실은 새로운 앞으로 굉장히 발전의 무궁무중 할 수 있는\n",
      "\n",
      "149\n",
      "00:11:33,200 --> 00:11:43,040\n",
      "그런 물건입니다. 그냥 AI보다 AI가 우리가 이제 뭐 많은 세월 이렇게\n",
      "\n",
      "150\n",
      "00:11:43,040 --> 00:11:48,959\n",
      "변천해 왔는데 이 LLM 이라고 하는건 기존에 있었던 AI하고 많이 달라요.\n",
      "\n",
      "151\n",
      "00:11:48,959 --> 00:11:54,240\n",
      "그렇기 때문에 이거는 굉장히 우리가 생각을 많이 하고\n",
      "\n",
      "152\n",
      "00:11:54,240 --> 00:11:56,320\n",
      "이거를 이제 도입을 해야 된다라고 하는 겁니다.\n",
      "\n",
      "153\n",
      "00:11:56,320 --> 00:11:59,759\n",
      "채집 PT나 또는 뭐 LLM을\n",
      "\n",
      "154\n",
      "00:12:00,000 --> 00:12:04,520\n",
      "세가지 관점에서 기업을 봐야 되는데 첫번째는 프롬프트 엔지니어링,\n",
      "\n",
      "155\n",
      "00:12:04,520 --> 00:12:09,320\n",
      "두번째는 기업 자체의 에렐함을 부연하는 것, 세번째는 이걸 비즈니스로 활용하는 것.\n",
      "\n",
      "156\n",
      "00:12:09,320 --> 00:12:12,220\n",
      "그런데 프롬프트 엔지니어링은 다 아시죠.\n",
      "\n",
      "157\n",
      "00:12:12,220 --> 00:12:16,160\n",
      "프롬프트 엔지니어링을 얘기하게 되면, 이걸 얘기하는 사람도 굉장히 많죠.\n",
      "\n",
      "158\n",
      "00:12:16,160 --> 00:12:20,660\n",
      "책도 어마어마하게 많고, 쓰는 방법도 많고, 이거 거의 마치 옛날에\n",
      "\n",
      "159\n",
      "00:12:20,660 --> 00:12:22,260\n",
      "여러분 기억나세요?\n",
      "\n",
      "160\n",
      "00:12:22,260 --> 00:12:28,200\n",
      "엑셀 쓰는 방법, 아직도 책은 많지만, 엑셀 쓰는 방법 처음에 나왔을 때\n",
      "\n",
      "161\n",
      "00:12:28,320 --> 00:12:31,980\n",
      "어마어마하게 이거 강의도 많이 했었어요.\n",
      "\n",
      "162\n",
      "00:12:31,980 --> 00:12:35,080\n",
      "지금 똑같은 거예요. 채집필 쓰는 방법, 똑같은 거죠.\n",
      "\n",
      "163\n",
      "00:12:35,080 --> 00:12:42,099\n",
      "그런데 이게 엑셀을 요새 기업에서 안 쓰는 사람, 안 쓰는 데 없죠.\n",
      "\n",
      "164\n",
      "00:12:42,099 --> 00:12:46,560\n",
      "마찬가지로 엘렐렘이 그렇게 될 겁니다.\n",
      "\n",
      "165\n",
      "00:12:46,560 --> 00:12:50,900\n",
      "그래서 프롬프트 엔지니어링이라고 하는, 이걸 쓰는 모임이죠.\n",
      "\n",
      "166\n",
      "00:12:50,900 --> 00:12:55,099\n",
      "그러니까 뭐 채집필 쓰는 모임, 기업 내에서.\n",
      "\n",
      "167\n",
      "00:12:55,099 --> 00:13:02,000\n",
      "여러분 만약에 기업을 하신다고 하면, 반드시 프롬프트 엔지니어링 TF팀을 운영하셔야 합니다.\n",
      "\n",
      "168\n",
      "00:13:00,000 --> 00:13:06,400\n",
      "하기를 바랍니다. 왜 그러냐면 혼자 알아서 쓰는 거는\n",
      "\n",
      "169\n",
      "00:13:06,400 --> 00:13:09,440\n",
      "그거는 상당히 시간이 많이 걸렸을 때\n",
      "\n",
      "170\n",
      "00:13:09,440 --> 00:13:12,280\n",
      "엑셀도 시간이 많이 걸리면 다 알잖아요\n",
      "\n",
      "171\n",
      "00:13:12,280 --> 00:13:16,480\n",
      "근데 지금은 첫 끼니까 모여 가지고 토론하면서\n",
      "\n",
      "172\n",
      "00:13:16,480 --> 00:13:19,879\n",
      "서로 앓게 가면서 쓸 수밖에 없다는 거죠\n",
      "\n",
      "173\n",
      "00:13:19,879 --> 00:13:26,400\n",
      "그렇기 때문에 채찍pt가 우리 회사에\n",
      "\n",
      "174\n",
      "00:13:26,400 --> 00:13:30,959\n",
      "막도록 쓰게 하는 방법은 누구도 가르칠 수가 없는 겁니다\n",
      "\n",
      "175\n",
      "00:13:30,959 --> 00:13:36,599\n",
      "그건 우리가 우리 회사에 사람들이 직접 연구해서 써야 되는 부분이고요\n",
      "\n",
      "176\n",
      "00:13:36,599 --> 00:13:38,480\n",
      "그렇기 때문에 TF팀을 운전하는 게 좋고요\n",
      "\n",
      "177\n",
      "00:13:38,480 --> 00:13:42,279\n",
      "저는 작은 회사에 TF팀의 어드바이저로 일한 적이 있어요\n",
      "\n",
      "178\n",
      "00:13:42,279 --> 00:13:43,799\n",
      "그런데 굉장히 좋더라고요\n",
      "\n",
      "179\n",
      "00:13:43,799 --> 00:13:47,840\n",
      "한 6개월 동안 매주 했는데 대단한 성과를 얻었어요\n",
      "\n",
      "180\n",
      "00:13:47,840 --> 00:13:50,480\n",
      "그래서 아마 여러분들 회사도 이런 거를\n",
      "\n",
      "181\n",
      "00:13:50,480 --> 00:13:53,680\n",
      "한번 운영을 해보면 좋지 않을까 라고 하는 거고\n",
      "\n",
      "182\n",
      "00:13:54,119 --> 00:13:55,919\n",
      "이게 조금 더 지나게 되면\n",
      "\n",
      "183\n",
      "00:13:55,919 --> 00:13:58,919\n",
      "이 팜트엔지니어링이 조금 더 지나게 되면\n",
      "\n",
      "184\n",
      "00:14:00,000 --> 00:14:06,360\n",
      "발생하냐면 우리가 기업에서 일을 하는 거의 모든 일들이 업무 프로세스라고 하잖아요\n",
      "\n",
      "185\n",
      "00:14:06,360 --> 00:14:13,260\n",
      "이 업무 프로세스하고 체질 비트하고 녹아져 가지고 이건 당연히 이거는 여기서 쓰는 걸로 해서\n",
      "\n",
      "186\n",
      "00:14:13,260 --> 00:14:20,879\n",
      "이렇게 이제 기업 내부에 완벽하게 녹아들 거다 라고 하는 건데 예를 들면 이런 거죠\n",
      "\n",
      "187\n",
      "00:14:20,959 --> 00:14:33,080\n",
      "글로벌한 회의를 한다고 치면 예를 들면 중국하고 영국하고 우리나라하고 얘기를 한다고 하는 글로벌 기업 입장에서 보면\n",
      "\n",
      "188\n",
      "00:14:33,080 --> 00:14:43,119\n",
      "제일 문제가 회의록 작성이죠. 중국어도 잘 못하고 영어도 잘 못하는데 이걸 갖다가 다 알아들어 가지고 회의를 한다는 것은 힘들잖아요\n",
      "\n",
      "189\n",
      "00:14:43,119 --> 00:14:52,799\n",
      "그래서 이런 여러 개의 언어를 동시에 할 수 있는 그런 LLM을 가지고 회의록을 음성을 다 녹음한 다음에 회의록을 작성을 하고\n",
      "\n",
      "190\n",
      "00:14:52,799 --> 00:14:58,720\n",
      "거기에 대한 요약을 만들어서 자동적으로 이메일을 뿌리는 거 이거 있으면 되게 좋겠죠\n",
      "\n",
      "191\n",
      "00:14:58,720 --> 00:15:01,919\n",
      "근데 제가 이제 그\n",
      "\n",
      "192\n",
      "00:15:00,000 --> 00:15:06,760\n",
      "어드바이저를 했던 그런 기업은 이걸 직접 자신들이 독자적으로 만들었어요.\n",
      "\n",
      "193\n",
      "00:15:06,760 --> 00:15:10,240\n",
      "초기에 매달렸습니다. 근데 굉장히 좋죠.\n",
      "\n",
      "194\n",
      "00:15:10,240 --> 00:15:12,680\n",
      "그 기업도 글로벌 기업이라.\n",
      "\n",
      "195\n",
      "00:15:12,680 --> 00:15:18,600\n",
      "이런 것들이 기업 내에 녹아들면 굉장히 좋을 것이다 라고 생각이 들고\n",
      "\n",
      "196\n",
      "00:15:18,600 --> 00:15:22,719\n",
      "여러분들 TF 팀을 만들어서 운영해 주길 바라는 거예요.\n",
      "\n",
      "197\n",
      "00:15:22,719 --> 00:15:27,320\n",
      "근데 프롬프트 엔지니어라는 게 아는 만큼 쓰는 거예요.\n",
      "\n",
      "198\n",
      "00:15:27,320 --> 00:15:30,240\n",
      "그러니까 쉬운 것 같지만 쉽지 않습니다.\n",
      "\n",
      "199\n",
      "00:15:30,240 --> 00:15:34,919\n",
      "그리고 이게 기업의 활동에 도움이 된다고 하면\n",
      "\n",
      "200\n",
      "00:15:34,919 --> 00:15:41,759\n",
      "이게 업무에 대한 노하우와 이걸 쓰는 노하우가 결합이 돼야 되는 거죠.\n",
      "\n",
      "201\n",
      "00:15:41,759 --> 00:15:43,759\n",
      "그래서 어렵다고 하는 거예요.\n",
      "\n",
      "202\n",
      "00:15:43,759 --> 00:15:46,720\n",
      "근데 이게 왜 중요하냐면\n",
      "\n",
      "203\n",
      "00:15:46,720 --> 00:15:52,560\n",
      "여러분 개별적으로 체질 PT를 쓰든 또는 우리 기업 내부에\n",
      "\n",
      "204\n",
      "00:15:52,560 --> 00:15:56,080\n",
      "라이저 랭기지 모델을 도입을 하든 중요한 건 뭐냐면\n",
      "\n",
      "205\n",
      "00:15:56,119 --> 00:15:57,160\n",
      "요구 조건이죠.\n",
      "\n",
      "206\n",
      "00:15:57,160 --> 00:15:59,959\n",
      "특히나 이제 오픈서스\n",
      "\n",
      "207\n",
      "00:16:00,000 --> 00:16:02,700\n",
      "LLM을 도입할 때의 문제는\n",
      "\n",
      "208\n",
      "00:16:02,700 --> 00:16:05,740\n",
      "그걸 만들어 놓으면 누구나 다 쓰겠다고 생각하고\n",
      "\n",
      "209\n",
      "00:16:05,740 --> 00:16:07,840\n",
      "하지만 실제로 그렇게 다 쓰질 않아요\n",
      "\n",
      "210\n",
      "00:16:07,840 --> 00:16:10,500\n",
      "여러분 회사에 기업에서\n",
      "\n",
      "211\n",
      "00:16:10,500 --> 00:16:13,440\n",
      "터치피스티를 쓰는 사람 손들어라고 하면\n",
      "\n",
      "212\n",
      "00:16:13,440 --> 00:16:15,040\n",
      "안 써본 사람은 없을 텐데\n",
      "\n",
      "213\n",
      "00:16:15,040 --> 00:16:17,040\n",
      "매일매일 쓰는 사람은 없을 겁니다\n",
      "\n",
      "214\n",
      "00:16:17,040 --> 00:16:19,040\n",
      "그 정도로 사실은\n",
      "\n",
      "215\n",
      "00:16:19,040 --> 00:16:21,540\n",
      "쓰는 그런 강도나 이런 게 빈도가\n",
      "\n",
      "216\n",
      "00:16:21,540 --> 00:16:23,840\n",
      "많지 않습니다. 그렇기 때문에\n",
      "\n",
      "217\n",
      "00:16:23,840 --> 00:16:27,740\n",
      "이런 TF 팀을 확정해가지고 구체적으로 요구사항이 정리가 안 되면\n",
      "\n",
      "218\n",
      "00:16:28,379 --> 00:16:31,580\n",
      "있으면 쓰겠지가 아니라 있으면 안 쓴다라고 하는 거예요\n",
      "\n",
      "219\n",
      "00:16:31,580 --> 00:16:34,279\n",
      "돈들인 만큼 쓰지를 않는다라고 하는 겁니다\n",
      "\n",
      "220\n",
      "00:16:34,279 --> 00:16:36,580\n",
      "네, 프롬테이지는 여기까지만 하고요\n",
      "\n",
      "221\n",
      "00:16:36,580 --> 00:16:40,080\n",
      "더 많은 얘기는 이 전문가들이 많으니까\n",
      "\n",
      "222\n",
      "00:16:40,080 --> 00:16:41,279\n",
      "그분들한테 들으면 됩니다\n",
      "\n",
      "223\n",
      "00:16:41,279 --> 00:16:43,580\n",
      "채집피스티를 기업에서 활용하는\n",
      "\n",
      "224\n",
      "00:16:43,580 --> 00:16:46,580\n",
      "세 가지 방법이 아니라 다섯 가지 방법인데요\n",
      "\n",
      "225\n",
      "00:16:46,580 --> 00:16:50,080\n",
      "이제 다섯 가지 방법에 대해서 얘기를 드릴 겁니다\n",
      "\n",
      "226\n",
      "00:16:50,080 --> 00:16:51,580\n",
      "이거는 뭐냐면\n",
      "\n",
      "227\n",
      "00:16:51,580 --> 00:16:55,580\n",
      "채집피스티를 그러면 안 쓰는\n",
      "\n",
      "228\n",
      "00:16:55,919 --> 00:16:58,419\n",
      "라지 랭귀지를 만들려면 어떻게 되느냐\n",
      "\n",
      "229\n",
      "00:16:58,419 --> 00:17:00,419\n",
      "그걸 이제 얘기를\n",
      "\n",
      "230\n",
      "00:17:00,000 --> 00:17:09,000\n",
      "드리려고 하는 거예요. 그래서 기본적으로 인공지능은 이렇게 되어 있다. 학습하는 단계와 인퍼런스를 하는 단계가\n",
      "\n",
      "231\n",
      "00:17:09,000 --> 00:17:17,440\n",
      "이렇게 있는데 이 파운데이션 모델 GPT4 같은게 파운데이션 모델이죠. 그래가지고 그건 우리 회사에 맞추어서 쓰는\n",
      "\n",
      "232\n",
      "00:17:17,440 --> 00:17:25,320\n",
      "방법이 뭐냐 파인트닝 이다라고 하는 거죠. 파인트닝이 뭐냐면 그래서 파인트닝 할 때는 뭐가 필요하냐면\n",
      "\n",
      "233\n",
      "00:17:25,320 --> 00:17:31,920\n",
      "기존에 우리가 GPT4 쓰는 거랑 많이 달라요. 이거는 뭐냐면 파운데이션 모델이 반드시 필요해요.\n",
      "\n",
      "234\n",
      "00:17:31,920 --> 00:17:40,160\n",
      "파인트닝을 하려면. 두번째는 뭐냐면 내가 AI 모델을 가지고 있어야 해요. 프로그램이 있어야 해요.\n",
      "\n",
      "235\n",
      "00:17:40,160 --> 00:17:49,599\n",
      "AI 모델을 내가 직접 만들어야 해요. 그래서 파운데이션 모델이 있어야 하고 내가 우리 기업에 맞는 AI 모델을\n",
      "\n",
      "236\n",
      "00:17:49,599 --> 00:17:55,080\n",
      "만들어야 하고. 기업 자체 데이터가 있어야 하고.\n",
      "\n",
      "237\n",
      "00:17:55,080 --> 00:18:01,639\n",
      "기업 자체 GPU가 있어야 합니다. 그래야 학습을 할 거 아니에요.\n",
      "\n",
      "238\n",
      "00:18:00,000 --> 00:18:02,800\n",
      "이게 이 덩어리가 있어야 되는 게 되게 중요합니다.\n",
      "\n",
      "239\n",
      "00:18:02,800 --> 00:18:06,400\n",
      "근데 이제 사실 기업 자체 데이터가 굉장히 많죠.\n",
      "\n",
      "240\n",
      "00:18:06,400 --> 00:18:07,800\n",
      "당연히 이거는 있어요.\n",
      "\n",
      "241\n",
      "00:18:07,800 --> 00:18:12,400\n",
      "근데 기업 자체의 데이터를 가지고 있는다에서 중요한 게 아니라\n",
      "\n",
      "242\n",
      "00:18:12,400 --> 00:18:17,600\n",
      "이거를 파인트닉하는 포맷으로 만들어 놔야 돼요.\n",
      "\n",
      "243\n",
      "00:18:17,600 --> 00:18:22,500\n",
      "그니까 퀘스천 앤서, 퀘스천 앤서 요거의 쌍을 계속해서 만들어야 되는데\n",
      "\n",
      "244\n",
      "00:18:22,500 --> 00:18:24,700\n",
      "본원 만드는 거 쉽지 않아요.\n",
      "\n",
      "245\n",
      "00:18:24,700 --> 00:18:25,700\n",
      "본원 만드는 게.\n",
      "\n",
      "246\n",
      "00:18:25,700 --> 00:18:28,600\n",
      "그래서 기업 자체 데이터만 가지고 있다고 해서\n",
      "\n",
      "247\n",
      "00:18:29,200 --> 00:18:31,299\n",
      "파인트닉이 다 되는 게 아니라\n",
      "\n",
      "248\n",
      "00:18:31,299 --> 00:18:37,200\n",
      "그러면 파인트닉에 맞는 그런 포맷을 바꿔야 된다라고 하는 게 중요한 포인트고요.\n",
      "\n",
      "249\n",
      "00:18:37,200 --> 00:18:40,000\n",
      "그다음에 기업 자체 AI 모델이 필요한 거예요.\n",
      "\n",
      "250\n",
      "00:18:40,000 --> 00:18:42,900\n",
      "기업 자체 AI 모델을 어디서 구하나.\n",
      "\n",
      "251\n",
      "00:18:42,900 --> 00:18:45,000\n",
      "물론 기터를 하면 많이 있어요.\n",
      "\n",
      "252\n",
      "00:18:45,000 --> 00:18:48,500\n",
      "근데 내가 직접 만들기는 좀 어렵죠.\n",
      "\n",
      "253\n",
      "00:18:48,500 --> 00:18:56,400\n",
      "그다음에 이제 GPU는 내가 예를 들어서 1TB의 우리 회사 데이터가 있다고 하면\n",
      "\n",
      "254\n",
      "00:18:56,400 --> 00:18:59,599\n",
      "그걸 학습을 시키려고 하면 GPU가 당연히 필요하겠죠.\n",
      "\n",
      "255\n",
      "00:19:00,000 --> 00:19:08,180\n",
      "그래서 이렇게 튜닝하는게 우리 회사 제품으로 만드는게 파인튜닝이다 라고 하는겁니다.\n",
      "\n",
      "256\n",
      "00:19:08,180 --> 00:19:13,340\n",
      "근데 이제 또 한가지 방법은 뭐냐면 중국집 셰프가 생각하기에\n",
      "\n",
      "257\n",
      "00:19:13,340 --> 00:19:18,879\n",
      "내가 모든걸 다 아는데 왜 내가 레시피를 그냥 공짜로 쟤네들한테 줘야 되지?\n",
      "\n",
      "258\n",
      "00:19:18,879 --> 00:19:23,100\n",
      "그러지 말고 내가 아예 중국 중화요리 전문점을 만들어서\n",
      "\n",
      "259\n",
      "00:19:23,100 --> 00:19:28,559\n",
      "니네는 와가지고 돈만 내고 이거 와서 사먹어라 라고 할 수 있잖아요\n",
      "\n",
      "260\n",
      "00:19:28,559 --> 00:19:34,240\n",
      "이거는 현재 GPT4가 또는 네이버도 마찬가지죠.\n",
      "\n",
      "261\n",
      "00:19:34,240 --> 00:19:38,400\n",
      "그런거를 다 만들고 나서 전문점에 와서 니네가\n",
      "\n",
      "262\n",
      "00:19:38,400 --> 00:19:45,639\n",
      "니네는 우리 우리죠. 와서 그냥 들어와서 데이터를 액세스를 하게 되는\n",
      "\n",
      "263\n",
      "00:19:45,639 --> 00:19:50,599\n",
      "그런거가 현재 GPT4의 비즈니스 방식입니다.\n",
      "\n",
      "264\n",
      "00:19:50,599 --> 00:19:52,279\n",
      "근데 손님들이 원하는게 있잖아요.\n",
      "\n",
      "265\n",
      "00:19:52,279 --> 00:19:57,000\n",
      "야 니네 내가 치면은 우리 회사 데이터는 모르니까 안나올거 아니야.\n",
      "\n",
      "266\n",
      "00:19:57,000 --> 00:20:01,599\n",
      "그러면 나는 우리 회사 데이터를 어디다 집어넣고 싶어.\n",
      "\n",
      "267\n",
      "00:20:00,000 --> 00:20:02,000\n",
      "그러고 싶지 않나요? 그러니까\n",
      "\n",
      "268\n",
      "00:20:02,000 --> 00:20:04,000\n",
      "그런 부분은 또 뭐라고 하냐면\n",
      "\n",
      "269\n",
      "00:20:04,000 --> 00:20:08,000\n",
      "원샷 러닝, 퓨샷 러닝, 제로샷 러닝이라고\n",
      "\n",
      "270\n",
      "00:20:08,000 --> 00:20:10,000\n",
      "이런게 있어요. 그러니까\n",
      "\n",
      "271\n",
      "00:20:10,000 --> 00:20:12,000\n",
      "그거를\n",
      "\n",
      "272\n",
      "00:20:12,000 --> 00:20:15,000\n",
      "니네가 나한테 학습을 시키는데\n",
      "\n",
      "273\n",
      "00:20:15,000 --> 00:20:20,000\n",
      "그냥 아까처럼 쌍을 만들어서\n",
      "\n",
      "274\n",
      "00:20:20,000 --> 00:20:22,000\n",
      "GPU를 돌려서 하는거는 시간도 많이 들고\n",
      "\n",
      "275\n",
      "00:20:22,000 --> 00:20:24,000\n",
      "돈도 많이 드니까\n",
      "\n",
      "276\n",
      "00:20:24,000 --> 00:20:26,000\n",
      "그러지 말고\n",
      "\n",
      "277\n",
      "00:20:26,000 --> 00:20:28,000\n",
      "니네가 원하는건 내가 프롬프에다가 올려\n",
      "\n",
      "278\n",
      "00:20:28,000 --> 00:20:32,000\n",
      "지금 GPT4는 100K 토큰\n",
      "\n",
      "279\n",
      "00:20:32,000 --> 00:20:36,000\n",
      "10만 단어에 토큰까지 되죠\n",
      "\n",
      "280\n",
      "00:20:36,000 --> 00:20:38,000\n",
      "그 때문에 꽤 한거\n",
      "\n",
      "281\n",
      "00:20:38,000 --> 00:20:41,000\n",
      "한건까지는 안 그래도 한건 정도는 들어가더라구요\n",
      "\n",
      "282\n",
      "00:20:41,000 --> 00:20:43,000\n",
      "그렇게 하게 되면 좋은거죠\n",
      "\n",
      "283\n",
      "00:20:43,000 --> 00:20:46,000\n",
      "그렇게 하면 거기에 대한 내용들을 내가 물어보면 되니까\n",
      "\n",
      "284\n",
      "00:20:46,000 --> 00:20:48,000\n",
      "그렇게 하는 방법이 있고\n",
      "\n",
      "285\n",
      "00:20:48,000 --> 00:20:50,000\n",
      "또 여러가지 방법이 있어요. 또 방법\n",
      "\n",
      "286\n",
      "00:20:50,000 --> 00:20:52,000\n",
      "GPT도\n",
      "\n",
      "287\n",
      "00:20:52,000 --> 00:20:54,000\n",
      "그래서 이제\n",
      "\n",
      "288\n",
      "00:20:54,000 --> 00:20:56,000\n",
      "이런 쓸 방법들을 지금부터 얘기하겠다\n",
      "\n",
      "289\n",
      "00:20:56,000 --> 00:20:58,000\n",
      "회사에서 그런 내용입니다\n",
      "\n",
      "290\n",
      "00:20:58,000 --> 00:21:00,000\n",
      "파인트닝 기법들은\n",
      "\n",
      "291\n",
      "00:21:00,000 --> 00:21:04,360\n",
      "LLM을 오픈소스 LLM을 파인트니닝 하는데\n",
      "\n",
      "292\n",
      "00:21:04,360 --> 00:21:06,600\n",
      "이렇게 여러가지 기법들을 써가지고\n",
      "\n",
      "293\n",
      "00:21:06,600 --> 00:21:09,180\n",
      "조금 GPU를 안쓰고\n",
      "\n",
      "294\n",
      "00:21:09,180 --> 00:21:12,920\n",
      "또는 데이터가 좀 그렇다\n",
      "\n",
      "295\n",
      "00:21:12,920 --> 00:21:16,340\n",
      "많아도 할 수 있도록 만들어주는 게 이런 기법들입니다.\n",
      "\n",
      "296\n",
      "00:21:16,340 --> 00:21:18,799\n",
      "LLM을 하려는 게 현재는\n",
      "\n",
      "297\n",
      "00:21:18,799 --> 00:21:22,799\n",
      "GPT 3, GPT 4, GPT 3.5\n",
      "\n",
      "298\n",
      "00:21:22,799 --> 00:21:24,500\n",
      "여기가 이런 식으로 되어 있고요.\n",
      "\n",
      "299\n",
      "00:21:25,180 --> 00:21:28,400\n",
      "그다음에 기업 데이터를 가지고 여기서\n",
      "\n",
      "300\n",
      "00:21:28,500 --> 00:21:31,600\n",
      "딥에다 집어넣어서\n",
      "\n",
      "301\n",
      "00:21:31,600 --> 00:21:33,500\n",
      "여기 갖다가 퀘스처를 던질 수 있는\n",
      "\n",
      "302\n",
      "00:21:33,500 --> 00:21:36,799\n",
      "이게 이제 인 컨텍스트 러닝이라고 하는 게 있고요.\n",
      "\n",
      "303\n",
      "00:21:36,799 --> 00:21:40,759\n",
      "그 다음에 이제 오픈소스 데이터에서\n",
      "\n",
      "304\n",
      "00:21:40,759 --> 00:21:42,799\n",
      "일루서 AI라고 하는\n",
      "\n",
      "305\n",
      "00:21:42,799 --> 00:21:44,639\n",
      "여러분 잘 모르시죠?\n",
      "\n",
      "306\n",
      "00:21:44,639 --> 00:21:48,680\n",
      "일루서 AI라고 하는 이거\n",
      "\n",
      "307\n",
      "00:21:48,680 --> 00:21:56,480\n",
      "이게 굉장히 유명한 단체예요.\n",
      "\n",
      "308\n",
      "00:21:56,480 --> 00:21:59,959\n",
      "일루서 AI가 여기 한국 사람들이 있는데\n",
      "\n",
      "309\n",
      "00:22:00,000 --> 00:22:07,000\n",
      "이게 오픈AI라는 연구소가 처음에 생길 때부터 얘네들이 있었어요.\n",
      "\n",
      "310\n",
      "00:22:07,000 --> 00:22:10,700\n",
      "GPT-3 때문에 얘네들이 만들어진 건데\n",
      "\n",
      "311\n",
      "00:22:10,700 --> 00:22:16,400\n",
      "본면은 일루사AI라고 하는 얘네들이 사실은 제가 보기에는 대단한 사람들이에요.\n",
      "\n",
      "312\n",
      "00:22:16,400 --> 00:22:22,100\n",
      "얘네들이 뭘 만드냐면 GPT-J라는 걸 만들었어요. GPT-3하고 경쟁을 하고 있고요.\n",
      "\n",
      "313\n",
      "00:22:22,100 --> 00:22:29,700\n",
      "폴리글라스라고 하는 오픈소스 트리트레인드 모델, 파운데이션 모델이죠.\n",
      "\n",
      "314\n",
      "00:22:29,799 --> 00:22:34,200\n",
      "한국어 파운데이션 모델이 폴리글라스라는 게 있어요.\n",
      "\n",
      "315\n",
      "00:22:34,200 --> 00:22:37,099\n",
      "폴리글라스-KR이라고 하는 게 있는데\n",
      "\n",
      "316\n",
      "00:22:37,099 --> 00:22:39,000\n",
      "한국어로 이제 학습을 한 겁니다.\n",
      "\n",
      "317\n",
      "00:22:39,000 --> 00:22:43,000\n",
      "그러니까 한국어로 하고 싶다고 하면\n",
      "\n",
      "318\n",
      "00:22:43,000 --> 00:22:48,099\n",
      "라마보다 여기 있는 라마2, 라마2 조금 있으면 라마3가 생긴다고 하는데\n",
      "\n",
      "319\n",
      "00:22:48,099 --> 00:22:53,900\n",
      "이것도 있지만 폴리글라스라고 하는 오픈소스 엘렐라임이 있어요.\n",
      "\n",
      "320\n",
      "00:22:53,900 --> 00:22:57,099\n",
      "이거 사용해도 좋은 거죠.\n",
      "\n",
      "321\n",
      "00:22:57,099 --> 00:23:02,500\n",
      "그런데 이걸 이제 이거가 어떤 히트로 치면\n",
      "\n",
      "322\n",
      "00:23:00,000 --> 00:23:09,440\n",
      "오픈 데이터라고 하는 1TB 정도의 오픈 소스 데이터를 가지고\n",
      "\n",
      "323\n",
      "00:23:09,440 --> 00:23:15,040\n",
      "이걸 가지고 트레이닝 시킨 겁니다. 대단히 방대해요.\n",
      "\n",
      "324\n",
      "00:23:15,040 --> 00:23:18,879\n",
      "이걸 가지고 이제 그 이스락션 파이팅을 해서\n",
      "\n",
      "325\n",
      "00:23:18,879 --> 00:23:22,080\n",
      "돌리트라는 게 나왔고 스테이블 LM이 나왔고\n",
      "\n",
      "326\n",
      "00:23:22,080 --> 00:23:23,879\n",
      "100이 다 나왔죠.\n",
      "\n",
      "327\n",
      "00:23:23,879 --> 00:23:25,480\n",
      "그 다음에 이제 여러분 잘 아시나요?\n",
      "\n",
      "328\n",
      "00:23:25,480 --> 00:23:30,799\n",
      "페이스북에서 만든 Lama 2가 있는데\n",
      "\n",
      "329\n",
      "00:23:30,799 --> 00:23:35,919\n",
      "현재 쓰고 있는 오픈 소스에서 돌아다니는 그런 것들은\n",
      "\n",
      "330\n",
      "00:23:35,919 --> 00:23:39,639\n",
      "이 다 Lama 2를 가지고 파인튜닝을 한 겁니다.\n",
      "\n",
      "331\n",
      "00:23:39,639 --> 00:23:42,880\n",
      "그래서 초기에는 조금 문제가 있었어요.\n",
      "\n",
      "332\n",
      "00:23:42,880 --> 00:23:47,439\n",
      "왜냐면 누가 연구소에 있는 걸 훔쳐가지고 공개를 했거든요.\n",
      "\n",
      "333\n",
      "00:23:47,439 --> 00:23:51,200\n",
      "그러니까 그때 이제 Lama 1 때죠.\n",
      "\n",
      "334\n",
      "00:23:51,200 --> 00:23:57,240\n",
      "근데 가만히 보니까 페이스북 메타에서 입장해서 보면\n",
      "\n",
      "335\n",
      "00:23:57,240 --> 00:24:00,040\n",
      "이게 전세계로 막 터지는데 이건 나쁘지 않다.\n",
      "\n",
      "336\n",
      "00:24:00,000 --> 00:24:03,920\n",
      "그래서 라마2를 아예 공개를 해버렸습니다.\n",
      "\n",
      "337\n",
      "00:24:03,920 --> 00:24:07,760\n",
      "조금 있으면 뭐가 나오냐면 라마3가 나오는데\n",
      "\n",
      "338\n",
      "00:24:07,760 --> 00:24:12,360\n",
      "그 메타에서는 저걸 만들기 위해서\n",
      "\n",
      "339\n",
      "00:24:12,360 --> 00:24:15,960\n",
      "GPU 최신 GPU가 H100인데요.\n",
      "\n",
      "340\n",
      "00:24:15,960 --> 00:24:19,559\n",
      "이걸 35만 대를 보달했다고 합니다.\n",
      "\n",
      "341\n",
      "00:24:19,559 --> 00:24:21,520\n",
      "35만 대\n",
      "\n",
      "342\n",
      "00:24:21,520 --> 00:24:26,360\n",
      "한 대가 8만 달러인가 그렇다고 하는데\n",
      "\n",
      "343\n",
      "00:24:26,360 --> 00:24:30,879\n",
      "현재 시가가 35만 대면 얼마인지 계산이 안 되죠.\n",
      "\n",
      "344\n",
      "00:24:30,879 --> 00:24:35,480\n",
      "어쨌든 수십조 단위의 GPU 보달을 했다고 해요.\n",
      "\n",
      "345\n",
      "00:24:35,480 --> 00:24:41,720\n",
      "어쨌든 지금 이제 이 정도로 어마어마한 경쟁에 있다고 하는 거고\n",
      "\n",
      "346\n",
      "00:24:41,720 --> 00:24:46,799\n",
      "메타는 이 라마2의 경쟁력을 살리기 위해서\n",
      "\n",
      "347\n",
      "00:24:46,799 --> 00:24:49,560\n",
      "라마3를 굉장히 멋지게 만들어가지고\n",
      "\n",
      "348\n",
      "00:24:49,560 --> 00:24:55,959\n",
      "아마 제가 보기에는 GPT4 분위기 레벨로 만들려고 하는 것 같아요.\n",
      "\n",
      "349\n",
      "00:24:55,959 --> 00:24:57,599\n",
      "GPT4 V\n",
      "\n",
      "350\n",
      "00:24:57,599 --> 00:25:00,000\n",
      "현재 V라고 붙는 것은\n",
      "\n",
      "351\n",
      "00:25:00,000 --> 00:25:02,000\n",
      "멀티 모델이잖아요.\n",
      "\n",
      "352\n",
      "00:25:02,000 --> 00:25:05,500\n",
      "근데 이거를 그렇게 만들려고 노력하는 것 같다.\n",
      "\n",
      "353\n",
      "00:25:05,500 --> 00:25:11,500\n",
      "그래서 이제 이런 점에서 오픈소스에 그게 굉장히 이제\n",
      "\n",
      "354\n",
      "00:25:11,500 --> 00:25:15,000\n",
      "기추가 주목이 되는 거고,\n",
      "\n",
      "355\n",
      "00:25:15,000 --> 00:25:20,500\n",
      "여기 알파카라든가 빕쿠나 GPT-4 오픈 어시스템 등등 해가지고\n",
      "\n",
      "356\n",
      "00:25:20,500 --> 00:25:25,500\n",
      "이쪽에 오픈소스로 달려있는 게 요새 거의 수천 개가 됩니다. 수천 개.\n",
      "\n",
      "357\n",
      "00:25:26,500 --> 00:25:31,500\n",
      "이거를 이제 성능을 자랑하기 위해서 전 세계적으로\n",
      "\n",
      "358\n",
      "00:25:31,500 --> 00:25:34,500\n",
      "누구누구가 잘하나 그런게 이제\n",
      "\n",
      "359\n",
      "00:25:34,500 --> 00:25:37,500\n",
      "오픈소스 LLM 리더보드라는 게 있어요.\n",
      "\n",
      "360\n",
      "00:25:37,500 --> 00:25:42,500\n",
      "리더보드를 보면 매주 매주 바뀌고 있는 거죠.\n",
      "\n",
      "361\n",
      "00:25:42,500 --> 00:25:44,500\n",
      "어쨌든 현황은 이렇습니다.\n",
      "\n",
      "362\n",
      "00:25:44,500 --> 00:25:48,500\n",
      "그런데 갑자기 유엔이도 나와서 이런 거 만들고 있고요.\n",
      "\n",
      "363\n",
      "00:25:48,500 --> 00:25:50,000\n",
      "무리나도 만들고 있고요.\n",
      "\n",
      "364\n",
      "00:25:50,000 --> 00:25:54,000\n",
      "그 다음에 이스라엘도 만들고 있습니다.\n",
      "\n",
      "365\n",
      "00:25:54,000 --> 00:26:00,000\n",
      "이제 지금은 LLM을 안 하면 바보인 그런 세상이 돌아온 거죠.\n",
      "\n",
      "366\n",
      "00:26:00,000 --> 00:26:03,240\n",
      "이거는 좀 옛날 현황이고\n",
      "\n",
      "367\n",
      "00:26:03,240 --> 00:26:06,700\n",
      "그 다음에 중요한 게 뭐냐면 랭체인이라는 겁니다.\n",
      "\n",
      "368\n",
      "00:26:06,700 --> 00:26:08,700\n",
      "랭체인은 또 오프소스랑 관계가 없는 게\n",
      "\n",
      "369\n",
      "00:26:08,700 --> 00:26:12,640\n",
      "이거는 라이저 랭게이지 모델을 가지고\n",
      "\n",
      "370\n",
      "00:26:12,640 --> 00:26:15,840\n",
      "기업 내부에서 이걸 어떻게 연결을 해가지고\n",
      "\n",
      "371\n",
      "00:26:15,840 --> 00:26:18,459\n",
      "프로그래머를 할 수 있느냐라고 하는\n",
      "\n",
      "372\n",
      "00:26:18,459 --> 00:26:20,160\n",
      "이 라이브러리입니다.\n",
      "\n",
      "373\n",
      "00:26:20,160 --> 00:26:24,900\n",
      "근데 랭체인이 작년에 조용히 대박을 쳤습니다.\n",
      "\n",
      "374\n",
      "00:26:24,900 --> 00:26:27,400\n",
      "이거는 현재 직원이 7명밖에 안되는데\n",
      "\n",
      "375\n",
      "00:26:27,639 --> 00:26:31,940\n",
      "회사 가치가 2억 달러 정도 된다고 그래요.\n",
      "\n",
      "376\n",
      "00:26:31,940 --> 00:26:35,299\n",
      "근데 지금은 더 크겠죠.\n",
      "\n",
      "377\n",
      "00:26:35,299 --> 00:26:42,799\n",
      "이거는 랭체인은 반드시 여러분들 회사에 개발자가 있으면\n",
      "\n",
      "378\n",
      "00:26:42,799 --> 00:26:45,000\n",
      "무조건 시켜야 되는 겁니다.\n",
      "\n",
      "379\n",
      "00:26:45,000 --> 00:26:49,700\n",
      "이거는 이거를 이해를 해야\n",
      "\n",
      "380\n",
      "00:26:49,700 --> 00:26:51,000\n",
      "라이저 랭게이지 모델을 가지고\n",
      "\n",
      "381\n",
      "00:26:51,000 --> 00:26:52,840\n",
      "어플리케이션들을 만들 수가 있어요.\n",
      "\n",
      "382\n",
      "00:26:52,840 --> 00:26:57,000\n",
      "그래서 기업에서는 제가 꼭 얘기하고 오는 건\n",
      "\n",
      "383\n",
      "00:26:57,000 --> 00:27:00,240\n",
      "전산실에 있는 사람들 이제부터\n",
      "\n",
      "384\n",
      "00:27:00,000 --> 00:27:06,080\n",
      "머리를 끼워 가지고 이거를 공부를 시켜라 라고 저는 얘기를 하고 있습니다\n",
      "\n",
      "385\n",
      "00:27:06,080 --> 00:27:14,440\n",
      "랭체인은 LLM, 인베이딩, 덕키멜로더, 벡터스토어, 툴 이런 굵직굵직한 개념들이 있고\n",
      "\n",
      "386\n",
      "00:27:14,440 --> 00:27:20,320\n",
      "이걸 전체를 연결하는 그런 프로그래밍 랭귀지 라이브러입니다\n",
      "\n",
      "387\n",
      "00:27:20,320 --> 00:27:24,879\n",
      "그러니까 파이썬으로 할 수 있는 거대한 툴들이죠\n",
      "\n",
      "388\n",
      "00:27:24,879 --> 00:27:28,639\n",
      "이게 엄청나게 변화를 빠르게 하고 있고요\n",
      "\n",
      "389\n",
      "00:27:28,639 --> 00:27:32,599\n",
      "진열하고 있고 컨텐츠 이렇게 쓰는 방법이 이렇습니다\n",
      "\n",
      "390\n",
      "00:27:32,599 --> 00:27:36,599\n",
      "우리가 라이저랭귀지 모델을 써 가지고 뭔가를 만들자고 하면\n",
      "\n",
      "391\n",
      "00:27:36,599 --> 00:27:40,880\n",
      "여기 사용자들이 있잖아요 회사 사용자 회사 내 사용자들이 뭐라고 뭐라고 치잖아요\n",
      "\n",
      "392\n",
      "00:27:40,880 --> 00:27:42,400\n",
      "프롬트라고 하잖아요\n",
      "\n",
      "393\n",
      "00:27:42,400 --> 00:27:46,160\n",
      "프롬트가 어디로 가느냐 랭체인 프로그램으로 받아들여져요\n",
      "\n",
      "394\n",
      "00:27:46,160 --> 00:27:51,119\n",
      "그러면 랭체인 프로그램이 들어오는 응답을 팔씽을 해 가지고\n",
      "\n",
      "395\n",
      "00:27:51,119 --> 00:27:55,560\n",
      "제일 먼저 어디다 하느냐 벡터스토어에다가 캐시처리를 던져요\n",
      "\n",
      "396\n",
      "00:27:55,560 --> 00:28:00,400\n",
      "그러면 이제 벡터스토어는 회사 내부에 있는 다큐멘터리를\n",
      "\n",
      "397\n",
      "00:28:00,000 --> 00:28:04,200\n",
      "잘라서 벡터화 시켜가지고 이제 Word Embedding 이라는 걸 시켜서\n",
      "\n",
      "398\n",
      "00:28:04,200 --> 00:28:05,800\n",
      "여기다가 로딩을 하는 거거든요.\n",
      "\n",
      "399\n",
      "00:28:05,800 --> 00:28:12,400\n",
      "요 구조가 요게 이제 Word Embedding 을 하게 하는 요런 라이브러리를 통해가지고\n",
      "\n",
      "400\n",
      "00:28:12,400 --> 00:28:17,400\n",
      "이 단어를 잘라서 벡터 형태로 해가지고 집어넣는 거거든요.\n",
      "\n",
      "401\n",
      "00:28:17,400 --> 00:28:21,799\n",
      "그래서 이제 여기서 코를 던지면 우리는 뭐 데이터베이스 같은 경우에\n",
      "\n",
      "402\n",
      "00:28:21,799 --> 00:28:23,000\n",
      "인덱스라는 게 있잖아요.\n",
      "\n",
      "403\n",
      "00:28:23,000 --> 00:28:26,400\n",
      "인덱스를 가지고 SQL을 이렇게 찾는 건데\n",
      "\n",
      "404\n",
      "00:28:26,799 --> 00:28:32,000\n",
      "이거는 뭘로 하냐면은 Similar Search 라고 하는 수학적 계산 모형이 있어요.\n",
      "\n",
      "405\n",
      "00:28:32,000 --> 00:28:37,400\n",
      "계산을 하면 여기서 진짜 유사한 것들이 딱 차고 나온다고 하는 거죠.\n",
      "\n",
      "406\n",
      "00:28:37,400 --> 00:28:44,400\n",
      "그래서 우리 회사에 예를 들면 휴가 정책 이렇게 물어보면\n",
      "\n",
      "407\n",
      "00:28:44,400 --> 00:28:48,400\n",
      "얘는 이제 여기다 학습을 시켰으니까 이제 그 정책이 딱 나오는 거죠.\n",
      "\n",
      "408\n",
      "00:28:48,400 --> 00:28:53,400\n",
      "정책이 나온 다음에 요놈을 이렇게 프롬트를 다시 만들어가지고\n",
      "\n",
      "409\n",
      "00:28:53,400 --> 00:29:00,400\n",
      "프롬트를 만들어서 GPT4나 우리 회사에 맞게 만든 LLM\n",
      "\n",
      "410\n",
      "00:29:00,000 --> 00:29:02,000\n",
      "랭 체인을 여기다가 집어넣어주면\n",
      "\n",
      "411\n",
      "00:29:02,000 --> 00:29:04,000\n",
      "제가 그래가지고 답변을 해서\n",
      "\n",
      "412\n",
      "00:29:04,000 --> 00:29:06,000\n",
      "랭 체인 프로그램에다가\n",
      "\n",
      "413\n",
      "00:29:06,000 --> 00:29:08,000\n",
      "답변을 주면\n",
      "\n",
      "414\n",
      "00:29:08,000 --> 00:29:10,000\n",
      "그게 인간이 말하는 말로\n",
      "\n",
      "415\n",
      "00:29:10,000 --> 00:29:12,000\n",
      "이 벡터가 변경이 되가지고\n",
      "\n",
      "416\n",
      "00:29:12,000 --> 00:29:14,000\n",
      "들어온다 이거죠.\n",
      "\n",
      "417\n",
      "00:29:14,000 --> 00:29:16,000\n",
      "그 결과물을\n",
      "\n",
      "418\n",
      "00:29:16,000 --> 00:29:18,000\n",
      "회사의 사용자에 던지는\n",
      "\n",
      "419\n",
      "00:29:18,000 --> 00:29:20,000\n",
      "요걸로 되어있습니다.\n",
      "\n",
      "420\n",
      "00:29:20,000 --> 00:29:22,000\n",
      "근데 그렇게 되면은\n",
      "\n",
      "421\n",
      "00:29:22,000 --> 00:29:24,000\n",
      "묻는 말에 답변만 하는거죠.\n",
      "\n",
      "422\n",
      "00:29:24,000 --> 00:29:26,000\n",
      "근데 중간중간에 뭐가 필요하느냐\n",
      "\n",
      "423\n",
      "00:29:26,000 --> 00:29:28,000\n",
      "인터넷도 검색을 해야 되죠.\n",
      "\n",
      "424\n",
      "00:29:28,000 --> 00:29:30,000\n",
      "현재 무슨 뭐\n",
      "\n",
      "425\n",
      "00:29:30,000 --> 00:29:32,000\n",
      "이...\n",
      "\n",
      "426\n",
      "00:29:32,000 --> 00:29:34,000\n",
      "이자율이 얼마가 되고 주가가 어떻게 되고\n",
      "\n",
      "427\n",
      "00:29:34,000 --> 00:29:36,000\n",
      "이런거 질문하면은 인터넷\n",
      "\n",
      "428\n",
      "00:29:36,000 --> 00:29:38,000\n",
      "검색도 해야 되고\n",
      "\n",
      "429\n",
      "00:29:38,000 --> 00:29:40,000\n",
      "회사 내에 있는 내부 시스템이나 외부 시스템을\n",
      "\n",
      "430\n",
      "00:29:40,000 --> 00:29:42,000\n",
      "또 왔다갔다 해야 되는거고\n",
      "\n",
      "431\n",
      "00:29:42,000 --> 00:29:44,000\n",
      "그쵸. 그 다음에 이제 여기\n",
      "\n",
      "432\n",
      "00:29:44,000 --> 00:29:46,000\n",
      "에이전트라고 하는게 굉장히\n",
      "\n",
      "433\n",
      "00:29:46,000 --> 00:29:48,000\n",
      "특이한데 요거는\n",
      "\n",
      "434\n",
      "00:29:48,000 --> 00:29:50,000\n",
      "GPT-4의\n",
      "\n",
      "435\n",
      "00:29:50,000 --> 00:29:52,000\n",
      "기능 중에 하나인데요.\n",
      "\n",
      "436\n",
      "00:29:52,000 --> 00:29:54,000\n",
      "이 에이전트라는걸 잘 활용을 하게 되면\n",
      "\n",
      "437\n",
      "00:29:54,000 --> 00:29:56,000\n",
      "굉장히 많은 일을 할 수 있고\n",
      "\n",
      "438\n",
      "00:29:56,000 --> 00:29:58,000\n",
      "앞으로 이제 AI 에이전트가\n",
      "\n",
      "439\n",
      "00:29:58,000 --> 00:30:00,000\n",
      "세상에 옵니다.\n",
      "\n",
      "440\n",
      "00:30:00,000 --> 00:30:02,700\n",
      "세 번째 단에서 얘기를 드릴게요.\n",
      "\n",
      "441\n",
      "00:30:02,700 --> 00:30:09,540\n",
      "이렇게 해가지고 랭칭을 통해서 우리 기업에 맞는 어플리케이션을 만들게 된다고 하는 겁니다.\n",
      "\n",
      "442\n",
      "00:30:09,540 --> 00:30:13,560\n",
      "그래서 우리 기업에서 LLM을 도입을 할 때\n",
      "\n",
      "443\n",
      "00:30:13,560 --> 00:30:18,660\n",
      "이 5가지 방법이 있는데 이 5가지 방법 중에 하나를 선택을 하셔야 돼요.\n",
      "\n",
      "444\n",
      "00:30:18,660 --> 00:30:20,860\n",
      "첫 번째는 뭐냐면\n",
      "\n",
      "445\n",
      "00:30:20,860 --> 00:30:34,720\n",
      "Sam Altman에 있는 OpenAI 아까 제가 얘기했던 우리가 제공해주는 OpenAI의 API를 쓰게 되면\n",
      "\n",
      "446\n",
      "00:30:34,720 --> 00:30:39,020\n",
      "GPT4를 연결을 해서 OpenAI의 API를 쓰게 되면\n",
      "\n",
      "447\n",
      "00:30:39,020 --> 00:30:42,759\n",
      "우리는 그 안에 있는 데이터를 쓰지 않을 거라고 아까 약속했잖아요.\n",
      "\n",
      "448\n",
      "00:30:42,759 --> 00:30:44,520\n",
      "홈페이지에 있는 대로.\n",
      "\n",
      "449\n",
      "00:30:44,520 --> 00:30:48,160\n",
      "그 말을 만약에 믿는다면 이렇게 시스템을 구축하는 겁니다.\n",
      "\n",
      "450\n",
      "00:30:48,160 --> 00:30:54,360\n",
      "뭐냐면 랭체인으로 아까처럼 똑같은 방식으로 직원이 물어보면\n",
      "\n",
      "451\n",
      "00:30:54,360 --> 00:30:59,160\n",
      "이렇게 LLM 랭체인 프로그램을 받아가지고\n",
      "\n",
      "452\n",
      "00:30:59,160 --> 00:31:00,959\n",
      "벡터 DB를\n",
      "\n",
      "453\n",
      "00:31:00,000 --> 00:31:06,400\n",
      "query를 던지고 거기에 답변을 받아서 다시 요 놈을 갖다가 gpt4에 던져가지고 얘가\n",
      "\n",
      "454\n",
      "00:31:06,400 --> 00:31:12,280\n",
      "답변을 사람이 하는 포맷으로 나오게끔 하게 되는 겁니다\n",
      "\n",
      "455\n",
      "00:31:12,280 --> 00:31:19,040\n",
      "근데 굳이 gpt4를 가야 되죠. 왜냐면 vector star을 뒤집으면 결과물이 나오지만\n",
      "\n",
      "456\n",
      "00:31:19,040 --> 00:31:25,160\n",
      "그건 뭘로 되어 있어요? 그건 vector로만 숫자의 나열입니다. 숫자의 나열. 인간이 봐도 몰라요\n",
      "\n",
      "457\n",
      "00:31:25,160 --> 00:31:32,119\n",
      "근데 그 숫자를 얘한테 던지면 얘가 그걸 해석을 해가지고 거기에 딱 맞는 답변을\n",
      "\n",
      "458\n",
      "00:31:32,119 --> 00:31:38,599\n",
      "한글이면 한글, 영어면 영어로 이렇게 던져주기 때문에 얘한테 꼭 보내야 된다라고 하는 거죠\n",
      "\n",
      "459\n",
      "00:31:38,599 --> 00:31:45,959\n",
      "그러면 중요한 포인트가 뭐냐 이 vector db에서 벡터화 시켜 가지고 만든 그\n",
      "\n",
      "460\n",
      "00:31:45,959 --> 00:31:50,840\n",
      "벡터를 얘한테 던질 거 아니에요? 그러면 얘가 그거를 이해를 해야 되잖아요\n",
      "\n",
      "461\n",
      "00:31:50,840 --> 00:31:56,119\n",
      "그죠 그래서 이 기업 데이터를 vector db에 집어넣을 때 뭘 하냐면 아까\n",
      "\n",
      "462\n",
      "00:31:56,119 --> 00:31:59,320\n",
      "embedding 이라는 걸 한다고 했잖아요\n",
      "\n",
      "463\n",
      "00:32:00,000 --> 00:32:07,000\n",
      "이 인베이딩을 하는 방법이 내가 오픈소스를 쓰는 방법과\n",
      "\n",
      "464\n",
      "00:32:07,000 --> 00:32:11,000\n",
      "이 오픈AI GPT4 쓰는 방법이 달라요.\n",
      "\n",
      "465\n",
      "00:32:11,000 --> 00:32:14,000\n",
      "만약에 오픈AI를 쓴다고 하면\n",
      "\n",
      "466\n",
      "00:32:14,000 --> 00:32:18,000\n",
      "워드인베이딩을 오픈AI가 제공하는 인베이딩을 해야 되고요.\n",
      "\n",
      "467\n",
      "00:32:18,000 --> 00:32:20,000\n",
      "만약에 라마 형태로 한다고 하면\n",
      "\n",
      "468\n",
      "00:32:20,000 --> 00:32:25,000\n",
      "얘가 라마가 제공해주는 그런 벡터화 시키는 것을 해야 돼요.\n",
      "\n",
      "469\n",
      "00:32:25,000 --> 00:32:29,000\n",
      "그렇지 않으면 얘가 당연히 이해를 못 하겠죠.\n",
      "\n",
      "470\n",
      "00:32:30,000 --> 00:32:35,000\n",
      "여기서 중요한 포인트는 우리나라 기업은 전부 다 방화벽이 있어요.\n",
      "\n",
      "471\n",
      "00:32:35,000 --> 00:32:39,000\n",
      "그래서 이 방화벽을 왔다 갔다 해야 되는 그런데\n",
      "\n",
      "472\n",
      "00:32:39,000 --> 00:32:44,000\n",
      "현재 제공되는 GPT4는 미국이죠. 미국에 왔다 갔다 해야 되는 거고\n",
      "\n",
      "473\n",
      "00:32:44,000 --> 00:32:49,000\n",
      "이 벡터스토어도 외국 거에요.\n",
      "\n",
      "474\n",
      "00:32:49,000 --> 00:32:51,000\n",
      "그렇기 때문에 벡터스토어도\n",
      "\n",
      "475\n",
      "00:32:59,000 --> 00:33:02,000\n",
      "벡터스토어는 이게 다 같은데\n",
      "\n",
      "476\n",
      "00:33:02,000 --> 00:33:04,000\n",
      "이게 다 같은 벡터스토어에요.\n",
      "\n",
      "477\n",
      "00:33:04,000 --> 00:33:07,000\n",
      "그래서 벡터스토어는 이게 다 같은 벡터스토어에요.\n",
      "\n",
      "478\n",
      "00:33:07,000 --> 00:33:10,000\n",
      "그래서 벡터스토어는 이게 다 같은 벡터스토어에요.\n",
      "\n",
      "479\n",
      "00:33:10,000 --> 00:33:13,000\n",
      "그래서 벡터스토어는 이게 다 같은 벡터스토어에요.\n",
      "\n",
      "480\n",
      "00:33:13,000 --> 00:33:16,000\n",
      "그래서 벡터스토어는 이게 다 같은 벡터스토어에요.\n",
      "\n",
      "481\n",
      "00:33:16,000 --> 00:33:19,000\n",
      "그래서 벡터스토어는 이게 다 같은 벡터스토어에요.\n",
      "\n",
      "482\n",
      "00:33:19,000 --> 00:33:22,000\n",
      "그래서 벡터스토어는 이게 다 같은 벡터스토어에요.\n",
      "\n",
      "483\n",
      "00:33:22,000 --> 00:33:25,000\n",
      "그래서 벡터스토어는 이게 다 같은 벡터스토어에요.\n",
      "\n",
      "484\n",
      "00:33:00,000 --> 00:33:04,700\n",
      "네, 여기 보면 질문이 들어왔는데\n",
      "\n",
      "485\n",
      "00:33:04,700 --> 00:33:08,000\n",
      "인베딩은 각자, 여기 보면 인베딩 있잖아요\n",
      "\n",
      "486\n",
      "00:33:08,000 --> 00:33:11,800\n",
      "인베딩 종류에는 오픈AI에서 제공하는 인베딩\n",
      "\n",
      "487\n",
      "00:33:11,800 --> 00:33:14,800\n",
      "CPP에서 라마조이고, 라마\n",
      "\n",
      "488\n",
      "00:33:14,800 --> 00:33:17,400\n",
      "그 다음에 Azure, OpenAI에서 하는 거\n",
      "\n",
      "489\n",
      "00:33:17,400 --> 00:33:20,400\n",
      "TensorFlow, Hub에서 하는 거, 이게 다 틀려요\n",
      "\n",
      "490\n",
      "00:33:20,400 --> 00:33:23,400\n",
      "근데 OpenAI에서 하는 건 돈을 받아요\n",
      "\n",
      "491\n",
      "00:33:23,400 --> 00:33:25,400\n",
      "인베딩 할 때 돈을 받어\n",
      "\n",
      "492\n",
      "00:33:25,400 --> 00:33:27,400\n",
      "아, 이거 좀 억울하죠\n",
      "\n",
      "493\n",
      "00:33:27,400 --> 00:33:29,400\n",
      "여기다 집어넣을 때 돈을 받는다는 거예요\n",
      "\n",
      "494\n",
      "00:33:29,400 --> 00:33:32,400\n",
      "그러면 이 DB에 돈을 주는 게 아니라\n",
      "\n",
      "495\n",
      "00:33:32,400 --> 00:33:35,400\n",
      "OpenAI한테 이걸 OpenAI 포매로 바꿔서\n",
      "\n",
      "496\n",
      "00:33:35,400 --> 00:33:37,400\n",
      "집어넣는데 돈을 냅니다\n",
      "\n",
      "497\n",
      "00:33:37,400 --> 00:33:40,400\n",
      "그렇기 때문에 테라바이트급으로\n",
      "\n",
      "498\n",
      "00:33:40,400 --> 00:33:43,400\n",
      "이렇게 막 VectorDB에다가 집어넣으려고 하면\n",
      "\n",
      "499\n",
      "00:33:43,400 --> 00:33:45,400\n",
      "돈 좀 들 겁니다\n",
      "\n",
      "500\n",
      "00:33:45,400 --> 00:33:49,400\n",
      "어쨌든, 이거 하는 게 돈이 든다고 하는 거고요\n",
      "\n",
      "501\n",
      "00:33:49,400 --> 00:33:51,400\n",
      "그래서 이렇게 기업 데이터 VectorDB에 집어넣는데\n",
      "\n",
      "502\n",
      "00:33:51,400 --> 00:33:53,400\n",
      "돈이 든다고 하는 거고\n",
      "\n",
      "503\n",
      "00:33:53,400 --> 00:33:58,400\n",
      "사실 이 천재, 유행하는 VectorDB는 뭐가 있느냐\n",
      "\n",
      "504\n",
      "00:34:00,000 --> 00:34:07,440\n",
      "여기 보면 파인콘, 크로마, 바이스, 질리즘 해서 수십종이 현재 있고요.\n",
      "\n",
      "505\n",
      "00:34:07,440 --> 00:34:10,720\n",
      "근데 이제 이게 벡터DB가 문제가 뭐냐면\n",
      "\n",
      "506\n",
      "00:34:10,720 --> 00:34:15,520\n",
      "클라우드에서 제공한단 말이에요.\n",
      "\n",
      "507\n",
      "00:34:15,520 --> 00:34:19,080\n",
      "그러면 이제 우리나라에서 보면 클라우드 제공하려면\n",
      "\n",
      "508\n",
      "00:34:19,080 --> 00:34:21,480\n",
      "방어벽을 또 밖으로 나가야 되잖아요.\n",
      "\n",
      "509\n",
      "00:34:21,480 --> 00:34:26,280\n",
      "우리나라 기업이 클라우드 방어벽 제공 왔다 갔다 하는 거 별로 안 좋아하죠.\n",
      "\n",
      "510\n",
      "00:34:26,280 --> 00:34:33,480\n",
      "그렇기 때문에 벡터DB를 국내에서 만드는 분들이 있어요.\n",
      "\n",
      "511\n",
      "00:34:33,480 --> 00:34:36,480\n",
      "여기 최창량씨 있죠?\n",
      "\n",
      "512\n",
      "00:34:36,480 --> 00:34:44,000\n",
      "최창량 대표가 하는게 한글 벡터DB를 만드는 거죠.\n",
      "\n",
      "513\n",
      "00:34:44,000 --> 00:34:49,599\n",
      "이거는 한국에서 만들었기 때문에 온프레미스도 가능하다.\n",
      "\n",
      "514\n",
      "00:34:49,599 --> 00:34:56,599\n",
      "파이워를 밖에서 하는게 아니라 안에서 할 수 있도록 만들어주는 겁니다.\n",
      "\n",
      "515\n",
      "00:34:56,599 --> 00:34:59,599\n",
      "자, 여기서 이제 이거는 문제가 뭐냐.\n",
      "\n",
      "516\n",
      "00:35:00,000 --> 00:35:05,840\n",
      "문제는 기업 데이터를 오픈 AI에 이걸로\n",
      "\n",
      "517\n",
      "00:35:05,840 --> 00:35:08,160\n",
      "이 인베이딩 해야 되는데 그때 돈을 낸다는 거\n",
      "\n",
      "518\n",
      "00:35:08,160 --> 00:35:09,960\n",
      "그거 문제가 하나.\n",
      "\n",
      "519\n",
      "00:35:09,960 --> 00:35:13,520\n",
      "그 다음에 기업의 방화벽을 여러 번 왔다 갔다 해야 된다는 거\n",
      "\n",
      "520\n",
      "00:35:13,520 --> 00:35:15,000\n",
      "그 두 번째 문제.\n",
      "\n",
      "521\n",
      "00:35:15,000 --> 00:35:18,879\n",
      "세 번째 문제는 이 GPT-4를 왔다 갔다 하려고 하면\n",
      "\n",
      "522\n",
      "00:35:18,879 --> 00:35:20,080\n",
      "얘가 또 돈을 받아요.\n",
      "\n",
      "523\n",
      "00:35:20,080 --> 00:35:21,719\n",
      "요 때도 돈을 받아요.\n",
      "\n",
      "524\n",
      "00:35:21,719 --> 00:35:24,600\n",
      "토큰당 얼마.\n",
      "\n",
      "525\n",
      "00:35:24,600 --> 00:35:28,639\n",
      "근데 이 GPT-4를 왔다 갔다 하는 게 돈이 많이 들기 때문에\n",
      "\n",
      "526\n",
      "00:35:28,639 --> 00:35:29,760\n",
      "꽤나 들어요.\n",
      "\n",
      "527\n",
      "00:35:29,760 --> 00:35:34,160\n",
      "그렇기 때문에 GPT-3.5를 활용을 해가지고\n",
      "\n",
      "528\n",
      "00:35:34,160 --> 00:35:37,040\n",
      "이렇게 GPT-4에 효과를 내게 하는\n",
      "\n",
      "529\n",
      "00:35:37,040 --> 00:35:40,240\n",
      "그런 거를 지금 사람들이 많이 생각을 하고 있습니다.\n",
      "\n",
      "530\n",
      "00:35:40,240 --> 00:35:42,040\n",
      "그 방법은 또 나중에 알려드릴게요.\n",
      "\n",
      "531\n",
      "00:35:42,040 --> 00:35:44,360\n",
      "자 어쨌든 여기서 이제 중요한 거는 뭐냐면\n",
      "\n",
      "532\n",
      "00:35:44,360 --> 00:35:47,680\n",
      "사실 이런 시스템을 만드는 거는 그렇게 어렵지는 않으나\n",
      "\n",
      "533\n",
      "00:35:47,680 --> 00:35:49,959\n",
      "이거 쓸 때마다 사용할 때마다\n",
      "\n",
      "534\n",
      "00:35:49,959 --> 00:35:53,720\n",
      "이제 오픈 AI 때 세금을 꼬박꼬박 받치는 게\n",
      "\n",
      "535\n",
      "00:35:53,720 --> 00:35:55,480\n",
      "좀 억울하긴 하죠.\n",
      "\n",
      "536\n",
      "00:35:55,480 --> 00:35:56,840\n",
      "근데 여기서 장점이 있어요.\n",
      "\n",
      "537\n",
      "00:35:56,840 --> 00:35:58,360\n",
      "뭐냐면\n",
      "\n",
      "538\n",
      "00:35:58,360 --> 00:36:00,119\n",
      "뭐 우리 회사\n",
      "\n",
      "539\n",
      "00:36:00,000 --> 00:36:04,000\n",
      "데이터나 이런 거를 벡터DB에 집어넣기 때문에\n",
      "\n",
      "540\n",
      "00:36:04,000 --> 00:36:07,000\n",
      "돈이 별로 안 뜨는 거고\n",
      "\n",
      "541\n",
      "00:36:07,000 --> 00:36:10,000\n",
      "벡터DB를 쓸 때도 돈을 냅니다.\n",
      "\n",
      "542\n",
      "00:36:10,000 --> 00:36:16,000\n",
      "무료로 쓸 때도 있지만 대부분 돈을 받죠.\n",
      "\n",
      "543\n",
      "00:36:16,000 --> 00:36:20,000\n",
      "이것도 이루고, GPT4를 쓸 때도 이것도 이루고\n",
      "\n",
      "544\n",
      "00:36:20,000 --> 00:36:23,000\n",
      "처음에 시작할 때는 돈 받는 거 없으니까\n",
      "\n",
      "545\n",
      "00:36:23,000 --> 00:36:25,000\n",
      "무료로 시작을 하는데\n",
      "\n",
      "546\n",
      "00:36:25,000 --> 00:36:29,000\n",
      "이게 돌아가면 이렇게 애플리케이션을 만들어가게 되면\n",
      "\n",
      "547\n",
      "00:36:29,000 --> 00:36:34,000\n",
      "돈이 좀 꽤 든다 하는 걸 많은 틀에 말씀드리고 싶고요.\n",
      "\n",
      "548\n",
      "00:36:34,000 --> 00:36:40,000\n",
      "그래서 돈 들어간 것도 싫고 GPT4 보안도 그렇고 그러면\n",
      "\n",
      "549\n",
      "00:36:40,000 --> 00:36:43,000\n",
      "나는 아예 이렇게 만들래 라고 하는 거죠.\n",
      "\n",
      "550\n",
      "00:36:43,000 --> 00:36:46,000\n",
      "Open Source LLM을 써가지고\n",
      "\n",
      "551\n",
      "00:36:46,000 --> 00:36:49,000\n",
      "Open Source LLM\n",
      "\n",
      "552\n",
      "00:36:49,000 --> 00:36:51,000\n",
      "여러가지 좋은 거 많이 나오죠. 솔라\n",
      "\n",
      "553\n",
      "00:36:51,000 --> 00:36:55,000\n",
      "요새 공전의 히트를 치고 있는 솔라 같은 거\n",
      "\n",
      "554\n",
      "00:36:55,000 --> 00:36:59,000\n",
      "업스테이지에 만든 솔라 같은 걸 쓰게 되면\n",
      "\n",
      "555\n",
      "00:37:00,000 --> 00:37:03,900\n",
      "우리 회사 안에서 이렇게 기업 데이터를 이렇게 만들어 가지고\n",
      "\n",
      "556\n",
      "00:37:03,900 --> 00:37:08,100\n",
      "어 오픈소스 LLM이 요구하는 인베딩이 있을 거 아니에요\n",
      "\n",
      "557\n",
      "00:37:08,100 --> 00:37:13,200\n",
      "그때 온포스 오픈소스는 대체적으로 돈을 안 받는 인베딩을 합니다\n",
      "\n",
      "558\n",
      "00:37:13,200 --> 00:37:16,459\n",
      "그래서 기업 데이터 요거 집어넣을 때 돈을 안 받고\n",
      "\n",
      "559\n",
      "00:37:16,459 --> 00:37:19,700\n",
      "그 다음에 이렇게 이제 여기 솔랄 같은 경우에는 돈을 안 받죠\n",
      "\n",
      "560\n",
      "00:37:19,700 --> 00:37:23,459\n",
      "그러니까 회사 내에서 이렇게 해서 만들어 가지고 쓰는 거는 돈이 안 들죠\n",
      "\n",
      "561\n",
      "00:37:23,459 --> 00:37:28,700\n",
      "요렇게 쓰는 거는 굉장히 좋죠\n",
      "\n",
      "562\n",
      "00:37:28,700 --> 00:37:34,639\n",
      "근데 이제 아까 제가 얘기한 대로 요구해서 그러면은 요 오픈소스 LLM하고\n",
      "\n",
      "563\n",
      "00:37:34,639 --> 00:37:39,139\n",
      "여기 GPT4하고 이제 사실 요 기능의 경쟁이잖아요\n",
      "\n",
      "564\n",
      "00:37:39,139 --> 00:37:42,700\n",
      "근데 GPT4가 좋다 그런거 인정을 한다고 하는 거죠\n",
      "\n",
      "565\n",
      "00:37:42,700 --> 00:37:47,340\n",
      "그래도 쓴다 그런거 보면 뭐 아무 문제 없습니다\n",
      "\n",
      "566\n",
      "00:37:47,340 --> 00:37:54,439\n",
      "근데 이제 이렇게 썼을 때 기업의 방화벽이 이렇게 안에 있어서 다 되는데\n",
      "\n",
      "567\n",
      "00:37:54,439 --> 00:37:59,680\n",
      "문제는 요거죠 아까 얘기한 원프레머스 베타 DB가 필요한 거에요\n",
      "\n",
      "568\n",
      "00:38:00,000 --> 00:38:06,000\n",
      "외국에서 만든 그런 애들은 주겠어요? 원프레미스로? 안주죠.\n",
      "\n",
      "569\n",
      "00:38:06,000 --> 00:38:15,000\n",
      "그러니까 이제 우리 최청란 대표가 하는 원프레미스 벡터TV를 많이 사시라고 하는 겁니다.\n",
      "\n",
      "570\n",
      "00:38:15,000 --> 00:38:21,000\n",
      "그래서 이렇게 상당 부분 많이 쓰고 있고요.\n",
      "\n",
      "571\n",
      "00:38:21,000 --> 00:38:24,000\n",
      "그 다음에 이렇게 B타입이고 C타입이 있어요.\n",
      "\n",
      "572\n",
      "00:38:24,000 --> 00:38:29,000\n",
      "C타입은 뭐냐면 아까 제가 얘기한 대로 파인트닝.\n",
      "\n",
      "573\n",
      "00:38:29,000 --> 00:38:40,000\n",
      "그렇지 말고 우리 회사 데이터를 전부 다 파운데이션 몰에다 다 때려 넣어가지고 파인트닝을 하자.\n",
      "\n",
      "574\n",
      "00:38:40,000 --> 00:38:44,000\n",
      "그래서 우리 회사만의 GPT를 만들자 하는 게 C타입입니다.\n",
      "\n",
      "575\n",
      "00:38:44,000 --> 00:38:55,000\n",
      "이거는 아까 제가 얘기한 대로 소스 코드가 필요하고 소스 데이터 트레이닝 시키는 데이터가 필요하고 그 다음에 GPU가 필요하다고 했어요.\n",
      "\n",
      "576\n",
      "00:38:55,000 --> 00:39:00,000\n",
      "그래서 이거를 만들어주는데 직접 하기는 좀 그렇잖아요.\n",
      "\n",
      "577\n",
      "00:39:00,000 --> 00:39:04,840\n",
      "직접하기에는 좀 뭐 어떻게 하지? 데이터를 어떻게 만들지? 이런 게 있으니까\n",
      "\n",
      "578\n",
      "00:39:04,840 --> 00:39:09,840\n",
      "요새 굉장히 많은 벤처 기업들이 이렇게 만든다 이거죠.\n",
      "\n",
      "579\n",
      "00:39:09,840 --> 00:39:14,620\n",
      "이런 스타일로 만들어서 이렇게 이제 파인트닝을 해서 만들어드립니다.\n",
      "\n",
      "580\n",
      "00:39:14,620 --> 00:39:19,920\n",
      "해가지고 장사를 지금 많이 하고 있죠.\n",
      "\n",
      "581\n",
      "00:39:19,920 --> 00:39:23,260\n",
      "이제 매덕시테어, 이거 하나 만들었는데\n",
      "\n",
      "582\n",
      "00:39:23,260 --> 00:39:26,500\n",
      "뭐 이렇게 해서 이제 쓰는 방법이 있습니다.\n",
      "\n",
      "583\n",
      "00:39:26,500 --> 00:39:31,219\n",
      "그래서 이거는 우리는 보수는 무지 무지 중요하고\n",
      "\n",
      "584\n",
      "00:39:31,219 --> 00:39:36,139\n",
      "그다음에 기업 방화벽에서 뭐 하면 안 되고 우리 내부에서 해야 되는 거고\n",
      "\n",
      "585\n",
      "00:39:36,139 --> 00:39:40,400\n",
      "그다음에 우리 회사 데이터를 잘 분석하고 이렇게 덕변을 할 수 있도록\n",
      "\n",
      "586\n",
      "00:39:40,400 --> 00:39:44,400\n",
      "만들어야 된다라고 하는 그런 회사에서는 이렇게 갑니다.\n",
      "\n",
      "587\n",
      "00:39:44,400 --> 00:39:49,020\n",
      "아마 제가 보기에는 올해 올해 이 비즈니스에 이제 드디어 이제\n",
      "\n",
      "588\n",
      "00:39:49,020 --> 00:39:54,639\n",
      "이 비즈니스가 꽃 피우지 않을까라는 생각이 들긴 해요.\n",
      "\n",
      "589\n",
      "00:39:54,639 --> 00:39:57,480\n",
      "자 그다음에 이제 그러다 보니까 오픈AI가\n",
      "\n",
      "590\n",
      "00:39:57,480 --> 00:40:02,040\n",
      "어 보니까 오픈소스로 저렇게\n",
      "\n",
      "591\n",
      "00:40:00,000 --> 00:40:04,320\n",
      "많이 한다 이거지 그러면 우리 걸 안 쓸 거 아니에요 그래서\n",
      "\n",
      "592\n",
      "00:40:04,320 --> 00:40:10,300\n",
      "OpenAI가 뭘 발표했냐면 그러지 말고 기업 데이터를 우리한테 줘\n",
      "\n",
      "593\n",
      "00:40:10,300 --> 00:40:16,280\n",
      "우리가 GPT 3.5 Turbo 버전에다가 파인트닝을 우리가 해서 우리가 우리가 만들고\n",
      "\n",
      "594\n",
      "00:40:16,280 --> 00:40:22,840\n",
      "소스코드 우리가 해가지고 파인트닝 해서 니네 회사 버전에 GPT 3.5 Turbo 버전을\n",
      "\n",
      "595\n",
      "00:40:22,840 --> 00:40:27,799\n",
      "만들어 줄게 니네들은 어떻게 되느냐 우리 쓸 때마다 이렇게\n",
      "\n",
      "596\n",
      "00:40:27,799 --> 00:40:33,240\n",
      "토큰당 돈을 내 이게 이제 B타입이에요 이게 D타입인데\n",
      "\n",
      "597\n",
      "00:40:33,240 --> 00:40:38,639\n",
      "이게 이렇게 하는 게 약간 비싸요 이렇게 트레이닝 하는 것도 뭐\n",
      "\n",
      "598\n",
      "00:40:38,639 --> 00:40:45,720\n",
      "뭐 1,000개 토큰당 0.008달러\n",
      "\n",
      "599\n",
      "00:40:45,720 --> 00:40:51,320\n",
      "0.8센트죠. 얼마 안될 것 같지만 이거 뭐 나중에 또 보면 또 꽤 돈이 돼요\n",
      "\n",
      "600\n",
      "00:40:51,320 --> 00:40:55,599\n",
      "그러니까 이거는 뭐냐면 우리가 GPU가 필요 없어요\n",
      "\n",
      "601\n",
      "00:40:55,599 --> 00:41:01,799\n",
      "여기처럼 요거 요거 요거 할 때 GPU가 필요하잖아요 근데 요거 할 때 이제\n",
      "\n",
      "602\n",
      "00:41:00,000 --> 00:41:02,580\n",
      "외국사람 써야되니까 돈이 들잖아요?\n",
      "\n",
      "603\n",
      "00:41:02,580 --> 00:41:05,080\n",
      "이거는 돈이 안드는거에요.\n",
      "\n",
      "604\n",
      "00:41:05,080 --> 00:41:08,460\n",
      "기억데이터만 원하는 스타일로 만들어주면\n",
      "\n",
      "605\n",
      "00:41:08,460 --> 00:41:12,580\n",
      "우리는 그냥 파인트니에 해가지고 쓰면 된다고 하는거죠.\n",
      "\n",
      "606\n",
      "00:41:12,580 --> 00:41:15,220\n",
      "현재 버전이 3.5인데\n",
      "\n",
      "607\n",
      "00:41:15,220 --> 00:41:20,160\n",
      "GPT-4도 이걸 쓰게 만들겠다고 샘 알트맨이 얘기했는데\n",
      "\n",
      "608\n",
      "00:41:20,160 --> 00:41:27,500\n",
      "아직까지는 4.0까지 GPT-4도 이렇게 되지는 않은거 같아요.\n",
      "\n",
      "609\n",
      "00:41:27,500 --> 00:41:30,840\n",
      "그 다음에 GPT-4V도 이렇게 했으면 좋겠는데\n",
      "\n",
      "610\n",
      "00:41:30,840 --> 00:41:33,840\n",
      "이게 아직 GPT-4V도 안되요.\n",
      "\n",
      "611\n",
      "00:41:33,840 --> 00:41:38,180\n",
      "사실상 이제 굉장히 많은 회사에서 기다리고 있는게 뭐냐면\n",
      "\n",
      "612\n",
      "00:41:38,180 --> 00:41:41,259\n",
      "멀티모델로 GPT-4가 멀티모델인데\n",
      "\n",
      "613\n",
      "00:41:41,259 --> 00:41:42,939\n",
      "아니 멀티모델로 내는건 좋아.\n",
      "\n",
      "614\n",
      "00:41:42,939 --> 00:41:44,779\n",
      "그런데 문제는 뭐냐면\n",
      "\n",
      "615\n",
      "00:41:44,779 --> 00:41:48,980\n",
      "이미 되어있는 거를 해석을 해줄 수는 있겠지만\n",
      "\n",
      "616\n",
      "00:41:48,980 --> 00:41:51,619\n",
      "우리 회사 데이터를 가지고 학습을 시켜서\n",
      "\n",
      "617\n",
      "00:41:51,619 --> 00:41:56,740\n",
      "대부분 우리가 파워포인트나 또는 PDF로 되어있는데\n",
      "\n",
      "618\n",
      "00:41:56,740 --> 00:42:00,060\n",
      "그런거를 학습을 시켜가지고 새로운 그런 그\n",
      "\n",
      "619\n",
      "00:42:00,000 --> 00:42:04,000\n",
      "우리 회사에 딱 맞는 그런 걸 해주길 바라는데\n",
      "\n",
      "620\n",
      "00:42:04,000 --> 00:42:06,000\n",
      "이게 되면 대박이죠.\n",
      "\n",
      "621\n",
      "00:42:06,000 --> 00:42:08,000\n",
      "근데 그게 안 돼야지.\n",
      "\n",
      "622\n",
      "00:42:08,000 --> 00:42:10,000\n",
      "근데 그게 뭔 말이냐라고 하면\n",
      "\n",
      "623\n",
      "00:42:10,000 --> 00:42:14,000\n",
      "예를 들어서 우리가 과거에 만든 ppt나 pdf나 이런 거가\n",
      "\n",
      "624\n",
      "00:42:14,000 --> 00:42:18,000\n",
      "대부분의 회사의 문서에 컴퓨터에 다 들어가 있는데\n",
      "\n",
      "625\n",
      "00:42:18,000 --> 00:42:21,000\n",
      "그런 거를 얘가 학습을 해가지고\n",
      "\n",
      "626\n",
      "00:42:21,000 --> 00:42:25,000\n",
      "예를 들어서 건설사에서 돈면을 딱 보여주면\n",
      "\n",
      "627\n",
      "00:42:25,000 --> 00:42:32,000\n",
      "돈면하고 거기에 필요한 여러 가지 데이터들이 있을 거 아니에요.\n",
      "\n",
      "628\n",
      "00:42:32,000 --> 00:42:34,000\n",
      "그걸 계속해서 학습시키면\n",
      "\n",
      "629\n",
      "00:42:34,000 --> 00:42:38,000\n",
      "이 GPT4가 돈면을 읽어가지고\n",
      "\n",
      "630\n",
      "00:42:38,000 --> 00:42:40,000\n",
      "얘가 돈이 얼마나 들고\n",
      "\n",
      "631\n",
      "00:42:40,000 --> 00:42:42,000\n",
      "그 다음에 스케줄은 어떻게 되고\n",
      "\n",
      "632\n",
      "00:42:42,000 --> 00:42:45,000\n",
      "이 돈면들을 뭔가를 하려면 리스크가 어떻게 되고\n",
      "\n",
      "633\n",
      "00:42:45,000 --> 00:42:47,000\n",
      "이런 것들을 얘기해 줄 수가 있다는 거죠.\n",
      "\n",
      "634\n",
      "00:42:47,000 --> 00:42:50,000\n",
      "그런 건 뭐냐면 건설에 전문가가 생겨요.\n",
      "\n",
      "635\n",
      "00:42:51,000 --> 00:42:56,000\n",
      "변호사 사무실에다가 그동안 나왔던 팔레이나 여러 가지 정황이나\n",
      "\n",
      "636\n",
      "00:42:56,000 --> 00:43:00,000\n",
      "그런 pdf 파일 같은 걸 학습시키면\n",
      "\n",
      "637\n",
      "00:43:00,000 --> 00:43:04,840\n",
      "사진까지 학습을 시키면 전문가가 되는 거죠.\n",
      "\n",
      "638\n",
      "00:43:04,840 --> 00:43:11,840\n",
      "그래서 이게 사실은 굉장히 큰 건데 아직 텍스트만 되고\n",
      "\n",
      "639\n",
      "00:43:11,840 --> 00:43:15,840\n",
      "V버전, 멀틱모더는 안 됩니다.\n",
      "\n",
      "640\n",
      "00:43:15,840 --> 00:43:21,340\n",
      "이제 그건 GPT-5에 기대를 할 수밖에 없는 거죠.\n",
      "\n",
      "641\n",
      "00:43:21,340 --> 00:43:23,340\n",
      "만약에 GPT-5가 나오면\n",
      "\n",
      "642\n",
      "00:43:23,340 --> 00:43:28,840\n",
      "Another Level에 LLM이 생길 거다라는 생각을 많이 하죠.\n",
      "\n",
      "643\n",
      "00:43:29,180 --> 00:43:34,180\n",
      "지금 사람보다 뛰어나다 이렇게 얘기를 하는 사람도 있지만\n",
      "\n",
      "644\n",
      "00:43:34,180 --> 00:43:38,180\n",
      "어쨌든 사람들보다 뛰어나도 안 뛰어나도 그게 중요한 게 아니라\n",
      "\n",
      "645\n",
      "00:43:38,180 --> 00:43:44,180\n",
      "우리 회사의 전문가, 우리가 시입사원 불러다 놓고\n",
      "\n",
      "646\n",
      "00:43:44,180 --> 00:43:47,180\n",
      "한 15년 정도 시켜야 알잖아요.\n",
      "\n",
      "647\n",
      "00:43:47,180 --> 00:43:50,180\n",
      "그런데 GPT-5가 나와서 예를 들어서\n",
      "\n",
      "648\n",
      "00:43:50,180 --> 00:43:54,180\n",
      "우리 회사에 있는 다큐멘터리 다 이해시켜서\n",
      "\n",
      "649\n",
      "00:43:54,180 --> 00:43:59,520\n",
      "갑자기 한 달 만에 도서 같은 놈이 나왔다.\n",
      "\n",
      "650\n",
      "00:44:00,000 --> 00:44:02,000\n",
      "그럼 돈 낼만 하죠.\n",
      "\n",
      "651\n",
      "00:44:02,000 --> 00:44:05,000\n",
      "뭔가 하나 생기는데 돈 내고 월급도 안 받고\n",
      "\n",
      "652\n",
      "00:44:05,000 --> 00:44:08,000\n",
      "그래서 그런 걸 지금 기다리고 있는 거예요.\n",
      "\n",
      "653\n",
      "00:44:08,000 --> 00:44:11,000\n",
      "자, 그 다음에 마지막 우리 GPT, 타이프2는 뭐냐면\n",
      "\n",
      "654\n",
      "00:44:11,000 --> 00:44:14,000\n",
      "이거는 이제 엔터프라이즈 채집 PT라고 하는데\n",
      "\n",
      "655\n",
      "00:44:14,000 --> 00:44:19,000\n",
      "이거는 이제 그야말로 이제 데이터와 줌이야.\n",
      "\n",
      "656\n",
      "00:44:19,000 --> 00:44:21,000\n",
      "우리가 오픈 AI가 다 알아서 해줄게요.\n",
      "\n",
      "657\n",
      "00:44:21,000 --> 00:44:26,000\n",
      "보안에서부터 거기 나오는 모든 거를 다 알아서 해준다 하는 게\n",
      "\n",
      "658\n",
      "00:44:26,000 --> 00:44:30,000\n",
      "한 개 이제 엔터프라이즈 채집 PT이고\n",
      "\n",
      "659\n",
      "00:44:30,000 --> 00:44:31,000\n",
      "이거 굉장히 많이 쓰고 있어요.\n",
      "\n",
      "660\n",
      "00:44:31,000 --> 00:44:34,000\n",
      "지금 미국에서도 수백 개가 이걸 쓰고 있는데\n",
      "\n",
      "661\n",
      "00:44:34,000 --> 00:44:38,000\n",
      "작년에 오픈 AI가 흑자를 봤죠.\n",
      "\n",
      "662\n",
      "00:44:38,000 --> 00:44:43,000\n",
      "그리고 매출, 레베뉴가 2밀년 달러라고 해요.\n",
      "\n",
      "663\n",
      "00:44:43,000 --> 00:44:46,000\n",
      "22조를 넘었다는 거예요. 2조를 넘었다는 거예요.\n",
      "\n",
      "664\n",
      "00:44:46,000 --> 00:44:50,000\n",
      "그런데 토큰도 얼마 받아가지고 돈도 안 되죠.\n",
      "\n",
      "665\n",
      "00:44:50,000 --> 00:44:51,000\n",
      "그런데 이게 돈이 되는 거예요.\n",
      "\n",
      "666\n",
      "00:44:51,000 --> 00:44:53,000\n",
      "엔터프라이즈 채집 PT 이거.\n",
      "\n",
      "667\n",
      "00:44:53,000 --> 00:44:57,000\n",
      "이거 하나 만들었는데 지나가는 얘기를 하는데\n",
      "\n",
      "668\n",
      "00:44:57,000 --> 00:45:00,000\n",
      "몇 백만 달러라고 얘기하더라고요.\n",
      "\n",
      "669\n",
      "00:45:00,000 --> 00:45:08,000\n",
      "몇백만 달러? 100만 달러가 얼마니까? 13억인데 그것도 몇백만 달러니까 돈 좀 내야 되겠죠.\n",
      "\n",
      "670\n",
      "00:45:08,000 --> 00:45:11,000\n",
      "근데 우리나라에서 이거 하고 싶은 회사들이 굉장히 많아요.\n",
      "\n",
      "671\n",
      "00:45:11,000 --> 00:45:15,000\n",
      "근데 아직은 한국 지사가 안 샜기 때문에 이거 안하죠.\n",
      "\n",
      "672\n",
      "00:45:15,000 --> 00:45:19,000\n",
      "아마도 이제 샘 알트만이 25일날 온다, 내일이나요? 벌써?\n",
      "\n",
      "673\n",
      "00:45:19,000 --> 00:45:22,000\n",
      "온다고 하는데 이거 한국 지사 얘기 나올 거예요 아마.\n",
      "\n",
      "674\n",
      "00:45:22,000 --> 00:45:28,000\n",
      "뭐 저기 저 칩 만드는 것도, AI 칩을 만드는 것도 있지만 어쨌든 그럴 거라고 저는 봅니다.\n",
      "\n",
      "675\n",
      "00:45:28,000 --> 00:45:32,000\n",
      "어쨌든 오픈 AI가 이런 걸 만드는데 중요한 거는\n",
      "\n",
      "676\n",
      "00:45:32,000 --> 00:45:39,000\n",
      "그러면 우리는 어떡하냐? 우리는 어떻게 해야 되냐면 여러 가지 방식은 있죠.\n",
      "\n",
      "677\n",
      "00:45:39,000 --> 00:45:41,000\n",
      "다섯 가지 방식 중에서 하나 선택하면 되는데\n",
      "\n",
      "678\n",
      "00:45:41,000 --> 00:45:49,000\n",
      "큰 기업들은 아마도 여기 보시는 B하고 C하고 합친 거\n",
      "\n",
      "679\n",
      "00:45:49,000 --> 00:45:55,000\n",
      "관공서나 이런 데는 또는 우리는 절대 보안이 누출되면 안 된다고 하면\n",
      "\n",
      "680\n",
      "00:45:55,000 --> 00:45:59,000\n",
      "파인트닝을 한 다음에 여기다가 또 뭘 달냐면\n",
      "\n",
      "681\n",
      "00:46:00,000 --> 00:46:06,000\n",
      "이렇게 벡터 DB를 달아서 B하고 C를 합치는 이런 방식으로 가능성이 높아요.\n",
      "\n",
      "682\n",
      "00:46:06,000 --> 00:46:10,500\n",
      "왜냐하면 이거 한번 여기서 이렇게 파인팅을 해놓으면 변하지 않잖아요.\n",
      "\n",
      "683\n",
      "00:46:10,500 --> 00:46:14,500\n",
      "근데 우리 비즈니스 샵에서 계속 다큐멘트는 쌓이는 거고\n",
      "\n",
      "684\n",
      "00:46:14,500 --> 00:46:17,500\n",
      "그럼 계속 쌓이는 다큐멘트는 어디에다 보관하냐?\n",
      "\n",
      "685\n",
      "00:46:17,500 --> 00:46:21,000\n",
      "온프레미스 벡터 DB에다가 보관을 하게 되는 거죠.\n",
      "\n",
      "686\n",
      "00:46:21,000 --> 00:46:26,500\n",
      "그래서 제 생각에 대기업은 B하고 C를 갖고 왔고\n",
      "\n",
      "687\n",
      "00:46:27,500 --> 00:46:34,000\n",
      "소기업은 어떻게 하느냐? 소기업은 이렇게 A5로 가도 소기업은 아무 상관이 없죠.\n",
      "\n",
      "688\n",
      "00:46:34,000 --> 00:46:43,500\n",
      "그 다음에 나중에 GPT5가 나오거나 그런 경우에는 약간 D타입 같은 경우도 생각할 필요가 있어요.\n",
      "\n",
      "689\n",
      "00:46:43,500 --> 00:46:51,500\n",
      "그림이 있는 PDF를 학습시킬 수 있다고 하면 이거는 진지하게 고민을 해보셔야 됩니다.\n",
      "\n",
      "690\n",
      "00:46:52,000 --> 00:46:58,000\n",
      "그림과 텍스트가 같이 있는 그런 PDF가 굉장히 우리 회사는 많다고 하면\n",
      "\n",
      "691\n",
      "00:46:58,000 --> 00:47:00,000\n",
      "건설회사가 그랬고요.\n",
      "\n",
      "692\n",
      "00:47:00,000 --> 00:47:05,500\n",
      "건축 노하우를 가지고 있는 그런 회사들 이런거 대부분이죠.\n",
      "\n",
      "693\n",
      "00:47:05,500 --> 00:47:11,000\n",
      "그래서 그런 경우에는 이렇게 D타입을 고민을 해보셔라고 하는 겁니다.\n",
      "\n",
      "694\n",
      "00:47:11,000 --> 00:47:14,000\n",
      "자 그리고 이제 이 얘기는 다 하고요.\n",
      "\n",
      "695\n",
      "00:47:14,000 --> 00:47:16,500\n",
      "그 다음에 이제 마지막으로 제가 얘기하고 싶은 거는\n",
      "\n",
      "696\n",
      "00:47:16,500 --> 00:47:18,500\n",
      "비즈니스를 어떻게 활용하느냐 이런거를\n",
      "\n",
      "697\n",
      "00:47:18,500 --> 00:47:21,000\n",
      "비즈니스 활용하는 방법은 꽤 많이 있어요.\n",
      "\n",
      "698\n",
      "00:47:21,000 --> 00:47:26,500\n",
      "그래서 여기 보면은 제일 중요한게 뭐가 나오냐면 이거죠.\n",
      "\n",
      "699\n",
      "00:47:27,000 --> 00:47:32,500\n",
      "그 오픈소스 API에는 API도 있고 채집기 플러그인도 있고 펑션 컬링도 있는데\n",
      "\n",
      "700\n",
      "00:47:32,500 --> 00:47:34,500\n",
      "이게 이제 중요한게 뭐냐면 이런거에요.\n",
      "\n",
      "701\n",
      "00:47:34,500 --> 00:47:41,000\n",
      "잘 보시면 이게 이제 세계를 어떻게 바꿀거냐 이걸 이제 얘기를 하는건데\n",
      "\n",
      "702\n",
      "00:47:41,000 --> 00:47:43,500\n",
      "자 여러분 물건을 살려면 어떻게 하시죠?\n",
      "\n",
      "703\n",
      "00:47:43,500 --> 00:47:45,500\n",
      "물건을 살려면 일단 검색을 하죠.\n",
      "\n",
      "704\n",
      "00:47:45,500 --> 00:47:48,500\n",
      "검색을 해서 검색을 해가지고\n",
      "\n",
      "705\n",
      "00:47:48,500 --> 00:47:53,000\n",
      "그 판은 커머스 웹사이트를 방문을 하죠.\n",
      "\n",
      "706\n",
      "00:47:53,500 --> 00:47:57,000\n",
      "해다가 뭐 대안을 평가하는거 해가지고 웹사이트를 방문한 다음에\n",
      "\n",
      "707\n",
      "00:47:57,000 --> 00:48:00,000\n",
      "웹사이트에서 뭘 보냐 가격하고\n",
      "\n",
      "708\n",
      "00:48:00,000 --> 00:48:05,360\n",
      "그 다음에 뭐 거기 이제 나오는 그런 여러가지 그 구매 후기 이런거 보고\n",
      "\n",
      "709\n",
      "00:48:05,360 --> 00:48:09,680\n",
      "그 다음에 구매를 결심하거나 이제 포기하거나 이제 연기하거나 이제 그렇게 된다.\n",
      "\n",
      "710\n",
      "00:48:09,680 --> 00:48:15,600\n",
      "이게 이제 우리가 물건을 살 때 이런 과정인데\n",
      "\n",
      "711\n",
      "00:48:15,600 --> 00:48:19,200\n",
      "이 과정이 생각보다는 길어요. 그리고 시간이 많이 걸려요.\n",
      "\n",
      "712\n",
      "00:48:19,200 --> 00:48:23,600\n",
      "그러면 우리는 아 좀 귀찮은 사람도 있을 거고\n",
      "\n",
      "713\n",
      "00:48:23,600 --> 00:48:27,040\n",
      "뭐 내가 이렇게 하는 거에다 시간 너무 많이 쏟는 것도 있을 거고\n",
      "\n",
      "714\n",
      "00:48:27,040 --> 00:48:34,799\n",
      "정확하지 않을 수도 있고 그래서 어 이거야 야 채찍 PT 니가 말이야 내가 지금\n",
      "\n",
      "715\n",
      "00:48:34,799 --> 00:48:40,959\n",
      "운동화를 살라고 그러는데 니가 좀 한번 어디서 좀 알아봐라 나한테 맞는거\n",
      "\n",
      "716\n",
      "00:48:40,959 --> 00:48:46,240\n",
      "요렇게 딱 질문하게 되면 얘가 또 기가 막히게 찾아가지고 옵션을 딱 얘기를 해서\n",
      "\n",
      "717\n",
      "00:48:46,240 --> 00:48:51,520\n",
      "거기에 파는 판매까지 쫙 얘기를 해주고 그 다음에 이제 그거를 내가\n",
      "\n",
      "718\n",
      "00:48:51,520 --> 00:48:56,959\n",
      "카드 번호를 딱 주면은 지가 알아가지고 만약에 이제 구매까지 해준다고 치면\n",
      "\n",
      "719\n",
      "00:48:56,959 --> 00:49:02,160\n",
      "이거 대박이죠 이거. 만약에 그렇게 된다고\n",
      "\n",
      "720\n",
      "00:49:00,000 --> 00:49:04,000\n",
      "그러면 전자상거래가 변화되는거에요.\n",
      "\n",
      "721\n",
      "00:49:04,000 --> 00:49:13,000\n",
      "여러분 혹시 네이버에서 만든 Q라는걸 써보셨어요?\n",
      "\n",
      "722\n",
      "00:49:13,000 --> 00:49:21,000\n",
      "Q라는걸 써보시면 진짜 이런거 비슷하게 해야되요.\n",
      "\n",
      "723\n",
      "00:49:21,000 --> 00:49:28,000\n",
      "네이버는 자사몰, 네이버 쇼핑몰에 있는 점주도 있잖아요.\n",
      "\n",
      "724\n",
      "00:49:28,000 --> 00:49:33,000\n",
      "컴퓨터에 딱 치면 고기만 나와요. 고기만 나와서 클릭하면 안으로 들어가게 만드는.\n",
      "\n",
      "725\n",
      "00:49:33,000 --> 00:49:39,000\n",
      "요거는 단계 1이라고 생각할 수 있는거죠.\n",
      "\n",
      "726\n",
      "00:49:39,000 --> 00:49:49,000\n",
      "그래서 만약에 채찍 PT를 활용해서 구매를 대행해주는 사이트를 만든다고 치면\n",
      "\n",
      "727\n",
      "00:49:49,000 --> 00:49:55,000\n",
      "이거야말로 앞으로 유통을 뒤바꿀 수 있는 그런 비즈니스가 된다.\n",
      "\n",
      "728\n",
      "00:49:55,000 --> 00:50:00,000\n",
      "앞으로 조금 있으면 아마존하고 검색하는 구글에서\n",
      "\n",
      "729\n",
      "00:50:00,000 --> 00:50:04,800\n",
      "사라진다고 얘기했잖아요. 딱 그 얘기가 생기는 거에요.\n",
      "\n",
      "730\n",
      "00:50:04,800 --> 00:50:10,300\n",
      "지금 뭐냐면 채찍기치나 LLM을 가지고 데이터를 써가지고\n",
      "\n",
      "731\n",
      "00:50:10,300 --> 00:50:14,800\n",
      "보고서 작성하고 글 작성하고 답변하고 뭐 이런 거잖아요.\n",
      "\n",
      "732\n",
      "00:50:14,800 --> 00:50:20,000\n",
      "근데 앞으로 어떻게 하냐면 앞으로는 얘한테 뭐든지 얘기를 해.\n",
      "\n",
      "733\n",
      "00:50:20,000 --> 00:50:28,000\n",
      "그러면 걔가 웹사이트를 다 뒤져가지고 우리가 원하는 걸 다 알아서 갖다 준다고 하는 거죠.\n",
      "\n",
      "734\n",
      "00:50:28,000 --> 00:50:30,400\n",
      "이게 뭔 말이냐 이거에요.\n",
      "\n",
      "735\n",
      "00:50:30,400 --> 00:50:39,000\n",
      "우리가 여행을 가려면 엑스페디아 하나, 우라기풍선, 복킹닷컴, 에어비앤비 등등 돌아다니면서 내가 다 해야 된단 말이에요.\n",
      "\n",
      "736\n",
      "00:50:39,000 --> 00:50:45,000\n",
      "여행사 패키지 상품을 사지 않는다 하면 다 이래야 되는 거죠.\n",
      "\n",
      "737\n",
      "00:50:45,000 --> 00:50:47,500\n",
      "내가 카드 결제를 해야 되는 거고.\n",
      "\n",
      "738\n",
      "00:50:47,500 --> 00:50:55,000\n",
      "근데 만약에 LLM으로 여행을 대행해주는 사이트 하나 만든다고 하면\n",
      "\n",
      "739\n",
      "00:50:55,000 --> 00:50:57,500\n",
      "이 사람이 카드까지 갖다주면\n",
      "\n",
      "740\n",
      "00:50:57,500 --> 00:51:00,000\n",
      "너 일본 후쿠오카 가고 싶은데\n",
      "\n",
      "741\n",
      "00:51:00,000 --> 00:51:05,340\n",
      "좋은데 한번 얘기해봐 그러면 얘가 엑스페디아나 하나토나 노랑북송\n",
      "\n",
      "742\n",
      "00:51:05,340 --> 00:51:10,480\n",
      "여기 웹사이트잖아 이거 다 뒤져 가지고 가장 좋은 놈으로 비교 선택을\n",
      "\n",
      "743\n",
      "00:51:10,480 --> 00:51:14,820\n",
      "해서 후기까지 다 읽어 가지고 거기에 대한 답변을 딱 해준다 라고 하는\n",
      "\n",
      "744\n",
      "00:51:14,820 --> 00:51:15,820\n",
      "거죠.\n",
      "\n",
      "745\n",
      "00:51:15,820 --> 00:51:23,139\n",
      "그럼 1번 2번 3번 4번 5번 자 그럼 3번 좋은데 3번 부킹 그러면 카드\n",
      "\n",
      "746\n",
      "00:51:23,139 --> 00:51:28,340\n",
      "딱 들고 이 LLM이 그걸 다 부킹을 해줘요.\n",
      "\n",
      "747\n",
      "00:51:28,340 --> 00:51:31,940\n",
      "만약에 그렇다고 하면 여러분 이거 대박인거죠.\n",
      "\n",
      "748\n",
      "00:51:31,940 --> 00:51:33,459\n",
      "그러면 밥 사시는거죠.\n",
      "\n",
      "749\n",
      "00:51:33,459 --> 00:51:37,380\n",
      "하나토나 노랑북송 에어비앤비 다 필요없죠.\n",
      "\n",
      "750\n",
      "00:51:37,380 --> 00:51:43,939\n",
      "물론 내가 만드는 이 사이트에 연결을 해가지고 개인들의 상품을 여기다\n",
      "\n",
      "751\n",
      "00:51:43,939 --> 00:51:49,180\n",
      "올려주면 내가 그걸 다 알아서 해준다는 이런 방식의 비즈니스를 앞으로 하게\n",
      "\n",
      "752\n",
      "00:51:49,180 --> 00:51:52,860\n",
      "될거고 이런 비즈니스를 추구하게 될거다 라고 하는거죠.\n",
      "\n",
      "753\n",
      "00:51:52,860 --> 00:51:53,860\n",
      "그러면 이게 뭐냐.\n",
      "\n",
      "754\n",
      "00:51:53,939 --> 00:51:59,259\n",
      "이거는 뒤에 우리가 하나토나 노랑북송 부킹닷컴 에어비앤비 이런거 플랫폼\n",
      "\n",
      "755\n",
      "00:51:59,259 --> 00:52:00,020\n",
      "이라고 그러잖아요.\n",
      "\n",
      "756\n",
      "00:52:00,000 --> 00:52:03,600\n",
      "근데 이게 플랫폼 위에다가 또 플랫폼을 또 만드는 거예요.\n",
      "\n",
      "757\n",
      "00:52:03,600 --> 00:52:06,800\n",
      "이거는 플랫폼 of 플랫폼이 되는 거죠.\n",
      "\n",
      "758\n",
      "00:52:06,800 --> 00:52:11,600\n",
      "그럼 이런 관점은 지금까지 보지 못한 관점인 거고\n",
      "\n",
      "759\n",
      "00:52:11,600 --> 00:52:16,799\n",
      "그런 거는 새로운 비즈니스를 완전히 휩쓸 수 있는 그런 대단한 거다.\n",
      "\n",
      "760\n",
      "00:52:16,799 --> 00:52:18,799\n",
      "근데 문제는 이제 내가 딱 쳤을 때\n",
      "\n",
      "761\n",
      "00:52:18,799 --> 00:52:24,799\n",
      "여기 다 뒤져가지고 다 이걸 갖다가 해낼 수 있는 그런 스피드가 나올까?\n",
      "\n",
      "762\n",
      "00:52:24,799 --> 00:52:26,799\n",
      "하는 그런 부분은 있고\n",
      "\n",
      "763\n",
      "00:52:26,799 --> 00:52:30,799\n",
      "왜냐하면 이거 자체가 굉장히 복잡한 코딩이기 때문에\n",
      "\n",
      "764\n",
      "00:52:30,799 --> 00:52:33,799\n",
      "이거 굉장히 잘할 수 있는 회사가 어디냐면 오픈AI죠.\n",
      "\n",
      "765\n",
      "00:52:33,799 --> 00:52:38,799\n",
      "오픈AI, 지가 진에다가 만든 그런 에이전트를 가지고 진에다가 한다고 하면\n",
      "\n",
      "766\n",
      "00:52:38,799 --> 00:52:39,799\n",
      "경쟁자가 없어 보여요.\n",
      "\n",
      "767\n",
      "00:52:39,799 --> 00:52:46,799\n",
      "근데 가만히 제가 작년서부터 오픈AI가 이런 비즈니스를 하지 않을까 해가지고 지켜봤는데\n",
      "\n",
      "768\n",
      "00:52:46,799 --> 00:52:49,799\n",
      "오픈AI는 그런 비즈니스에 관심이 없는 거야.\n",
      "\n",
      "769\n",
      "00:52:49,799 --> 00:52:52,799\n",
      "가만히, 고맙게도.\n",
      "\n",
      "770\n",
      "00:52:52,799 --> 00:52:56,799\n",
      "오픈AI가 뭐라고 하냐면 자기네들은 돈을 버는 게\n",
      "\n",
      "771\n",
      "00:52:56,799 --> 00:52:59,799\n",
      "AI를 연구하기 위해서 돈을 버는 거지.\n",
      "\n",
      "772\n",
      "00:53:00,000 --> 00:53:05,280\n",
      "자기네들은 돈을 벌기 위해서 AI를 하지 않는다 이렇게 얘기하는거에요\n",
      "\n",
      "773\n",
      "00:53:05,280 --> 00:53:06,920\n",
      "되게 멋있죠?\n",
      "\n",
      "774\n",
      "00:53:06,920 --> 00:53:09,120\n",
      "그러니까 이런 돈을 벌기 위한\n",
      "\n",
      "775\n",
      "00:53:09,120 --> 00:53:13,360\n",
      "뭐 이거는 지금까지 나와있는 그런 AI 기술을 쓰면 되니까\n",
      "\n",
      "776\n",
      "00:53:13,360 --> 00:53:16,000\n",
      "근데 이런거를 만들지는 않는다고 하는거죠\n",
      "\n",
      "777\n",
      "00:53:16,000 --> 00:53:18,200\n",
      "오픈웨어 가끔 얘기하는거\n",
      "\n",
      "778\n",
      "00:53:18,200 --> 00:53:22,360\n",
      "우리는 고객이랑 경쟁 안해요 뭐 이런 얘기도 하고 그러니까\n",
      "\n",
      "779\n",
      "00:53:22,360 --> 00:53:27,719\n",
      "오케이 그러면 누구든지 이거에 대한 아이디어를 생각해가지고\n",
      "\n",
      "780\n",
      "00:53:27,719 --> 00:53:32,520\n",
      "비즈니스를 만들면 이런거 대박인거죠\n",
      "\n",
      "781\n",
      "00:53:32,520 --> 00:53:37,599\n",
      "그런데 이게 여행 제가 이해를 했지만은 사실상\n",
      "\n",
      "782\n",
      "00:53:37,599 --> 00:53:41,000\n",
      "이커머스 물건 사는거나 여행이나 금융 투자나\n",
      "\n",
      "783\n",
      "00:53:41,000 --> 00:53:45,240\n",
      "금융이나 투자 뭐 부동산 매매 중고품 판매 등등등\n",
      "\n",
      "784\n",
      "00:53:45,240 --> 00:53:47,160\n",
      "무시하게 많잖아요 여기\n",
      "\n",
      "785\n",
      "00:53:47,160 --> 00:53:49,400\n",
      "그럼 이런걸 내가 다 받아 쓸 수 있는거에요\n",
      "\n",
      "786\n",
      "00:53:49,400 --> 00:53:51,680\n",
      "플랫폼이\n",
      "\n",
      "787\n",
      "00:53:51,680 --> 00:53:55,919\n",
      "그럼 이런거를 안하고 무슨 일을 하겠다라고 하는거냐 이거죠\n",
      "\n",
      "788\n",
      "00:53:55,919 --> 00:53:57,080\n",
      "제가 보기에는\n",
      "\n",
      "789\n",
      "00:53:57,080 --> 00:54:00,160\n",
      "우리나라에서 많은 사람들이 이런거 했으면 좋겠어요\n",
      "\n",
      "790\n",
      "00:54:00,000 --> 00:54:06,140\n",
      "이거 뭘 할 수 있느냐? AI 에이전트에요. AI 에이전트 뒤지면 다 나오죠.\n",
      "\n",
      "791\n",
      "00:54:06,140 --> 00:54:11,220\n",
      "그리고 오픈 AI 이런걸 뭐 하는걸 이런걸 하는걸 뭐라고 만들었냐면\n",
      "\n",
      "792\n",
      "00:54:11,220 --> 00:54:14,840\n",
      "어시스턴트 API 라고 했었어요.\n",
      "\n",
      "793\n",
      "00:54:14,840 --> 00:54:19,580\n",
      "그래서 어시스턴트 API를 공부를 하셔가지고 이런 비즈니스를 만드시면 됩니다.\n",
      "\n",
      "794\n",
      "00:54:19,580 --> 00:54:23,420\n",
      "그 다음에 이제 또 얘기하는게 뭐냐면\n",
      "\n",
      "795\n",
      "00:54:23,420 --> 00:54:26,500\n",
      "로봇이에요. 로봇. 제가 이제 이번에 그\n",
      "\n",
      "796\n",
      "00:54:26,500 --> 00:54:32,320\n",
      "CES에 갔다 왔는데 로봇이 엄청 많이 나올 것 같은데 사실 그렇게 많이 나오지는 않았어요.\n",
      "\n",
      "797\n",
      "00:54:32,320 --> 00:54:36,000\n",
      "근데 로봇이 지금 LLM 하고 뭐가 중요하냐면요\n",
      "\n",
      "798\n",
      "00:54:36,000 --> 00:54:41,259\n",
      "지금까지 로봇은 팔 움직이고 이렇게 가고 저벅저벅 걸어가고 이런게\n",
      "\n",
      "799\n",
      "00:54:41,259 --> 00:54:47,080\n",
      "엄청 쉬울 것 같지만 그렇지 않아요. 왜냐하면 이거 다 코딩을 해야 되는 겁니다. 개발을 해야 돼요.\n",
      "\n",
      "800\n",
      "00:54:47,080 --> 00:54:54,400\n",
      "뭐냐면 이게 개발이라는게 팔을 어떻게 움직이고 어떻게 걷고 이렇게 밸런스를 맞추고 하는 이런게\n",
      "\n",
      "801\n",
      "00:54:54,400 --> 00:55:01,740\n",
      "전부다 다 이 로봇 엔지니어들이 개발을 하는 겁니다.\n",
      "\n",
      "802\n",
      "00:55:00,000 --> 00:55:07,000\n",
      "로봇은 개발된 놈만 움직이게 되어 있는 거죠.\n",
      "\n",
      "803\n",
      "00:55:07,000 --> 00:55:11,000\n",
      "로봇에 개발되지 않는다면 어떻게 할지 몰라요.\n",
      "\n",
      "804\n",
      "00:55:11,000 --> 00:55:14,000\n",
      "LLM이 뭘 할 수 있냐면,\n",
      "\n",
      "805\n",
      "00:55:14,000 --> 00:55:18,000\n",
      "LLM이 코딩을 제너레이션 해주잖아요.\n",
      "\n",
      "806\n",
      "00:55:18,000 --> 00:55:25,000\n",
      "그래서 이 코드 제너레이션 하는 놈을 LLM에 특화시켜서\n",
      "\n",
      "807\n",
      "00:55:25,000 --> 00:55:35,000\n",
      "그러니까 로봇의 각 부분 부분을 움직이는 코드를 제너레이션 해주는 LLM을 만든다고 하는 거죠.\n",
      "\n",
      "808\n",
      "00:55:35,000 --> 00:55:39,000\n",
      "그러면 어떻게 되느냐. 그러면 내가 인간이 말로 해.\n",
      "\n",
      "809\n",
      "00:55:39,000 --> 00:55:43,000\n",
      "저기 있는 저 빨간 물건을 잡아가지고 이리로 가져와 라고 하면\n",
      "\n",
      "810\n",
      "00:55:43,000 --> 00:55:49,000\n",
      "그때 실시간으로 이 로봇한테 저기 빨간 물건을 보고\n",
      "\n",
      "811\n",
      "00:55:49,000 --> 00:55:58,000\n",
      "그 놈한테 가서 나한테 가져다 주는 그런 로봇을 만들 수 있다고 하는 그런 뜻이에요.\n",
      "\n",
      "812\n",
      "00:55:58,000 --> 00:56:00,000\n",
      "그럼 대박인거지 이거는.\n",
      "\n",
      "813\n",
      "00:56:00,000 --> 00:56:05,000\n",
      "이거야 말로 정말 앞으로 어마어마하게 많은 서버가 올 수 있는 게 많은 거죠.\n",
      "\n",
      "814\n",
      "00:56:05,000 --> 00:56:11,000\n",
      "드론에다가 얘기해가지고, 야 저기 드론 저기 위에 가서 이거 가져와.\n",
      "\n",
      "815\n",
      "00:56:11,000 --> 00:56:16,000\n",
      "또는 이 폭탄 가지고 저기다 가서 떨어뜨려 그러면 계속 알아들 거 아니에요.\n",
      "\n",
      "816\n",
      "00:56:16,000 --> 00:56:19,000\n",
      "기계에 사람, 인간의 말을 알아 듣는다.\n",
      "\n",
      "817\n",
      "00:56:19,000 --> 00:56:23,000\n",
      "이 얘기는 이 로봇의 월드가 근본적으로 변한다고 하는 거고,\n",
      "\n",
      "818\n",
      "00:56:23,000 --> 00:56:31,000\n",
      "이게 올해 어마어마하게 개발이 많이 되는 그런 분야다.\n",
      "\n",
      "819\n",
      "00:56:31,000 --> 00:56:36,000\n",
      "기업을 위한 파운데이션 모델인데, 이거는 조금 더 뒤에 얘기해요.\n",
      "\n",
      "820\n",
      "00:56:36,000 --> 00:56:41,000\n",
      "조금 더 뒤에 얘기인데, 요새는 파운데이션 모델,\n",
      "\n",
      "821\n",
      "00:56:41,000 --> 00:56:48,000\n",
      "아까 얘기했던 GPT4나 이런 파운데이션 모델이 AI 모델을 컨트롤 할 수가 있어요.\n",
      "\n",
      "822\n",
      "00:56:48,000 --> 00:56:52,000\n",
      "그러니까 AI 모델을 검색할 수도 있고, AI 모델을 실행할 수도 있고,\n",
      "\n",
      "823\n",
      "00:56:52,000 --> 00:57:00,000\n",
      "AI 모델이 뭔가 메시지가 나오는 걸 알아 듣고 그걸 지시할 수도 있고.\n",
      "\n",
      "824\n",
      "00:57:00,000 --> 00:57:02,580\n",
      "이런 파운데이션 모델이 나온다고 합니다.\n",
      "\n",
      "825\n",
      "00:57:02,580 --> 00:57:07,160\n",
      "이런 걸 쓸 때는 각 공장이나 기업에 있는\n",
      "\n",
      "826\n",
      "00:57:07,160 --> 00:57:11,580\n",
      "각 여러 가지 공장이나 기업에는 다 센서들이 많잖아요.\n",
      "\n",
      "827\n",
      "00:57:11,580 --> 00:57:13,920\n",
      "센서를 AI에 다 집어넣죠.\n",
      "\n",
      "828\n",
      "00:57:13,920 --> 00:57:15,420\n",
      "온 디바이스 AI잖아요.\n",
      "\n",
      "829\n",
      "00:57:15,420 --> 00:57:20,639\n",
      "거기서 나오는 중요한 메시지나 신호나 이런 데이터들을\n",
      "\n",
      "830\n",
      "00:57:20,639 --> 00:57:24,219\n",
      "파운데이션 모델을 받아가지고 그걸 해석하는 거지.\n",
      "\n",
      "831\n",
      "00:57:24,219 --> 00:57:26,180\n",
      "해석을 해서 상황 판단을 하는 거야.\n",
      "\n",
      "832\n",
      "00:57:26,180 --> 00:57:27,840\n",
      "상황 판단을 하고\n",
      "\n",
      "833\n",
      "00:57:27,840 --> 00:57:32,580\n",
      "그 다음에 문제가 생겼다, 어떤 특정 지점에 뭐가 깨졌다, 무너졌다,\n",
      "\n",
      "834\n",
      "00:57:32,580 --> 00:57:36,220\n",
      "뭐가 생기면 거기에 대한 시뮬레이션을 하는 거야.\n",
      "\n",
      "835\n",
      "00:57:36,220 --> 00:57:38,099\n",
      "AI가 LLM이.\n",
      "\n",
      "836\n",
      "00:57:38,099 --> 00:57:42,520\n",
      "그래서 대륜책 1, 2, 3를 만들어가지고 거기다 인간한테 보고를 하고\n",
      "\n",
      "837\n",
      "00:57:42,520 --> 00:57:46,860\n",
      "거기서 인간이 뭐라고 답변을 해주면 그대로 실행하게 되는\n",
      "\n",
      "838\n",
      "00:57:46,860 --> 00:57:50,660\n",
      "이런 스타일의 AI 모델이 나온다고 하는 거예요.\n",
      "\n",
      "839\n",
      "00:57:50,660 --> 00:57:55,320\n",
      "그러니까 AI 모델이 AI를 컨트롤하는\n",
      "\n",
      "840\n",
      "00:57:55,320 --> 00:58:00,180\n",
      "그런 게 이제 지금 아마 오픈 AI 파일로도 그런 거를\n",
      "\n",
      "841\n",
      "00:58:00,000 --> 00:58:05,500\n",
      "가고 있을 거고 지금 이제 이 쪽 많은 쪽에 있는 사람들이 이렇게 해서\n",
      "\n",
      "842\n",
      "00:58:05,500 --> 00:58:10,260\n",
      "공장에 있는 것들을 갖다가 받으러 들여가지고 상황 판단을 한 다음에\n",
      "\n",
      "843\n",
      "00:58:10,260 --> 00:58:16,379\n",
      "시뮬레이션 전략을 얘가 만들어 가지고 사람한테 시나리오 1,2,3\n",
      "\n",
      "844\n",
      "00:58:16,379 --> 00:58:21,219\n",
      "보고를 해서 의사결정을 해가지고 실행하게 되는 이런 이제 조금 이렇게\n",
      "\n",
      "845\n",
      "00:58:21,219 --> 00:58:26,180\n",
      "되면 이제 사람이 이제 기업 내 공장 내에 사람이 없어지는 거죠.\n",
      "\n",
      "846\n",
      "00:58:26,180 --> 00:58:29,059\n",
      "사람을 없애는 그런 시대가 온다라고 하는 겁니다.\n",
      "\n",
      "847\n",
      "00:58:29,059 --> 00:58:35,540\n",
      "그래서 이제 중요한 거는 뭐 AGI가 언제 오느냐 지금 사실 이건 난리인데\n",
      "\n",
      "848\n",
      "00:58:35,540 --> 00:58:37,380\n",
      "AGI가 뭐냐 라고 하면 다 틀려요.\n",
      "\n",
      "849\n",
      "00:58:37,380 --> 00:58:40,580\n",
      "그러면 뭐 AGI가 언제 오느냐 했다는 사람마다 너무 틀려가지고는 뭐\n",
      "\n",
      "850\n",
      "00:58:40,580 --> 00:58:41,580\n",
      "말장난이죠.\n",
      "\n",
      "851\n",
      "00:58:41,580 --> 00:58:44,580\n",
      "제가 저번에 얘기했던 얘기가 그거에요.\n",
      "\n",
      "852\n",
      "00:58:44,580 --> 00:58:46,980\n",
      "AGI가 기업 입장에서 언제 오는지 상관없어요.\n",
      "\n",
      "853\n",
      "00:58:46,980 --> 00:58:48,459\n",
      "빨리 오면 좋죠.\n",
      "\n",
      "854\n",
      "00:58:48,459 --> 00:58:54,259\n",
      "근데 어쨌든 그 현재 있는 LRM 기술이나 잘 활용을 해서 우리 비즈니스에\n",
      "\n",
      "855\n",
      "00:58:54,259 --> 00:58:57,580\n",
      "쓰는 게 최고다라고 생각하면 되는 거다 라고 하죠.\n",
      "\n",
      "856\n",
      "00:58:57,580 --> 00:59:00,020\n",
      "그래서 이제 마지막으로 제가 말씀드리면은\n",
      "\n",
      "857\n",
      "00:59:00,000 --> 00:59:02,340\n",
      "여기서 우리 직원들이 있단 말이에요.\n",
      "\n",
      "858\n",
      "00:59:02,340 --> 00:59:04,840\n",
      "그러면 회사원들은 고민이 많아요.\n",
      "\n",
      "859\n",
      "00:59:04,840 --> 00:59:08,840\n",
      "왜냐면 이런 새로운 기술들이 떨어지는데\n",
      "\n",
      "860\n",
      "00:59:08,840 --> 00:59:11,540\n",
      "그러면 우리 기업은 이거하고 어떻게 접목을 해야 돼?\n",
      "\n",
      "861\n",
      "00:59:11,540 --> 00:59:13,780\n",
      "또 어떻게 우리 직원들이 이걸 교육시켜야 되고\n",
      "\n",
      "862\n",
      "00:59:13,780 --> 00:59:16,180\n",
      "우리가 어떻게 이걸 활용하느냐 하는 거예요.\n",
      "\n",
      "863\n",
      "00:59:16,180 --> 00:59:18,780\n",
      "근데 직원들의 생각은 어떻게 되냐면\n",
      "\n",
      "864\n",
      "00:59:18,780 --> 00:59:20,700\n",
      "야, 그거는 내 일이 아니고\n",
      "\n",
      "865\n",
      "00:59:20,700 --> 00:59:23,740\n",
      "뭐 사정이 되니까 나는 몰라 라고 생각들 하죠.\n",
      "\n",
      "866\n",
      "00:59:23,740 --> 00:59:25,840\n",
      "근데 여기서 중요한 건 뭐냐면\n",
      "\n",
      "867\n",
      "00:59:25,840 --> 00:59:28,980\n",
      "AI 부서를 만드는 거나 전산실이나\n",
      "\n",
      "868\n",
      "00:59:28,980 --> 00:59:34,480\n",
      "협업 담당자들이 함께 아까 얘기했던 그런\n",
      "\n",
      "869\n",
      "00:59:34,480 --> 00:59:38,720\n",
      "프롬프트 엔지니어링 TF팀을 만들어서\n",
      "\n",
      "870\n",
      "00:59:38,720 --> 00:59:41,520\n",
      "함께 이걸 계속해서 얘기를 하지 않으면 안 돼요.\n",
      "\n",
      "871\n",
      "00:59:41,520 --> 00:59:44,720\n",
      "그러니까 외부에서 AI 전문가를 뽑아도 문제가 뭐냐면\n",
      "\n",
      "872\n",
      "00:59:44,720 --> 00:59:46,980\n",
      "걔는 우리 회사 비즈니스를 모르잖아.\n",
      "\n",
      "873\n",
      "00:59:46,980 --> 00:59:50,020\n",
      "그렇기 때문에 AI를 어디에다 적용할지 모른다고 하는 거죠.\n",
      "\n",
      "874\n",
      "00:59:50,020 --> 00:59:53,279\n",
      "그래서 중요한 건 부서 간의 협업이고\n",
      "\n",
      "875\n",
      "00:59:53,279 --> 00:59:55,380\n",
      "맨날 똑같은 얘기지만\n",
      "\n",
      "876\n",
      "00:59:55,380 --> 00:59:57,720\n",
      "이거를 전대를 진두지휘하고\n",
      "\n",
      "877\n",
      "00:59:57,720 --> 01:00:00,119\n",
      "이걸 갖다가 특별한 사람으로\n",
      "\n",
      "878\n",
      "01:00:00,000 --> 01:00:06,480\n",
      "사람들이 이걸 전체 통합해서 운영해 주는 그런 구직적 체계가 필요하다.\n",
      "\n",
      "879\n",
      "01:00:06,480 --> 01:00:09,000\n",
      "ai 문화가 필요하다라고 저는 얘기를 합니다.\n",
      "\n",
      "880\n",
      "01:00:09,000 --> 01:00:10,720\n",
      "제가 강의 여기까지 하겠습니다.\n",
      "\n",
      "881\n",
      "01:00:10,720 --> 01:00:16,040\n",
      "참여해주셔서 정말 어떻게 감사를 표현해야 될지 모르겠습니다.\n",
      "\n",
      "882\n",
      "01:00:16,040 --> 01:00:25,440\n",
      "오늘 되게 바쁘신 시간인데 많은 분들이 참석을 해서 경청을 많이 해주셨어요.\n",
      "\n",
      "883\n",
      "01:00:25,440 --> 01:00:34,639\n",
      "그래서 여러 가지 기업의 규모나 그리고 하고자 하는 현재 상황이나 목적에 따라서\n",
      "\n",
      "884\n",
      "01:00:34,639 --> 01:00:40,160\n",
      "이것들을 어떻게 대응을 해야 되는지가 정말 LNM 모델처럼\n",
      "\n",
      "885\n",
      "01:00:40,160 --> 01:00:48,400\n",
      "파운데이션 모델처럼 전체를 다 꿰고 있지 않고는 대안을 마련하기가 참 어려운 상황인데\n",
      "\n",
      "886\n",
      "01:00:48,400 --> 01:00:53,840\n",
      "우리에게는 장동인 교수님이 계셔서 정말 파운데이션 모델처럼\n",
      "\n",
      "887\n",
      "01:00:53,840 --> 01:01:00,439\n",
      "아까 마지막에 발표해 주신 것처럼 어떤 시나리오를 가지고 대응을 해야 되는지 설명을 드릴 수 있습니다.\n",
      "\n",
      "888\n",
      "01:01:00,000 --> 01:01:02,200\n",
      "있었던 아주 좋은 시간이었던 것 같습니다.\n",
      "\n",
      "889\n",
      "01:01:02,200 --> 01:01:06,840\n",
      "그러면 디지털 트랜스포메이션 얘기를 굉장히 많이 했잖아요.\n",
      "\n",
      "890\n",
      "01:01:06,840 --> 01:01:11,080\n",
      "한 10년 넘게 했는데 실제로 기업체 내부에 들어가 보면\n",
      "\n",
      "891\n",
      "01:01:11,080 --> 01:01:16,719\n",
      "그거 제대로 적용하는 큰 회사는 말고 작은 회사들은\n",
      "\n",
      "892\n",
      "01:01:16,719 --> 01:01:20,120\n",
      "거의 엄두도 못 내는 것 같고 공공 분야에서 제가\n",
      "\n",
      "893\n",
      "01:01:20,120 --> 01:01:22,920\n",
      "작년까지 공공 분야에서 일을 했는데\n",
      "\n",
      "894\n",
      "01:01:22,920 --> 01:01:27,000\n",
      "공공 분야에서는 크라우드 도입하는 것도\n",
      "\n",
      "895\n",
      "01:01:27,639 --> 01:01:31,879\n",
      "법률 때문에 못하고 있고 그렇게 됐던데\n",
      "\n",
      "896\n",
      "01:01:31,879 --> 01:01:36,480\n",
      "우리 장 교수님 보니까 사스코리아에서도 일을 하시고\n",
      "\n",
      "897\n",
      "01:01:36,480 --> 01:01:40,959\n",
      "영앤 컨설팅 회사에서 일을 하신 경험이 계신 것 같은데\n",
      "\n",
      "898\n",
      "01:01:40,959 --> 01:01:47,279\n",
      "혹시 사스에 아직 계시다라고 본인이 그렇게 생각을 하신다면\n",
      "\n",
      "899\n",
      "01:01:47,279 --> 01:01:51,480\n",
      "AI 트랜스포메이션, AX 얘기를 많이 하는데\n",
      "\n",
      "900\n",
      "01:01:51,520 --> 01:01:57,520\n",
      "AX의 관점에서 중소기업들 사스코리아에 계셨다고 하면\n",
      "\n",
      "901\n",
      "01:01:57,520 --> 01:01:59,919\n",
      "어떤 지금 아까 말씀하셨던\n",
      "\n",
      "902\n",
      "01:02:00,000 --> 01:02:07,580\n",
      "그런 이제 도움들을 해주셨는데 사실 그게 CEO가 이렇게 결정해서\n",
      "\n",
      "903\n",
      "01:02:07,580 --> 01:02:11,840\n",
      "그 부서에다 얘기를 한다고 해서 진행되는 건 아니잖아요?\n",
      "\n",
      "904\n",
      "01:02:11,840 --> 01:02:17,680\n",
      "그러면은 이런 컨설팅 업체를 통해서 이런 걸 도입을 해야 되는 건지\n",
      "\n",
      "905\n",
      "01:02:17,680 --> 01:02:20,920\n",
      "공공기관에서는 또 어떻게 해야 되는 건지 그 두 가지\n",
      "\n",
      "906\n",
      "01:02:20,920 --> 01:02:24,400\n",
      "디지털 트랜스포메이션 말은 좋게 엄청나게 많이 하잖아요\n",
      "\n",
      "907\n",
      "01:02:24,400 --> 01:02:27,000\n",
      "근데 이게 다 안 돼요 왜 안 되느냐\n",
      "\n",
      "908\n",
      "01:02:27,000 --> 01:02:31,040\n",
      "디지털 트랜스포메이션을 할 때에 정식 부서는 어떻게 하냐면요\n",
      "\n",
      "909\n",
      "01:02:31,040 --> 01:02:34,439\n",
      "DT 본부를 하나 만들어요\n",
      "\n",
      "910\n",
      "01:02:34,439 --> 01:02:37,580\n",
      "DT 당당 임원을 하죠\n",
      "\n",
      "911\n",
      "01:02:37,580 --> 01:02:42,580\n",
      "이 임원이 말하자면 이제 DX 임원이라고 많이 하죠 DX\n",
      "\n",
      "912\n",
      "01:02:42,580 --> 01:02:46,279\n",
      "그 다음에 팀도 거기 밑에다가 팀 만들면 뭐 대개 빅데이터 팀\n",
      "\n",
      "913\n",
      "01:02:46,279 --> 01:02:51,959\n",
      "그 다음에 뭐 AI팀 이런 신기술 쪽으로 해놓고 외부에서 뽑아요\n",
      "\n",
      "914\n",
      "01:02:51,959 --> 01:02:54,000\n",
      "대기업이 이제 이렇게 하거든요\n",
      "\n",
      "915\n",
      "01:02:54,000 --> 01:02:59,000\n",
      "그러면 대기업이 그걸 해가지고 지금 진짜 디지털 트랜스포메이션에 대한\n",
      "\n",
      "916\n",
      "01:02:59,000 --> 01:03:02,000\n",
      "뭘 했냐라고 하면\n",
      "\n",
      "917\n",
      "01:03:00,000 --> 01:03:07,520\n",
      "못하죠. 대개 신문에 나오는 디지털로 어쩌고 어쩌고 많이 얘기를 하는데\n",
      "\n",
      "918\n",
      "01:03:07,520 --> 01:03:14,240\n",
      "진짜 그 회사 담당자들 물어보면 잘 몰라요. 이렇게 신문에다 얘기하는 거지.\n",
      "\n",
      "919\n",
      "01:03:14,240 --> 01:03:21,280\n",
      "왜 그러냐면요. 이런 빅데이터나 AI나 이런 기술을 아는 사람은 당연히 외부에서 들어옵니다.\n",
      "\n",
      "920\n",
      "01:03:21,280 --> 01:03:26,600\n",
      "그러면 외부에서 들어오면 뭘 모르냐. 그 회사의 비즈니스를 몰라요.\n",
      "\n",
      "921\n",
      "01:03:26,600 --> 01:03:31,840\n",
      "그럼 비즈니스를 모르는 상황에서 AI나 빅데이터를 적용할 수 있나요?\n",
      "\n",
      "922\n",
      "01:03:31,840 --> 01:03:37,560\n",
      "당연히 안 되죠. 그러니까 AI라는 게 어떤 특정한 도메인에서 특정한 데이터를 가지고\n",
      "\n",
      "923\n",
      "01:03:37,560 --> 01:03:44,360\n",
      "학습을 해서 그걸 하려고 하는 건데 특정한 도메인과 데이터를 모르는 상황에서\n",
      "\n",
      "924\n",
      "01:03:44,360 --> 01:03:49,720\n",
      "AI가 어떻게 적용되느냐 하는 거죠. 그러면 결론적으로 뭘 하게 되냐면\n",
      "\n",
      "925\n",
      "01:03:49,720 --> 01:03:59,759\n",
      "신문에 나오는 AI 하는 거예요. 인간 AI 비서 AI 점원 로봇을 도입해서\n",
      "\n",
      "926\n",
      "01:04:00,000 --> 01:04:02,000\n",
      "이런 것만 하게 되는 거예요.\n",
      "\n",
      "927\n",
      "01:04:02,000 --> 01:04:04,440\n",
      "그러니까 전혀 비즈니스 도우면 안 돼요.\n",
      "\n",
      "928\n",
      "01:04:04,440 --> 01:04:10,440\n",
      "그러니까 제가 지금 말씀드리고 싶은 것은 중소기업이든 공공기관이든\n",
      "\n",
      "929\n",
      "01:04:10,440 --> 01:04:16,160\n",
      "대기업이든 외부에서 뭘 도입해서 하지 말자는 거예요.\n",
      "\n",
      "930\n",
      "01:04:16,160 --> 01:04:19,680\n",
      "왜냐하면 그렇게 하면 돈도 많이 들고 시간도 많이 걸리고 효과도\n",
      "\n",
      "931\n",
      "01:04:19,680 --> 01:04:20,680\n",
      "없어요.\n",
      "\n",
      "932\n",
      "01:04:20,680 --> 01:04:25,200\n",
      "그러니까 뭘 해야 되냐 우리 내부에 있는 직원들을 교육을 시키고 그\n",
      "\n",
      "933\n",
      "01:04:25,200 --> 01:04:29,000\n",
      "다음에 직원 교육시키면 열심히 안 할 거 아니에요.\n",
      "\n",
      "934\n",
      "01:04:29,000 --> 01:04:30,520\n",
      "인센티브를 줘야지.\n",
      "\n",
      "935\n",
      "01:04:30,520 --> 01:04:35,799\n",
      "인센티브를 줘서 만약에 AI를 적용 해가지고 새로운 비즈니스 프로듀스\n",
      "\n",
      "936\n",
      "01:04:35,799 --> 01:04:42,320\n",
      "과 새로운 서비스를 만들면 거기서 우리 회사가 어떤 도움을 받게 되면\n",
      "\n",
      "937\n",
      "01:04:42,320 --> 01:04:46,880\n",
      "우리가 거기서 나오는 무슨 뭔가 를 너한테 줄게.\n",
      "\n",
      "938\n",
      "01:04:46,880 --> 01:04:51,560\n",
      "인센티브로 만약에 해준다고 하면 기수하죠 직원들이.\n",
      "\n",
      "939\n",
      "01:04:51,560 --> 01:04:55,919\n",
      "그러니까 문제는 뭐냐면 업무를 아는 사람이 AI 공부를 적극적으로\n",
      "\n",
      "940\n",
      "01:04:55,919 --> 01:05:00,639\n",
      "해가지고 이거를 내 업무에 적용 하지 않으면 디지털 트레이너가\n",
      "\n",
      "941\n",
      "01:05:00,639 --> 01:05:01,139\n",
      "안 되죠.\n",
      "\n",
      "942\n",
      "01:05:00,000 --> 01:05:06,380\n",
      "트랜스포메이션이고 안 된다라고 하는데 기존에 있는 직원들이 움직이지\n",
      "\n",
      "943\n",
      "01:05:06,380 --> 01:05:08,840\n",
      "않으면 절대로 그건 불가능합니다.\n",
      "\n",
      "944\n",
      "01:05:08,840 --> 01:05:15,520\n",
      "우리나라가 혹시 저기 옛날에 그 식스시그마 하실 때 아시나요 그거\n",
      "\n",
      "945\n",
      "01:05:15,520 --> 01:05:17,520\n",
      "네 아시죠 식스시그마 했잖아요.\n",
      "\n",
      "946\n",
      "01:05:17,520 --> 01:05:18,520\n",
      "알죠.\n",
      "\n",
      "947\n",
      "01:05:18,520 --> 01:05:19,520\n",
      "LG 있을 때 왔습니다.\n",
      "\n",
      "948\n",
      "01:05:19,520 --> 01:05:20,520\n",
      "그렇죠.\n",
      "\n",
      "949\n",
      "01:05:20,520 --> 01:05:23,000\n",
      "그러면 식스시그마 블랙벨트 있었 잖아요 그때.\n",
      "\n",
      "950\n",
      "01:05:23,000 --> 01:05:29,240\n",
      "식스시그마는 우리나라에서 유일하게 성공한 그런 거였죠.\n",
      "\n",
      "951\n",
      "01:05:29,240 --> 01:05:30,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "952\n",
      "01:05:30,240 --> 01:05:31,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "953\n",
      "01:05:31,240 --> 01:05:32,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "954\n",
      "01:05:32,240 --> 01:05:33,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "955\n",
      "01:05:33,240 --> 01:05:34,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "956\n",
      "01:05:34,240 --> 01:05:35,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "957\n",
      "01:05:35,240 --> 01:05:36,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "958\n",
      "01:05:36,240 --> 01:05:37,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "959\n",
      "01:05:37,240 --> 01:05:38,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "960\n",
      "01:05:38,240 --> 01:05:39,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "961\n",
      "01:05:39,240 --> 01:05:40,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "962\n",
      "01:05:40,240 --> 01:05:41,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "963\n",
      "01:05:41,240 --> 01:05:42,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "964\n",
      "01:05:42,240 --> 01:05:43,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "965\n",
      "01:05:44,240 --> 01:05:45,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "966\n",
      "01:05:45,240 --> 01:05:46,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "967\n",
      "01:05:46,240 --> 01:05:47,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "968\n",
      "01:05:47,240 --> 01:05:48,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "969\n",
      "01:05:48,240 --> 01:05:49,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "970\n",
      "01:05:49,240 --> 01:05:50,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "971\n",
      "01:05:50,240 --> 01:05:51,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "972\n",
      "01:05:51,240 --> 01:05:52,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "973\n",
      "01:05:52,240 --> 01:05:53,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "974\n",
      "01:05:53,240 --> 01:05:54,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "975\n",
      "01:05:54,240 --> 01:05:55,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "976\n",
      "01:05:55,240 --> 01:05:56,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "977\n",
      "01:05:56,240 --> 01:05:57,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "978\n",
      "01:05:57,240 --> 01:05:58,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "979\n",
      "01:05:58,240 --> 01:05:59,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "980\n",
      "01:05:59,240 --> 01:06:00,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "981\n",
      "01:06:00,240 --> 01:06:01,240\n",
      "유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게 유일하게\n",
      "\n",
      "982\n",
      "01:06:00,000 --> 01:06:04,880\n",
      "그래서 그때 DMAIC인가요?\n",
      "\n",
      "983\n",
      "01:06:04,880 --> 01:06:06,380\n",
      "그때 식신흥화?\n",
      "\n",
      "984\n",
      "01:06:06,380 --> 01:06:11,960\n",
      "DMAIC이라는 것만 가르쳐가지고 AI 가르쳐서 내가 소조활동을 해가지고\n",
      "\n",
      "985\n",
      "01:06:11,960 --> 01:06:14,720\n",
      "뭔가 결론이 나오면 우리가 포상을 해줄게.\n",
      "\n",
      "986\n",
      "01:06:14,720 --> 01:06:15,720\n",
      "그랬잖아요.\n",
      "\n",
      "987\n",
      "01:06:15,720 --> 01:06:19,200\n",
      "그러니까 그런 식으로 하시면 됩니다.\n",
      "\n",
      "988\n",
      "01:06:19,200 --> 01:06:25,280\n",
      "외부에서 박사 AI 해서 우리 회사 걔네들 보고 하겠다.\n",
      "\n",
      "989\n",
      "01:06:25,280 --> 01:06:26,280\n",
      "절대 안 됩니다.\n",
      "\n",
      "990\n",
      "01:06:26,280 --> 01:06:34,279\n",
      "공공시장에 대한 거, 공공기관이나 행정부, 우리나라 정부에서 도입할\n",
      "\n",
      "991\n",
      "01:06:34,279 --> 01:06:36,360\n",
      "가능성이 있을까요?\n",
      "\n",
      "992\n",
      "01:06:36,360 --> 01:06:38,279\n",
      "그것도 똑같은 거예요.\n",
      "\n",
      "993\n",
      "01:06:38,279 --> 01:06:46,439\n",
      "식시그마 하나 똑같은 게 공공기관은 약간의 개개인의 공공기관에 보면\n",
      "\n",
      "994\n",
      "01:06:46,439 --> 01:06:51,520\n",
      "되게 열심히 하지만 실제로 열심히 한듯 보이는 거예요.\n",
      "\n",
      "995\n",
      "01:06:51,520 --> 01:06:57,119\n",
      "그러니까 진짜 AI를 도입을 하려고 하는 거는 각각 담당자 머릿속에\n",
      "\n",
      "996\n",
      "01:06:57,119 --> 01:07:00,000\n",
      "나오는 거지 외부 인사가 할 수 있는 건 절대 아닙니다.\n",
      "\n",
      "997\n",
      "01:07:00,000 --> 01:07:05,120\n",
      "그래서 공공기관도 마찬가지로 식스위크와 유사한 그런 활동을 해서\n",
      "\n",
      "998\n",
      "01:07:05,120 --> 01:07:11,000\n",
      "이거 뭔가 이렇게 효과 나오면 우리가 공급을 얼마 더 줄게\n",
      "\n",
      "999\n",
      "01:07:11,000 --> 01:07:14,500\n",
      "뭐 이런 식으로 해지지 않으면 공공기관 되게 힘들죠.\n",
      "\n",
      "1000\n",
      "01:07:14,500 --> 01:07:18,400\n",
      "기업들마다 요즘에 작년에는 POC 형태를 많이 했고\n",
      "\n",
      "1001\n",
      "01:07:18,400 --> 01:07:24,260\n",
      "말씀 주신 것처럼 올해 나름대로 한 특정 영역을 정해서 인지하는 것들이 많은데\n",
      "\n",
      "1002\n",
      "01:07:24,260 --> 01:07:28,799\n",
      "하다 보니까 결론 아직 결론은 아니지만 그 과정에서 나온 게\n",
      "\n",
      "1003\n",
      "01:07:28,799 --> 01:07:34,779\n",
      "아까 LNM 말씀하신 LNM을 보편적인 일반적인 그 퍼블릭에 있는 거든\n",
      "\n",
      "1004\n",
      "01:07:34,779 --> 01:07:39,840\n",
      "혹은 자기들이 오픈소스를 다운받아서 하든 만능적인 LNM은 없다.\n",
      "\n",
      "1005\n",
      "01:07:39,840 --> 01:07:46,040\n",
      "그 즉 기업 내 도메인 스페이스픽한 LNM들을 굉장히 많이 가지고서 해야 된다.\n",
      "\n",
      "1006\n",
      "01:07:46,040 --> 01:07:48,880\n",
      "3개, 4개. 그러다 보니까 막 3개, 4개가 생기는 거예요.\n",
      "\n",
      "1007\n",
      "01:07:48,880 --> 01:07:52,779\n",
      "물론 큰 기업들은 그걸 감당할 만한 여력들이 되어지는데\n",
      "\n",
      "1008\n",
      "01:07:52,779 --> 01:07:55,080\n",
      "적은 기업들은 그런 여력들도 안 되어지고\n",
      "\n",
      "1009\n",
      "01:07:55,080 --> 01:08:00,180\n",
      "그러다 보니 적은 기업들한테는 아까 말씀하신 외부의 그것들을 잘하는 데를 속일 수 있는\n",
      "\n",
      "1010\n",
      "01:08:00,000 --> 01:08:09,000\n",
      "시켜주고 그러면 또 비용적인 부담들이 발생을 하고 그래서 이런 도메인 스페이스픽한 것들을 지금 단계에서 여러 개 가져가는 게 현실적 대안인 것 같은데\n",
      "\n",
      "1011\n",
      "01:08:09,000 --> 01:08:11,400\n",
      "향후에는 거기에 대한 것들을 어떻게 보시는지.\n",
      "\n",
      "1012\n",
      "01:08:11,400 --> 01:08:17,600\n",
      "아 이게 여러분 gpt4 그러면 별걸 다 물어봐도 별걸 다 답변을 하잖아요.\n",
      "\n",
      "1013\n",
      "01:08:17,600 --> 01:08:23,900\n",
      "근데 오픈소스 LLM을 똑같은 생각을 하시게 되면 절대 그렇지 않아요.\n",
      "\n",
      "1014\n",
      "01:08:23,900 --> 01:08:29,200\n",
      "왜냐하면 이게 오픈소스하고 돈 내고 쓰는 거하고 다른 게 그거거든요.\n",
      "\n",
      "1015\n",
      "01:08:29,200 --> 01:08:39,099\n",
      "사실 gpt4를 뒤져보면 그 안에 MOE라고 하는 분야별 엑스퍼트가 쫙 있어요.\n",
      "\n",
      "1016\n",
      "01:08:39,099 --> 01:08:43,000\n",
      "수십 개가 있어요. 수십 개. 열 여섯 개인가 그렇다던데.\n",
      "\n",
      "1017\n",
      "01:08:43,000 --> 01:08:46,200\n",
      "그 하나하나가 다 LLM인 거예요.\n",
      "\n",
      "1018\n",
      "01:08:46,200 --> 01:08:52,099\n",
      "그러니까 우리 기업에서 파인트닝 해가지고 LLM 만드는 건 그 중에 하나밖에 안 되는 거예요.\n",
      "\n",
      "1019\n",
      "01:08:52,099 --> 01:09:00,000\n",
      "그러면서 보면 현재 수준으로 보면 기업 내에서 모든\n",
      "\n",
      "1020\n",
      "01:09:00,000 --> 01:09:02,920\n",
      "그 분야에 다 하는 그런 LLM을 만들기는 힘듭니다.\n",
      "\n",
      "1021\n",
      "01:09:02,920 --> 01:09:05,820\n",
      "그렇기 때문에 분야별로 만들 수밖에 없는 거고요.\n",
      "\n",
      "1022\n",
      "01:09:06,160 --> 01:09:08,720\n",
      "지금 말씀하시는데 이걸 해결할 수 있는 방안이 있냐?\n",
      "\n",
      "1023\n",
      "01:09:08,720 --> 01:09:11,640\n",
      "혹시나 그 이 Lama3가 나와가지고\n",
      "\n",
      "1024\n",
      "01:09:11,640 --> 01:09:14,220\n",
      "Lama3가 그걸 할 수 있으면 몰라도\n",
      "\n",
      "1025\n",
      "01:09:14,600 --> 01:09:19,360\n",
      "현재 상황에서는 없다라고 하는 것을 말씀드리고 싶은 거죠.\n",
      "\n",
      "1026\n",
      "01:09:19,360 --> 01:09:21,920\n",
      "AI를 좀 공구를 해보고 싶은데\n",
      "\n",
      "1027\n",
      "01:09:22,139 --> 01:09:25,139\n",
      "실제로 아까 말씀드린 ABCDE 모델을\n",
      "\n",
      "1028\n",
      "01:09:25,139 --> 01:09:27,940\n",
      "이제 좀 만질 수 있는 레벨이 되려면\n",
      "\n",
      "1029\n",
      "01:09:28,280 --> 01:09:31,160\n",
      "어떤 방향으로 공구를 해야 될지\n",
      "\n",
      "1030\n",
      "01:09:31,160 --> 01:09:36,259\n",
      "좀 좋은 책이나 코스 같은 게 있는지 말씀을 해주셨으면 좋겠습니다.\n",
      "\n",
      "1031\n",
      "01:09:36,259 --> 01:09:39,840\n",
      "이거 저기 저 유튜브 뒤집어는 나오고요.\n",
      "\n",
      "1032\n",
      "01:09:39,840 --> 01:09:41,080\n",
      "왕도라는 게 없고요.\n",
      "\n",
      "1033\n",
      "01:09:41,080 --> 01:09:42,759\n",
      "패스 캠퍼스인가?\n",
      "\n",
      "1034\n",
      "01:09:42,759 --> 01:09:46,520\n",
      "뭐 패스.. 거기 보면 또 유사한 과정이 있습니다.\n",
      "\n",
      "1035\n",
      "01:09:47,020 --> 01:09:51,759\n",
      "그리고 열심히 쫓아다니면 유튜브나 책이나 이런 거는 되게 많고\n",
      "\n",
      "1036\n",
      "01:09:52,200 --> 01:09:54,520\n",
      "이제 결국에는 사실 논문도 쳐다보고\n",
      "\n",
      "1037\n",
      "01:09:54,520 --> 01:09:57,840\n",
      "그러셔야 되는 그런 고난의 길입니다.\n",
      "\n",
      "1038\n",
      "01:09:57,840 --> 01:09:59,980\n",
      "쉬운 방법은 없어요.\n",
      "\n",
      "1039\n",
      "01:10:00,000 --> 01:10:06,240\n",
      "쉽게 가려고 할 거 아니에요. 얘기만 있고 공감하고 저도 그렇게 움직이고 있는데\n",
      "\n",
      "1040\n",
      "01:10:06,240 --> 01:10:13,440\n",
      "외부에서 들어와서 도메인의 지식이 없어서 이게 데이터가 안 들어오는 거죠. 알고리즘이 있어도\n",
      "\n",
      "1041\n",
      "01:10:13,440 --> 01:10:20,080\n",
      "또 데이터가 있는 사람은 알고리즘을 또 못합니다. 그래서 이게 안 풀리는 함수에요. 그래서 안 풀립니다.\n",
      "\n",
      "1042\n",
      "01:10:20,080 --> 01:10:28,799\n",
      "그래서 아까 TF가 구성이 필요하다. TF가 계속 돌아간다는 그 자체만으로도 기업 자체가 뜨겁기 때문에\n",
      "\n",
      "1043\n",
      "01:10:28,799 --> 01:10:39,200\n",
      "그때 갈급할 때 외부의 도움이 적게 들어가면 나오는 거고. 아까 식스시그마를 생성 AI 활용에 되게 좋은 사례로 드신 것은\n",
      "\n",
      "1044\n",
      "01:10:39,200 --> 01:10:49,000\n",
      "저도 아주 되게 좋은 비유다. 변역기라는 거 보이시죠? 변역기가 제가 보면은 한 10년에서 15년 사이 한 번씩 오는 것 같아요.\n",
      "\n",
      "1045\n",
      "01:10:49,000 --> 01:10:56,400\n",
      "기업이 왕창 잘되는 거하고, 기업이 왕창 못되는 거하고는 이 변역기의 이 트렌드를\n",
      "\n",
      "1046\n",
      "01:10:56,400 --> 01:11:01,880\n",
      "내가 리딩하면 엄청 잘되는 거고요.\n",
      "\n",
      "1047\n",
      "01:11:00,000 --> 01:11:05,500\n",
      "하는 걸 쫓아다니면 안 되는 겁니다. 그러니까 과거에서 지금까지 쳐다\n",
      "\n",
      "1048\n",
      "01:11:05,500 --> 01:11:10,580\n",
      "보면 이 변역기에 어떻게 내가 반응 하느냐에 따라서 우리 회사의 운명\n",
      "\n",
      "1049\n",
      "01:11:10,580 --> 01:11:15,580\n",
      "이 왔다갔다 한다고 하는 거죠. 그래서 지금 이 변역기에 잘 적응\n",
      "\n",
      "1050\n",
      "01:11:15,580 --> 01:11:21,780\n",
      "하시고 잘 이걸 리딩하는 그런 포지션 에 계셨으면 좋겠습니다.\n",
      "\n",
      "1051\n",
      "01:11:21,780 --> 01:11:27,139\n",
      "네 큰 박수로 오늘 특별한 특강 감사를 전하면서 마무리하겠습니다.\n",
      "\n",
      "1052\n",
      "01:11:27,139 --> 01:11:28,639\n",
      "감사합니다.\n"
     ]
    }
   ],
   "source": [
    "merged_transcript = merge_transcripts(*transcripts)\n",
    "print(merged_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample.srt\", \"w\") as f:\n",
    "    f.write(merged_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "MAX_COMPLETION_TOKENS = 4096\n",
    "MAX_CONTEXT_LENGTH = 8192  # GPT-4의 일반적인 컨텍스트 길이\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def chunk_text(text: str, max_chunk_tokens: int = 3000) -> list[str]:\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in text.split(\". \"):\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "        if current_tokens + sentence_tokens > max_chunk_tokens:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "            current_tokens = sentence_tokens\n",
    "        else:\n",
    "            current_chunk += sentence + \". \"\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_chunk(chunk: str, system_prompt: str) -> str:\n",
    "    prompt_tokens = count_tokens(system_prompt) + count_tokens(chunk)\n",
    "    max_response_tokens = min(\n",
    "        MAX_COMPLETION_TOKENS, MAX_CONTEXT_LENGTH - prompt_tokens - 100\n",
    "    )\n",
    "\n",
    "    if max_response_tokens <= 0:\n",
    "        return \"Error: 입력 텍스트가 너무 깁니다.\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0.1,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": chunk},\n",
    "            ],\n",
    "            max_tokens=max_response_tokens,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: API 호출 중 오류 발생 - {str(e)}\"\n",
    "\n",
    "\n",
    "def format_summary(summary: str) -> str:\n",
    "    lines = summary.split(\"\\n\")\n",
    "    formatted_lines = []\n",
    "    for line in lines:\n",
    "        if line.strip():\n",
    "            match = re.match(r\"^(\\d{2}:\\d{2})\\s+(.+)$\", line)\n",
    "            if match:\n",
    "                time, content = match.groups()\n",
    "                if not any(\n",
    "                    emoji in content\n",
    "                    for emoji in [\n",
    "                        \"🔶\",\n",
    "                        \"👋\",\n",
    "                        \"👀\",\n",
    "                        \"💡\",\n",
    "                        \"🎯\",\n",
    "                        \"📊\",\n",
    "                        \"🔑\",\n",
    "                        \"💻\",\n",
    "                        \"🤔\",\n",
    "                        \"📝\",\n",
    "                    ]\n",
    "                ):\n",
    "                    content = \"🔶 \" + content\n",
    "                formatted_lines.append(f\"{time} {content}\")\n",
    "            else:\n",
    "                formatted_lines.append(line)\n",
    "    return \"\\n\".join(formatted_lines)\n",
    "\n",
    "\n",
    "def post_processing(instruction: str) -> str:\n",
    "    system_prompt = \"\"\"\n",
    "    당신은 비디오 자막을 분석하고 요약하는 AI 어시스턴트입니다. 주어진 자막 정보를 바탕으로 다음 작업을 수행해야 합니다:\n",
    "    1. 중요한 주제를 시간 순서에 맞게 선정하세요.\n",
    "    2. 각 주제에 대해 한 줄 요약을 작성하세요.\n",
    "    3. 각 주제가 시작되는 시간을 mm:ss(분:초) 형식으로 기록하세요.\n",
    "    4. 각 요약 문장에 적합한 이모지를 추가하세요.\n",
    "    5. 요약은 한글로 작성하세요.\n",
    "    6. 주제는 최대한 많이 추출해 주세요.\n",
    "    7. 시간 표기에 오류가 없도록 주의하세요.\n",
    "\n",
    "    출력 형식:\n",
    "    mm:ss 🔶 주제에 대한 한 줄 요약\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = chunk_text(instruction)\n",
    "    summaries = []\n",
    "\n",
    "    print(f\"총 {len(chunks)}개의 청크로 나누어 처리합니다.\\n\")\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"\\n청크 {i+1}/{len(chunks)} 처리 중...\\n\")\n",
    "        summary = process_chunk(chunk, system_prompt)\n",
    "        summaries.append(summary)\n",
    "        print(summary)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    final_summary = \"\\n\".join(summaries)\n",
    "    formatted_summary = format_summary(final_summary)\n",
    "\n",
    "    return formatted_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 16개의 청크로 나누어 처리합니다.\n",
      "\n",
      "\n",
      "청크 1/16 처리 중...\n",
      "\n",
      "00:00 🔶 생성 AI의 중요성: 생성 AI는 단순한 트렌드가 아닌 중요한 기술입니다. 🤖\n",
      "\n",
      "00:08 🔶 기업의 생성 AI 준비: 기업이 생성 AI를 이해하고 준비하는 방법에 대해 논의합니다. 🏢\n",
      "\n",
      "00:15 🔶 경영자와 생성 AI: 경영자들이 생성 AI를 어떻게 활용해야 하는지에 대한 이론과 현안을 다룹니다. 📚\n",
      "\n",
      "00:25 🔶 장동인 교수 소개: 장동인 교수님이 초청되어 특강을 진행합니다. 🎓\n",
      "\n",
      "00:32 🔶 첫 GPT 기업 생존법: 첫 GPT를 활용해 기업이 생존하는 방법을 소개합니다. 💼\n",
      "\n",
      "00:50 🔶 경영자와 개발자의 중간 역할: 경영자와 개발자 사이의 중간 역할이 중요하다고 강조합니다. 🔄\n",
      "\n",
      "01:11 🔶 경영자와 개발자의 협력 필요성: 경영자와 개발자가 서로의 역할을 이해해야 한다고 설명합니다. 🤝\n",
      "\n",
      "01:34 🔶 AI 이해의 어려움: AI를 이해하는 것이 어렵다는 점을 지적합니다. 🧠\n",
      "\n",
      "02:00 🔶 AI 시대의 역할: AI 시대에 경영자와 개발자의 역할을 메꿔주는 것이 중요하다고 말합니다. 🌐\n",
      "\n",
      "02:06 🔶 책 소개: '기업이 살아남는 법'이라는 책을 통해 경영자와 개발자의 결합을 시도합니다. 📖\n",
      "\n",
      "02:18 🔶 GPT의 발전: GPT 3.5, GPT 4, 그리고 곧 나올 GPT 5에 대해 언급합니다. 🚀\n",
      "\n",
      "02:30 🔶 GPT의 활용과 변화: GPT가 기업에 미치는 영향과 활용 방법에 대해 논의합니다. 🔄\n",
      "\n",
      "02:39 🔶 GPT 도입의 고민: 많은 기업들이 GPT 도입을 고민하고 있는 상황을 설명합니다. 🤔\n",
      "\n",
      "02:50 🔶 GPT의 한계: GPT의 한계를 명확히 이해해야 한다고 강조합니다. 🚧\n",
      "\n",
      "03:00 🔶 콜센터에서의 GPT 활용: 콜센터에서 GPT를 활용하는 방법과 그 한계를 설명합니다. 📞\n",
      "\n",
      "03:26 🔶 법규 준수의 어려움: GPT가 법규를 준수하는 데 어려움이 있다는 점을 지적합니다. ⚖️\n",
      "\n",
      "03:46 🔶 콜센터의 책임: 콜센터에서 GPT가 고객에게 하는 말을 기업이 책임져야 한다는 점을 강조합니다. 📋\n",
      "\n",
      "04:00 🔶 Hallucination 문제: GPT의 Hallucination 문제와 이를 해결하기 위한 ROG 방안을 설명합니다. 🌀\n",
      "\n",
      "04:21 🔶 ROG의 한계: ROG가 완벽한 해결책이 아니라는 점을 지적합니다. ❌\n",
      "\n",
      "04:34 🔶 GPT-5의 기대: GPT-5가 Hallucination 문제를 거의 해결할 것이라는 기대를 언급합니다. 🌟\n",
      "\n",
      "04:50 🔶 온 디바이스 구현의 어려움: GPT의 사이즈가 커서 온 디바이스 구현이 어렵다는 점을 설명합니다. 📱\n",
      "\n",
      "05:00 🔶 자동차와 GPT: 메르세데스 벤츠가 구글 클라우드와 협력해 GPT를 자동차에 연결하는 계약을 체결한 사례를 소개합니다. 🚗\n",
      "\n",
      "\n",
      "\n",
      "청크 2/16 처리 중...\n",
      "\n",
      "00:05:15 🔶 삼성전자가 갤럭시24를 통해 네트워크가 끊어져도 번역과 통역이 가능한 기술을 선보임 📱\n",
      "\n",
      "00:05:33 🔶 온 디바이스 AI 구현의 중요성 강조, 스마트폰에 라지 랭귀지 모델(LLM) 적용 가능 📲\n",
      "\n",
      "00:05:57 🔶 LLM을 다양한 디바이스에 적용 가능, 스마트 가전 시대 도래 예고 🤖\n",
      "\n",
      "00:06:34 🔶 모델 경량화가 핵심, LLM의 경량화가 비즈니스 기회로 작용할 것 💡\n",
      "\n",
      "00:06:50 🔶 데이터 보안 이슈, GPT-4 사용 시 데이터 학습 문제 제기 🔒\n",
      "\n",
      "00:07:37 🔶 오픈 AI의 데이터 프라이버시 정책, API 사용 시 데이터 학습 여부 논란 🤔\n",
      "\n",
      "00:08:10 🔶 클라우드 컴퓨팅 초기와 유사한 데이터 신뢰 문제, GPT-4와 오픈소스 LLM 비교 🌐\n",
      "\n",
      "00:09:53 🔶 GPT-4를 뛰어넘는 오픈소스 LLM은 아직 없으며, GPT-4의 다양한 기능 강조 📊\n",
      "\n",
      "00:10:49 🔶 멀티모델 기능의 중요성, GPT-4의 멀티모델 기능이 아직 오픈소스 LLM에 비해 우위에 있음 🖼️\n",
      "\n",
      "00:11:04 🔶 기업에서의 LLM 활용 방법 세 가지, 오픈소스 LLM의 이해와 활용 중요성 📈\n",
      "\n",
      "\n",
      "\n",
      "청크 3/16 처리 중...\n",
      "\n",
      "1. \n",
      "00:11:43 🔶 LLM의 도입 필요성: LLM은 기존 AI와 달라서 신중한 도입이 필요합니다. 🤖\n",
      "\n",
      "2. \n",
      "00:12:00 🔶 기업에서 LLM을 활용하는 세 가지 관점: 프롬프트 엔지니어링, 기업 자체의 활용, 비즈니스 활용입니다. 📊\n",
      "\n",
      "3. \n",
      "00:12:16 🔶 프롬프트 엔지니어링의 중요성: 엑셀처럼 프롬프트 엔지니어링도 기업에서 필수적으로 사용될 것입니다. 📚\n",
      "\n",
      "4. \n",
      "00:12:55 🔶 프롬프트 엔지니어링 TF팀 운영: 기업 내에서 프롬프트 엔지니어링 TF팀을 운영하는 것이 중요합니다. 🛠️\n",
      "\n",
      "5. \n",
      "00:13:19 🔶 기업 내 LLM 활용: LLM을 통해 글로벌 회의록 작성과 요약 자동화가 가능합니다. 🌐\n",
      "\n",
      "6. \n",
      "00:14:00 🔶 업무 프로세스와 LLM의 통합: LLM이 기업의 업무 프로세스에 완벽히 녹아들 것입니다. 🏢\n",
      "\n",
      "7. \n",
      "00:15:22 🔶 프롬프트 엔지니어링의 어려움: 프롬프트 엔지니어링은 쉬운 것 같지만 실제로는 어렵습니다. 🧩\n",
      "\n",
      "8. \n",
      "00:16:23 🔶 TF팀의 필요성: TF팀이 없으면 LLM 도입 후에도 제대로 활용되지 않을 수 있습니다. 🚀\n",
      "\n",
      "9. \n",
      "00:16:46 🔶 LLM 활용 방법: 기업에서 LLM을 활용하는 다섯 가지 방법에 대해 설명합니다. 📈\n",
      "\n",
      "10. \n",
      "00:17:00 🔶 인공지능의 학습과 인퍼런스 단계: 인공지능은 학습 단계와 인퍼런스 단계로 나뉩니다. 🧠\n",
      "\n",
      "\n",
      "\n",
      "청크 4/16 처리 중...\n",
      "\n",
      "00:17:17 🔶 파인트닝의 필요 요소 설명: 파인트닝을 위해 파운데이션 모델, AI 모델, 기업 데이터, GPU가 필요함을 설명합니다. 🤖\n",
      "\n",
      "00:18:02 🔶 기업 데이터의 포맷 변환: 기업 데이터는 파인트닝에 맞는 포맷으로 변환되어야 함을 강조합니다. 📊\n",
      "\n",
      "00:19:00 🔶 파인트닝의 정의: 파인트닝은 기업의 제품에 맞게 AI 모델을 튜닝하는 과정임을 설명합니다. 🛠️\n",
      "\n",
      "00:19:13 🔶 GPT-4의 비즈니스 모델: GPT-4의 비즈니스 모델을 중국집 셰프의 비유로 설명합니다. 🍜\n",
      "\n",
      "00:20:08 🔶 원샷, 퓨샷, 제로샷 러닝: 원샷, 퓨샷, 제로샷 러닝의 개념을 소개합니다. 🎯\n",
      "\n",
      "00:21:00 🔶 다양한 파인트닝 기법: GPU 사용을 최소화하고 데이터를 효율적으로 활용하는 다양한 파인트닝 기법을 설명합니다. 🧩\n",
      "\n",
      "00:21:36 🔶 인 컨텍스트 러닝: 기업 데이터를 활용한 인 컨텍스트 러닝 기법을 소개합니다. 🧠\n",
      "\n",
      "00:21:48 🔶 일루서 AI 소개: 일루서 AI와 그들이 만든 GPT-J에 대해 설명합니다. 🌐\n",
      "\n",
      "\n",
      "\n",
      "청크 5/16 처리 중...\n",
      "\n",
      "00:22:22 🔶 폴리글라스: 한국어 파운데이션 모델 소개 🌐\n",
      "00:22:48 🔶 라마 시리즈: 라마2와 곧 출시될 라마3 언급 🦙\n",
      "00:23:09 🔶 오픈 데이터: 1TB의 오픈 소스 데이터로 트레이닝 🗂️\n",
      "00:23:18 🔶 다양한 모델: 돌리트, 스테이블 LM 등 다양한 모델 등장 🤖\n",
      "00:23:25 🔶 Lama 2: 페이스북의 오픈 소스 모델 라마2 소개 📘\n",
      "00:23:42 🔶 라마1 문제: 초기 라마1의 문제와 공개 과정 🕵️‍♂️\n",
      "00:24:03 🔶 라마3 준비: 메타의 라마3 개발과 GPU 대량 구매 💻\n",
      "00:24:41 🔶 메타의 목표: 라마3를 GPT-4 수준으로 만들려는 목표 🎯\n",
      "00:25:15 🔶 오픈소스 모델: 알파카, 빕쿠나 등 수천 개의 오픈소스 모델 🌟\n",
      "\n",
      "\n",
      "\n",
      "청크 6/16 처리 중...\n",
      "\n",
      "00:25:26 🔶 전 세계적으로 오픈소스 LLM 리더보드가 존재하며 매주 업데이트됨 🌍\n",
      "\n",
      "00:25:48 🔶 유엔, 무리나, 이스라엘 등 다양한 국가들이 LLM을 개발 중임 🇺🇳🇮🇱\n",
      "\n",
      "00:26:00 🔶 LLM을 사용하지 않으면 뒤처지는 시대가 도래함 ⏳\n",
      "\n",
      "00:26:06 🔶 랭체인은 기업 내부에서 라이저 랭게이지 모델을 연결하는 라이브러리임 🏢\n",
      "\n",
      "00:26:24 🔶 랭체인은 작년에 큰 성공을 거두었으며, 현재 회사 가치가 2억 달러에 달함 💰\n",
      "\n",
      "00:26:42 🔶 랭체인은 기업 개발자들이 반드시 배워야 하는 필수 기술임 📚\n",
      "\n",
      "00:27:00 🔶 랭체인은 LLM, 인베이딩, 벡터스토어 등을 연결하는 프로그래밍 라이브러리임 🔗\n",
      "\n",
      "00:27:32 🔶 랭체인을 통해 회사 내부 사용자들이 프롬트를 입력하면 벡터스토어에 캐시 처리됨 🗂️\n",
      "\n",
      "00:28:00 🔶 벡터스토어는 회사 내부 문서를 벡터화하여 저장함 📄\n",
      "\n",
      "00:28:32 🔶 Similar Search를 통해 유사한 정보를 빠르게 찾아냄 🔍\n",
      "\n",
      "00:28:48 🔶 랭체인을 통해 회사의 정책 등을 쉽게 검색하고 응답할 수 있음 🏢\n",
      "\n",
      "00:29:00 🔶 GPT-4나 맞춤형 LLM을 통해 사용자에게 자연스러운 답변을 제공함 🤖\n",
      "\n",
      "00:29:22 🔶 인터넷 검색과 내부 시스템을 연동하여 다양한 정보를 제공함 🌐\n",
      "\n",
      "\n",
      "\n",
      "청크 7/16 처리 중...\n",
      "\n",
      "00:29:44 🔶 GPT-4의 에이전트 기능 소개: 에이전트를 활용하면 많은 일을 할 수 있음 🤖\n",
      "\n",
      "00:30:02 🔶 기업 맞춤형 어플리케이션 개발: 랭체인을 통해 기업에 맞는 어플리케이션을 개발할 수 있음 🏢\n",
      "\n",
      "00:30:13 🔶 LLM 도입 방법: 기업에서 LLM을 도입할 때 선택할 수 있는 5가지 방법 소개 📚\n",
      "\n",
      "00:30:20 🔶 OpenAI API 사용: OpenAI의 API를 사용하여 GPT-4와 연결하는 방법 설명 🌐\n",
      "\n",
      "00:30:48 🔶 랭체인과 벡터 DB: 랭체인 프로그램을 통해 벡터 DB에서 데이터를 쿼리하고 GPT-4로 답변 받기 💾\n",
      "\n",
      "00:31:19 🔶 벡터 데이터 해석: GPT-4가 벡터 데이터를 해석하여 사람이 이해할 수 있는 답변 제공 🧠\n",
      "\n",
      "00:31:50 🔶 인베이딩 방법: 오픈소스와 OpenAI GPT-4의 인베이딩 방법 차이 설명 🔍\n",
      "\n",
      "00:32:30 🔶 방화벽 문제: 한국 기업의 방화벽 문제와 GPT-4 사용의 어려움 🔒\n",
      "\n",
      "\n",
      "\n",
      "청크 8/16 처리 중...\n",
      "\n",
      "00:32:44 🔶 벡터스토어는 외국 제품입니다. 🌍\n",
      "00:33:08 🔶 인베딩 종류와 비용에 대한 설명입니다. 💸\n",
      "00:34:00 🔶 다양한 벡터DB 종류와 클라우드 제공 문제점입니다. ☁️\n",
      "00:34:33 🔶 국내에서 벡터DB를 만드는 시도입니다. 🇰🇷\n",
      "00:35:00 🔶 기업 데이터 인베딩과 비용 문제입니다. 🏢\n",
      "00:35:18 🔶 GPT-4 사용 시 비용 문제와 대안으로 GPT-3.5 활용 방안입니다. 🤖\n",
      "00:35:44 🔶 시스템 사용 시 오픈AI에 지불해야 하는 비용 문제입니다. 💰\n",
      "00:36:10 🔶 벡터DB와 GPT-4 사용 시 비용 발생에 대한 설명입니다. 💵\n",
      "00:36:43 🔶 Open Source LLM을 활용한 대안 제시입니다. 🆓\n",
      "\n",
      "\n",
      "\n",
      "청크 9/16 처리 중...\n",
      "\n",
      "00:36:51 🔶 솔라의 인기와 활용 방안에 대해 설명합니다. 🌟\n",
      "\n",
      "00:37:00 🔶 기업 데이터와 오픈소스 LLM의 인베딩 비용 문제를 논의합니다. 💼\n",
      "\n",
      "00:37:19 🔶 솔라를 사용하면 비용 절감이 가능하다는 점을 강조합니다. 💰\n",
      "\n",
      "00:37:34 🔶 오픈소스 LLM과 GPT-4의 기능 경쟁에 대해 이야기합니다. 🏆\n",
      "\n",
      "00:37:54 🔶 원프레미스 벡터 DB의 필요성과 외국 제품의 한계를 언급합니다. 🌐\n",
      "\n",
      "00:38:06 🔶 원프레미스 벡터 DB를 많이 사용하라고 권장합니다. 📊\n",
      "\n",
      "00:38:24 🔶 파운데이션 모델에 기업 데이터를 넣어 파인트닝하는 방법을 설명합니다. 🏭\n",
      "\n",
      "00:38:44 🔶 기업 맞춤형 GPT를 만드는 C타입 방법을 소개합니다. 🛠️\n",
      "\n",
      "00:39:00 🔶 벤처 기업들이 파인트닝을 통해 맞춤형 모델을 제공하는 사례를 설명합니다. 🚀\n",
      "\n",
      "00:39:19 🔶 맞춤형 모델을 통해 비즈니스 기회를 창출하는 방법을 논의합니다. 💡\n",
      "\n",
      "00:39:54 🔶 오픈AI가 기업 데이터를 활용해 GPT 3.5 Turbo 버전을 제공하는 방안을 발표합니다. 📈\n",
      "\n",
      "00:40:16 🔶 GPT 3.5 Turbo 버전의 비용 구조와 트레이닝 비용을 설명합니다. 💸\n",
      "\n",
      "\n",
      "\n",
      "청크 10/16 처리 중...\n",
      "\n",
      "1. 00:40:51 🔶 GPU가 필요 없는 작업 설명\n",
      "   - GPU 없이도 특정 작업을 수행할 수 있는 방법을 설명합니다. 💻\n",
      "\n",
      "2. 00:41:15 🔶 GPT-4와 GPT-4V의 현재 상태\n",
      "   - GPT-4와 GPT-4V가 아직 완전히 구현되지 않았음을 언급합니다. 🤖\n",
      "\n",
      "3. 00:41:38 🔶 GPT-4의 멀티모델 기능\n",
      "   - GPT-4의 멀티모델 기능과 이를 활용한 데이터 해석 가능성을 설명합니다. 📊\n",
      "\n",
      "4. 00:42:00 🔶 회사 데이터 학습의 중요성\n",
      "   - 회사의 PPT나 PDF 데이터를 학습시켜 맞춤형 솔루션을 제공하는 가능성을 논의합니다. 📁\n",
      "\n",
      "5. 00:42:34 🔶 건설사 예시를 통한 GPT-4 활용\n",
      "   - 건설사 데이터를 학습시켜 비용, 스케줄, 리스크 분석을 수행하는 예시를 듭니다. 🏗️\n",
      "\n",
      "6. 00:42:51 🔶 변호사 사무실에서의 GPT-4 활용\n",
      "   - 변호사 사무실에서 PDF 파일과 사진을 학습시켜 전문가 역할을 할 수 있는 가능성을 설명합니다. ⚖️\n",
      "\n",
      "7. 00:43:15 🔶 GPT-5에 대한 기대\n",
      "   - GPT-5가 나오면 LLM이 새로운 수준에 도달할 것이라는 기대를 표현합니다. 🌟\n",
      "\n",
      "8. 00:43:44 🔶 GPT-5의 회사 전문가 역할\n",
      "   - GPT-5가 회사의 모든 문서를 이해하고 전문가 역할을 할 수 있는 가능성을 설명합니다. 🏢\n",
      "\n",
      "9. 00:44:11 🔶 엔터프라이즈 채집 PT\n",
      "   - 엔터프라이즈 채집 PT가 데이터 보안과 관리에서 중요한 역할을 한다고 설명합니다. 🔒\n",
      "\n",
      "10. 00:44:34 🔶 오픈 AI의 흑자와 매출\n",
      "    - 오픈 AI가 흑자를 기록하고 매출이 2밀리언 달러를 넘었다는 사실을 언급합니다. 💰\n",
      "\n",
      "\n",
      "\n",
      "청크 11/16 처리 중...\n",
      "\n",
      "00:44:46 🔶 토큰 수익이 적지만 엔터프라이즈 채집 PT가 큰 수익을 창출함 💰\n",
      "\n",
      "00:45:08 🔶 한국에서 엔터프라이즈 채집 PT에 관심 있는 회사들이 많음 🇰🇷\n",
      "\n",
      "00:45:19 🔶 샘 알트만의 한국 방문과 한국 지사 설립 가능성 🏢\n",
      "\n",
      "00:45:28 🔶 오픈 AI의 다양한 활용 방식 제안 🤖\n",
      "\n",
      "00:45:55 🔶 대기업과 관공서의 보안 유지 방법 제안 🔒\n",
      "\n",
      "00:46:17 🔶 온프레미스 벡터 DB를 활용한 문서 보관 방법 📂\n",
      "\n",
      "00:46:27 🔶 소기업과 대기업의 AI 활용 방식 차이 설명 🏢\n",
      "\n",
      "00:46:43 🔶 GPT5와 D타입 활용 가능성 검토 📊\n",
      "\n",
      "00:47:00 🔶 그림과 텍스트가 있는 PDF 학습의 중요성 🖼️\n",
      "\n",
      "00:47:14 🔶 비즈니스 활용 방법과 오픈소스 API의 중요성 📈\n",
      "\n",
      "00:47:43 🔶 전자상거래의 변화와 AI의 역할 🛒\n",
      "\n",
      "00:49:00 🔶 네이버의 Q 서비스와 전자상거래의 미래 🔍\n",
      "\n",
      "00:49:49 🔶 AI를 활용한 구매 대행 사이트의 가능성 🌐\n",
      "\n",
      "\n",
      "\n",
      "청크 12/16 처리 중...\n",
      "\n",
      "00:50:04 🔶 LLM을 활용한 데이터 기반 보고서 및 글 작성 📝\n",
      "00:50:20 🔶 LLM이 웹사이트를 검색해 원하는 정보를 제공하는 기능 🌐\n",
      "00:50:39 🔶 여행 계획 시 다양한 사이트를 비교하는 과정 설명 🧳\n",
      "00:50:55 🔶 LLM을 이용한 여행 대행 서비스의 가능성 🌍\n",
      "00:51:15 🔶 플랫폼 위의 플랫폼 개념 설명 🏗️\n",
      "00:52:00 🔶 오픈AI의 비즈니스 전략과 철학 💡\n",
      "00:53:00 🔶 다양한 분야에서 LLM을 활용한 비즈니스 아이디어 💼\n",
      "00:54:00 🔶 로봇과 LLM의 결합 가능성 🤖\n",
      "00:54:36 🔶 로봇의 움직임을 위한 코딩의 중요성 및 LLM의 역할 🛠️\n",
      "\n",
      "\n",
      "\n",
      "청크 13/16 처리 중...\n",
      "\n",
      "00:55 🔶 로봇이 인간의 말을 이해하고 행동하는 기술의 발전 🌟\n",
      "00:56 🔶 드론과 로봇이 인간의 명령을 이해하고 수행하는 시대 도래 🚁\n",
      "00:57 🔶 파운데이션 모델이 AI 모델을 제어하고 실행하는 기능 🧠\n",
      "00:58 🔶 AI가 공장과 기업의 데이터를 해석하고 상황 판단을 수행 🏭\n",
      "00:59 🔶 AI가 시뮬레이션을 통해 문제 해결책을 제시하고 실행 🛠️\n",
      "01:00 🔶 AGI(인공지능 일반)의 도래와 기업의 대응 전략 🤖\n",
      "01:01 🔶 AI 부서와 협업을 통한 기업 내 AI 활용 방안 🏢\n",
      "01:02 🔶 디지털 트랜스포메이션의 필요성과 실행 방안 💡\n",
      "\n",
      "\n",
      "\n",
      "청크 14/16 처리 중...\n",
      "\n",
      "01:03:07 🔶 신문에 나오는 디지털 기술에 대해 실제 담당자들은 잘 모른다. 📰\n",
      "01:03:14 🔶 빅데이터와 AI 기술은 외부 전문가가 도입하지만, 그들은 회사 비즈니스를 모른다. 🤖\n",
      "01:03:21 🔶 비즈니스를 모르는 상태에서 AI와 빅데이터를 적용하는 것은 불가능하다. 🚫\n",
      "01:03:31 🔶 AI는 특정 도메인과 데이터를 알아야 효과적으로 학습하고 적용될 수 있다. 📊\n",
      "01:03:44 🔶 결국 신문에 나오는 AI는 실제로는 제대로 적용되지 않는다. 📰🚫\n",
      "\n",
      "\n",
      "\n",
      "청크 15/16 처리 중...\n",
      "\n",
      "01:04:00 🔶 외부 AI 도입의 비효율성 강조: 외부에서 AI를 도입하는 것은 비용과 시간이 많이 들고 효과가 적다고 설명합니다. 💸\n",
      "\n",
      "01:04:20 🔶 내부 직원 교육과 인센티브의 중요성: 내부 직원들을 교육시키고 인센티브를 제공하여 AI를 적용하는 것이 중요하다고 주장합니다. 🎓\n",
      "\n",
      "01:04:55 🔶 직원들의 AI 학습 필요성: 업무를 아는 직원들이 AI를 공부하고 적용해야 디지털 트랜스포메이션이 가능하다고 강조합니다. 📚\n",
      "\n",
      "01:05:08 🔶 식스시그마의 성공 사례: 식스시그마가 한국에서 성공한 유일한 사례로 언급되며, 이를 AI 도입에 비유합니다. 🏆\n",
      "\n",
      "01:06:04 🔶 DMAIC 방법론의 활용: DMAIC 방법론을 통해 AI를 가르치고 소조활동을 통해 결과를 도출하는 방식을 제안합니다. 🛠️\n",
      "\n",
      "01:06:26 🔶 공공기관의 AI 도입 어려움: 공공기관에서도 외부 인사가 아닌 내부 담당자가 AI 도입을 주도해야 한다고 설명합니다. 🏛️\n",
      "\n",
      "01:07:00 🔶 공공기관의 인센티브 필요성: 공공기관에서도 식스시그마와 유사한 활동을 통해 인센티브를 제공해야 효과적이라고 주장합니다. 🎁\n",
      "\n",
      "01:07:14 🔶 기업의 POC 형태와 LNM의 한계: 기업들이 POC 형태로 AI를 도입하고 있으며, 만능적인 LNM은 없고 도메인 특화된 LNM이 필요하다고 설명합니다. 🏢\n",
      "\n",
      "\n",
      "\n",
      "청크 16/16 처리 중...\n",
      "\n",
      "01:07:48 💼 큰 기업과 작은 기업의 여력 차이\n",
      "01:08:00 💸 외부 업체를 통한 비용 부담 문제\n",
      "01:08:11 🤔 도메인 스페이스픽한 해결책의 필요성\n",
      "01:08:17 🧠 GPT-4와 오픈소스 LLM의 차이\n",
      "01:08:29 📚 GPT-4의 MOE(분야별 전문가) 구조\n",
      "01:08:46 🏢 기업 내 LLM 구축의 어려움\n",
      "01:09:00 🔄 분야별 LLM의 필요성\n",
      "01:09:08 🔍 Lama3의 가능성\n",
      "01:09:19 📘 AI 학습을 위한 자료와 코스 추천\n",
      "01:09:36 📺 유튜브와 책을 통한 학습 방법\n",
      "01:10:00 🔄 도메인 지식과 데이터의 중요성\n",
      "01:10:20 🛠 TF 구성의 필요성\n",
      "01:10:39 🏆 식스시그마와 생성 AI의 활용 사례\n",
      "01:10:49 🔄 변역기의 중요성과 기업의 대응\n",
      "01:11:21 👏 특강 마무리와 감사 인사\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instruction = f\"\"\"주어진 비디오 \"자막정보\" 를 바탕으로 [요청사항]을 차례대로 수행해주세요.\n",
    "[자막정보]\n",
    "{merged_transcript}\n",
    "\n",
    "[요청사항]\n",
    "1. 주어진 [자막정보]에서 중요한 주제를 시간 순서에 맞게 선정하고, 주제가 시작되는 시간을 기록해주세요.\n",
    "2. 주제는 한 줄 요약을 작성하고, 자막에서 주제가 시작되는 시간을 mm:ss(분:초) 형식으로 작성하세요. (예: 00:05 👋 자막 생성 기능에 대한 소개)\n",
    "3. 요약은 한글로 작성해주세요.\n",
    "4. 각 문장에 적합한 emoji를 최대한 활용해 주세요\n",
    "\n",
    "[출력예시]\n",
    "00:12 👋 GPTs 의 주요 개념에 대하여 소개해요\n",
    "03:13 👀 GPTs 의 장단점과 활용 사례에 대하여 자세히 알아봐요\n",
    "\n",
    "[주의사항]\n",
    "- 주제는 최대한 많이 추출해 주세요.\n",
    "- 시간표기에 오류가 없도록 주의해 주세요. 분:초 형식으로 작성합니다. (예: 00:12)\n",
    "\n",
    "run step-by-step. Take a deep breath. You can do it!\n",
    "\"\"\"\n",
    "summary_output = post_processing(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00 🔶 생성 AI의 중요성: 생성 AI는 단순한 트렌드가 아닌 중요한 기술입니다. 🤖\n",
      "00:08 🔶 기업의 생성 AI 준비: 기업이 생성 AI를 이해하고 준비하는 방법에 대해 논의합니다. 🏢\n",
      "00:15 🔶 경영자와 생성 AI: 경영자들이 생성 AI를 어떻게 활용해야 하는지에 대한 이론과 현안을 다룹니다. 📚\n",
      "00:25 🔶 장동인 교수 소개: 장동인 교수님이 초청되어 특강을 진행합니다. 🎓\n",
      "00:32 🔶 첫 GPT 기업 생존법: 첫 GPT를 활용해 기업이 생존하는 방법을 소개합니다. 💼\n",
      "00:50 🔶 경영자와 개발자의 중간 역할: 경영자와 개발자 사이의 중간 역할이 중요하다고 강조합니다. 🔄\n",
      "01:11 🔶 경영자와 개발자의 협력 필요성: 경영자와 개발자가 서로의 역할을 이해해야 한다고 설명합니다. 🤝\n",
      "01:34 🔶 AI 이해의 어려움: AI를 이해하는 것이 어렵다는 점을 지적합니다. 🧠\n",
      "02:00 🔶 AI 시대의 역할: AI 시대에 경영자와 개발자의 역할을 메꿔주는 것이 중요하다고 말합니다. 🌐\n",
      "02:06 🔶 책 소개: '기업이 살아남는 법'이라는 책을 통해 경영자와 개발자의 결합을 시도합니다. 📖\n",
      "02:18 🔶 GPT의 발전: GPT 3.5, GPT 4, 그리고 곧 나올 GPT 5에 대해 언급합니다. 🚀\n",
      "02:30 🔶 GPT의 활용과 변화: GPT가 기업에 미치는 영향과 활용 방법에 대해 논의합니다. 🔄\n",
      "02:39 🔶 GPT 도입의 고민: 많은 기업들이 GPT 도입을 고민하고 있는 상황을 설명합니다. 🤔\n",
      "02:50 🔶 GPT의 한계: GPT의 한계를 명확히 이해해야 한다고 강조합니다. 🚧\n",
      "03:00 🔶 콜센터에서의 GPT 활용: 콜센터에서 GPT를 활용하는 방법과 그 한계를 설명합니다. 📞\n",
      "03:26 🔶 법규 준수의 어려움: GPT가 법규를 준수하는 데 어려움이 있다는 점을 지적합니다. ⚖️\n",
      "03:46 🔶 콜센터의 책임: 콜센터에서 GPT가 고객에게 하는 말을 기업이 책임져야 한다는 점을 강조합니다. 📋\n",
      "04:00 🔶 Hallucination 문제: GPT의 Hallucination 문제와 이를 해결하기 위한 ROG 방안을 설명합니다. 🌀\n",
      "04:21 🔶 ROG의 한계: ROG가 완벽한 해결책이 아니라는 점을 지적합니다. ❌\n",
      "04:34 🔶 GPT-5의 기대: GPT-5가 Hallucination 문제를 거의 해결할 것이라는 기대를 언급합니다. 🌟\n",
      "04:50 🔶 온 디바이스 구현의 어려움: GPT의 사이즈가 커서 온 디바이스 구현이 어렵다는 점을 설명합니다. 📱\n",
      "05:00 🔶 자동차와 GPT: 메르세데스 벤츠가 구글 클라우드와 협력해 GPT를 자동차에 연결하는 계약을 체결한 사례를 소개합니다. 🚗\n",
      "00:05:15 🔶 삼성전자가 갤럭시24를 통해 네트워크가 끊어져도 번역과 통역이 가능한 기술을 선보임 📱\n",
      "00:05:33 🔶 온 디바이스 AI 구현의 중요성 강조, 스마트폰에 라지 랭귀지 모델(LLM) 적용 가능 📲\n",
      "00:05:57 🔶 LLM을 다양한 디바이스에 적용 가능, 스마트 가전 시대 도래 예고 🤖\n",
      "00:06:34 🔶 모델 경량화가 핵심, LLM의 경량화가 비즈니스 기회로 작용할 것 💡\n",
      "00:06:50 🔶 데이터 보안 이슈, GPT-4 사용 시 데이터 학습 문제 제기 🔒\n",
      "00:07:37 🔶 오픈 AI의 데이터 프라이버시 정책, API 사용 시 데이터 학습 여부 논란 🤔\n",
      "00:08:10 🔶 클라우드 컴퓨팅 초기와 유사한 데이터 신뢰 문제, GPT-4와 오픈소스 LLM 비교 🌐\n",
      "00:09:53 🔶 GPT-4를 뛰어넘는 오픈소스 LLM은 아직 없으며, GPT-4의 다양한 기능 강조 📊\n",
      "00:10:49 🔶 멀티모델 기능의 중요성, GPT-4의 멀티모델 기능이 아직 오픈소스 LLM에 비해 우위에 있음 🖼️\n",
      "00:11:04 🔶 기업에서의 LLM 활용 방법 세 가지, 오픈소스 LLM의 이해와 활용 중요성 📈\n",
      "1. \n",
      "00:11:43 🔶 LLM의 도입 필요성: LLM은 기존 AI와 달라서 신중한 도입이 필요합니다. 🤖\n",
      "2. \n",
      "00:12:00 🔶 기업에서 LLM을 활용하는 세 가지 관점: 프롬프트 엔지니어링, 기업 자체의 활용, 비즈니스 활용입니다. 📊\n",
      "3. \n",
      "00:12:16 🔶 프롬프트 엔지니어링의 중요성: 엑셀처럼 프롬프트 엔지니어링도 기업에서 필수적으로 사용될 것입니다. 📚\n",
      "4. \n",
      "00:12:55 🔶 프롬프트 엔지니어링 TF팀 운영: 기업 내에서 프롬프트 엔지니어링 TF팀을 운영하는 것이 중요합니다. 🛠️\n",
      "5. \n",
      "00:13:19 🔶 기업 내 LLM 활용: LLM을 통해 글로벌 회의록 작성과 요약 자동화가 가능합니다. 🌐\n",
      "6. \n",
      "00:14:00 🔶 업무 프로세스와 LLM의 통합: LLM이 기업의 업무 프로세스에 완벽히 녹아들 것입니다. 🏢\n",
      "7. \n",
      "00:15:22 🔶 프롬프트 엔지니어링의 어려움: 프롬프트 엔지니어링은 쉬운 것 같지만 실제로는 어렵습니다. 🧩\n",
      "8. \n",
      "00:16:23 🔶 TF팀의 필요성: TF팀이 없으면 LLM 도입 후에도 제대로 활용되지 않을 수 있습니다. 🚀\n",
      "9. \n",
      "00:16:46 🔶 LLM 활용 방법: 기업에서 LLM을 활용하는 다섯 가지 방법에 대해 설명합니다. 📈\n",
      "10. \n",
      "00:17:00 🔶 인공지능의 학습과 인퍼런스 단계: 인공지능은 학습 단계와 인퍼런스 단계로 나뉩니다. 🧠\n",
      "00:17:17 🔶 파인트닝의 필요 요소 설명: 파인트닝을 위해 파운데이션 모델, AI 모델, 기업 데이터, GPU가 필요함을 설명합니다. 🤖\n",
      "00:18:02 🔶 기업 데이터의 포맷 변환: 기업 데이터는 파인트닝에 맞는 포맷으로 변환되어야 함을 강조합니다. 📊\n",
      "00:19:00 🔶 파인트닝의 정의: 파인트닝은 기업의 제품에 맞게 AI 모델을 튜닝하는 과정임을 설명합니다. 🛠️\n",
      "00:19:13 🔶 GPT-4의 비즈니스 모델: GPT-4의 비즈니스 모델을 중국집 셰프의 비유로 설명합니다. 🍜\n",
      "00:20:08 🔶 원샷, 퓨샷, 제로샷 러닝: 원샷, 퓨샷, 제로샷 러닝의 개념을 소개합니다. 🎯\n",
      "00:21:00 🔶 다양한 파인트닝 기법: GPU 사용을 최소화하고 데이터를 효율적으로 활용하는 다양한 파인트닝 기법을 설명합니다. 🧩\n",
      "00:21:36 🔶 인 컨텍스트 러닝: 기업 데이터를 활용한 인 컨텍스트 러닝 기법을 소개합니다. 🧠\n",
      "00:21:48 🔶 일루서 AI 소개: 일루서 AI와 그들이 만든 GPT-J에 대해 설명합니다. 🌐\n",
      "00:22:22 🔶 폴리글라스: 한국어 파운데이션 모델 소개 🌐\n",
      "00:22:48 🔶 라마 시리즈: 라마2와 곧 출시될 라마3 언급 🦙\n",
      "00:23:09 🔶 오픈 데이터: 1TB의 오픈 소스 데이터로 트레이닝 🗂️\n",
      "00:23:18 🔶 다양한 모델: 돌리트, 스테이블 LM 등 다양한 모델 등장 🤖\n",
      "00:23:25 🔶 Lama 2: 페이스북의 오픈 소스 모델 라마2 소개 📘\n",
      "00:23:42 🔶 라마1 문제: 초기 라마1의 문제와 공개 과정 🕵️‍♂️\n",
      "00:24:03 🔶 라마3 준비: 메타의 라마3 개발과 GPU 대량 구매 💻\n",
      "00:24:41 🔶 메타의 목표: 라마3를 GPT-4 수준으로 만들려는 목표 🎯\n",
      "00:25:15 🔶 오픈소스 모델: 알파카, 빕쿠나 등 수천 개의 오픈소스 모델 🌟\n",
      "00:25:26 🔶 전 세계적으로 오픈소스 LLM 리더보드가 존재하며 매주 업데이트됨 🌍\n",
      "00:25:48 🔶 유엔, 무리나, 이스라엘 등 다양한 국가들이 LLM을 개발 중임 🇺🇳🇮🇱\n",
      "00:26:00 🔶 LLM을 사용하지 않으면 뒤처지는 시대가 도래함 ⏳\n",
      "00:26:06 🔶 랭체인은 기업 내부에서 라이저 랭게이지 모델을 연결하는 라이브러리임 🏢\n",
      "00:26:24 🔶 랭체인은 작년에 큰 성공을 거두었으며, 현재 회사 가치가 2억 달러에 달함 💰\n",
      "00:26:42 🔶 랭체인은 기업 개발자들이 반드시 배워야 하는 필수 기술임 📚\n",
      "00:27:00 🔶 랭체인은 LLM, 인베이딩, 벡터스토어 등을 연결하는 프로그래밍 라이브러리임 🔗\n",
      "00:27:32 🔶 랭체인을 통해 회사 내부 사용자들이 프롬트를 입력하면 벡터스토어에 캐시 처리됨 🗂️\n",
      "00:28:00 🔶 벡터스토어는 회사 내부 문서를 벡터화하여 저장함 📄\n",
      "00:28:32 🔶 Similar Search를 통해 유사한 정보를 빠르게 찾아냄 🔍\n",
      "00:28:48 🔶 랭체인을 통해 회사의 정책 등을 쉽게 검색하고 응답할 수 있음 🏢\n",
      "00:29:00 🔶 GPT-4나 맞춤형 LLM을 통해 사용자에게 자연스러운 답변을 제공함 🤖\n",
      "00:29:22 🔶 인터넷 검색과 내부 시스템을 연동하여 다양한 정보를 제공함 🌐\n",
      "00:29:44 🔶 GPT-4의 에이전트 기능 소개: 에이전트를 활용하면 많은 일을 할 수 있음 🤖\n",
      "00:30:02 🔶 기업 맞춤형 어플리케이션 개발: 랭체인을 통해 기업에 맞는 어플리케이션을 개발할 수 있음 🏢\n",
      "00:30:13 🔶 LLM 도입 방법: 기업에서 LLM을 도입할 때 선택할 수 있는 5가지 방법 소개 📚\n",
      "00:30:20 🔶 OpenAI API 사용: OpenAI의 API를 사용하여 GPT-4와 연결하는 방법 설명 🌐\n",
      "00:30:48 🔶 랭체인과 벡터 DB: 랭체인 프로그램을 통해 벡터 DB에서 데이터를 쿼리하고 GPT-4로 답변 받기 💾\n",
      "00:31:19 🔶 벡터 데이터 해석: GPT-4가 벡터 데이터를 해석하여 사람이 이해할 수 있는 답변 제공 🧠\n",
      "00:31:50 🔶 인베이딩 방법: 오픈소스와 OpenAI GPT-4의 인베이딩 방법 차이 설명 🔍\n",
      "00:32:30 🔶 방화벽 문제: 한국 기업의 방화벽 문제와 GPT-4 사용의 어려움 🔒\n",
      "00:32:44 🔶 벡터스토어는 외국 제품입니다. 🌍\n",
      "00:33:08 🔶 인베딩 종류와 비용에 대한 설명입니다. 💸\n",
      "00:34:00 🔶 다양한 벡터DB 종류와 클라우드 제공 문제점입니다. ☁️\n",
      "00:34:33 🔶 국내에서 벡터DB를 만드는 시도입니다. 🇰🇷\n",
      "00:35:00 🔶 기업 데이터 인베딩과 비용 문제입니다. 🏢\n",
      "00:35:18 🔶 GPT-4 사용 시 비용 문제와 대안으로 GPT-3.5 활용 방안입니다. 🤖\n",
      "00:35:44 🔶 시스템 사용 시 오픈AI에 지불해야 하는 비용 문제입니다. 💰\n",
      "00:36:10 🔶 벡터DB와 GPT-4 사용 시 비용 발생에 대한 설명입니다. 💵\n",
      "00:36:43 🔶 Open Source LLM을 활용한 대안 제시입니다. 🆓\n",
      "00:36:51 🔶 솔라의 인기와 활용 방안에 대해 설명합니다. 🌟\n",
      "00:37:00 🔶 기업 데이터와 오픈소스 LLM의 인베딩 비용 문제를 논의합니다. 💼\n",
      "00:37:19 🔶 솔라를 사용하면 비용 절감이 가능하다는 점을 강조합니다. 💰\n",
      "00:37:34 🔶 오픈소스 LLM과 GPT-4의 기능 경쟁에 대해 이야기합니다. 🏆\n",
      "00:37:54 🔶 원프레미스 벡터 DB의 필요성과 외국 제품의 한계를 언급합니다. 🌐\n",
      "00:38:06 🔶 원프레미스 벡터 DB를 많이 사용하라고 권장합니다. 📊\n",
      "00:38:24 🔶 파운데이션 모델에 기업 데이터를 넣어 파인트닝하는 방법을 설명합니다. 🏭\n",
      "00:38:44 🔶 기업 맞춤형 GPT를 만드는 C타입 방법을 소개합니다. 🛠️\n",
      "00:39:00 🔶 벤처 기업들이 파인트닝을 통해 맞춤형 모델을 제공하는 사례를 설명합니다. 🚀\n",
      "00:39:19 🔶 맞춤형 모델을 통해 비즈니스 기회를 창출하는 방법을 논의합니다. 💡\n",
      "00:39:54 🔶 오픈AI가 기업 데이터를 활용해 GPT 3.5 Turbo 버전을 제공하는 방안을 발표합니다. 📈\n",
      "00:40:16 🔶 GPT 3.5 Turbo 버전의 비용 구조와 트레이닝 비용을 설명합니다. 💸\n",
      "1. 00:40:51 🔶 GPU가 필요 없는 작업 설명\n",
      "   - GPU 없이도 특정 작업을 수행할 수 있는 방법을 설명합니다. 💻\n",
      "2. 00:41:15 🔶 GPT-4와 GPT-4V의 현재 상태\n",
      "   - GPT-4와 GPT-4V가 아직 완전히 구현되지 않았음을 언급합니다. 🤖\n",
      "3. 00:41:38 🔶 GPT-4의 멀티모델 기능\n",
      "   - GPT-4의 멀티모델 기능과 이를 활용한 데이터 해석 가능성을 설명합니다. 📊\n",
      "4. 00:42:00 🔶 회사 데이터 학습의 중요성\n",
      "   - 회사의 PPT나 PDF 데이터를 학습시켜 맞춤형 솔루션을 제공하는 가능성을 논의합니다. 📁\n",
      "5. 00:42:34 🔶 건설사 예시를 통한 GPT-4 활용\n",
      "   - 건설사 데이터를 학습시켜 비용, 스케줄, 리스크 분석을 수행하는 예시를 듭니다. 🏗️\n",
      "6. 00:42:51 🔶 변호사 사무실에서의 GPT-4 활용\n",
      "   - 변호사 사무실에서 PDF 파일과 사진을 학습시켜 전문가 역할을 할 수 있는 가능성을 설명합니다. ⚖️\n",
      "7. 00:43:15 🔶 GPT-5에 대한 기대\n",
      "   - GPT-5가 나오면 LLM이 새로운 수준에 도달할 것이라는 기대를 표현합니다. 🌟\n",
      "8. 00:43:44 🔶 GPT-5의 회사 전문가 역할\n",
      "   - GPT-5가 회사의 모든 문서를 이해하고 전문가 역할을 할 수 있는 가능성을 설명합니다. 🏢\n",
      "9. 00:44:11 🔶 엔터프라이즈 채집 PT\n",
      "   - 엔터프라이즈 채집 PT가 데이터 보안과 관리에서 중요한 역할을 한다고 설명합니다. 🔒\n",
      "10. 00:44:34 🔶 오픈 AI의 흑자와 매출\n",
      "    - 오픈 AI가 흑자를 기록하고 매출이 2밀리언 달러를 넘었다는 사실을 언급합니다. 💰\n",
      "00:44:46 🔶 토큰 수익이 적지만 엔터프라이즈 채집 PT가 큰 수익을 창출함 💰\n",
      "00:45:08 🔶 한국에서 엔터프라이즈 채집 PT에 관심 있는 회사들이 많음 🇰🇷\n",
      "00:45:19 🔶 샘 알트만의 한국 방문과 한국 지사 설립 가능성 🏢\n",
      "00:45:28 🔶 오픈 AI의 다양한 활용 방식 제안 🤖\n",
      "00:45:55 🔶 대기업과 관공서의 보안 유지 방법 제안 🔒\n",
      "00:46:17 🔶 온프레미스 벡터 DB를 활용한 문서 보관 방법 📂\n",
      "00:46:27 🔶 소기업과 대기업의 AI 활용 방식 차이 설명 🏢\n",
      "00:46:43 🔶 GPT5와 D타입 활용 가능성 검토 📊\n",
      "00:47:00 🔶 그림과 텍스트가 있는 PDF 학습의 중요성 🖼️\n",
      "00:47:14 🔶 비즈니스 활용 방법과 오픈소스 API의 중요성 📈\n",
      "00:47:43 🔶 전자상거래의 변화와 AI의 역할 🛒\n",
      "00:49:00 🔶 네이버의 Q 서비스와 전자상거래의 미래 🔍\n",
      "00:49:49 🔶 AI를 활용한 구매 대행 사이트의 가능성 🌐\n",
      "00:50:04 🔶 LLM을 활용한 데이터 기반 보고서 및 글 작성 📝\n",
      "00:50:20 🔶 LLM이 웹사이트를 검색해 원하는 정보를 제공하는 기능 🌐\n",
      "00:50:39 🔶 여행 계획 시 다양한 사이트를 비교하는 과정 설명 🧳\n",
      "00:50:55 🔶 LLM을 이용한 여행 대행 서비스의 가능성 🌍\n",
      "00:51:15 🔶 플랫폼 위의 플랫폼 개념 설명 🏗️\n",
      "00:52:00 🔶 오픈AI의 비즈니스 전략과 철학 💡\n",
      "00:53:00 🔶 다양한 분야에서 LLM을 활용한 비즈니스 아이디어 💼\n",
      "00:54:00 🔶 로봇과 LLM의 결합 가능성 🤖\n",
      "00:54:36 🔶 로봇의 움직임을 위한 코딩의 중요성 및 LLM의 역할 🛠️\n",
      "00:55 🔶 로봇이 인간의 말을 이해하고 행동하는 기술의 발전 🌟\n",
      "00:56 🔶 드론과 로봇이 인간의 명령을 이해하고 수행하는 시대 도래 🚁\n",
      "00:57 🔶 파운데이션 모델이 AI 모델을 제어하고 실행하는 기능 🧠\n",
      "00:58 🔶 AI가 공장과 기업의 데이터를 해석하고 상황 판단을 수행 🏭\n",
      "00:59 🔶 AI가 시뮬레이션을 통해 문제 해결책을 제시하고 실행 🛠️\n",
      "01:00 🔶 AGI(인공지능 일반)의 도래와 기업의 대응 전략 🤖\n",
      "01:01 🔶 AI 부서와 협업을 통한 기업 내 AI 활용 방안 🏢\n",
      "01:02 🔶 디지털 트랜스포메이션의 필요성과 실행 방안 💡\n",
      "01:03:07 🔶 신문에 나오는 디지털 기술에 대해 실제 담당자들은 잘 모른다. 📰\n",
      "01:03:14 🔶 빅데이터와 AI 기술은 외부 전문가가 도입하지만, 그들은 회사 비즈니스를 모른다. 🤖\n",
      "01:03:21 🔶 비즈니스를 모르는 상태에서 AI와 빅데이터를 적용하는 것은 불가능하다. 🚫\n",
      "01:03:31 🔶 AI는 특정 도메인과 데이터를 알아야 효과적으로 학습하고 적용될 수 있다. 📊\n",
      "01:03:44 🔶 결국 신문에 나오는 AI는 실제로는 제대로 적용되지 않는다. 📰🚫\n",
      "01:04:00 🔶 외부 AI 도입의 비효율성 강조: 외부에서 AI를 도입하는 것은 비용과 시간이 많이 들고 효과가 적다고 설명합니다. 💸\n",
      "01:04:20 🔶 내부 직원 교육과 인센티브의 중요성: 내부 직원들을 교육시키고 인센티브를 제공하여 AI를 적용하는 것이 중요하다고 주장합니다. 🎓\n",
      "01:04:55 🔶 직원들의 AI 학습 필요성: 업무를 아는 직원들이 AI를 공부하고 적용해야 디지털 트랜스포메이션이 가능하다고 강조합니다. 📚\n",
      "01:05:08 🔶 식스시그마의 성공 사례: 식스시그마가 한국에서 성공한 유일한 사례로 언급되며, 이를 AI 도입에 비유합니다. 🏆\n",
      "01:06:04 🔶 DMAIC 방법론의 활용: DMAIC 방법론을 통해 AI를 가르치고 소조활동을 통해 결과를 도출하는 방식을 제안합니다. 🛠️\n",
      "01:06:26 🔶 공공기관의 AI 도입 어려움: 공공기관에서도 외부 인사가 아닌 내부 담당자가 AI 도입을 주도해야 한다고 설명합니다. 🏛️\n",
      "01:07:00 🔶 공공기관의 인센티브 필요성: 공공기관에서도 식스시그마와 유사한 활동을 통해 인센티브를 제공해야 효과적이라고 주장합니다. 🎁\n",
      "01:07:14 🔶 기업의 POC 형태와 LNM의 한계: 기업들이 POC 형태로 AI를 도입하고 있으며, 만능적인 LNM은 없고 도메인 특화된 LNM이 필요하다고 설명합니다. 🏢\n",
      "01:07:48 💼 큰 기업과 작은 기업의 여력 차이\n",
      "01:08:00 💸 외부 업체를 통한 비용 부담 문제\n",
      "01:08:11 🤔 도메인 스페이스픽한 해결책의 필요성\n",
      "01:08:17 🧠 GPT-4와 오픈소스 LLM의 차이\n",
      "01:08:29 📚 GPT-4의 MOE(분야별 전문가) 구조\n",
      "01:08:46 🏢 기업 내 LLM 구축의 어려움\n",
      "01:09:00 🔄 분야별 LLM의 필요성\n",
      "01:09:08 🔍 Lama3의 가능성\n",
      "01:09:19 📘 AI 학습을 위한 자료와 코스 추천\n",
      "01:09:36 📺 유튜브와 책을 통한 학습 방법\n",
      "01:10:00 🔄 도메인 지식과 데이터의 중요성\n",
      "01:10:20 🛠 TF 구성의 필요성\n",
      "01:10:39 🏆 식스시그마와 생성 AI의 활용 사례\n",
      "01:10:49 🔄 변역기의 중요성과 기업의 대응\n",
      "01:11:21 👏 특강 마무리와 감사 인사\n"
     ]
    }
   ],
   "source": [
    "print(summary_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
